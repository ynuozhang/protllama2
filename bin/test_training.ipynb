{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, DatasetDict, load_from_disk, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels'],\n        num_rows: 253440\n    })\n    valid: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels'],\n        num_rows: 232711\n    })\n})"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = f'/data/rozen/home/e0833634/lama/protllama/batch_script/uniref50_random90split_8k_512_first_1million_dataset.hf'\n",
    "dataset = load_from_disk(dataset_path)\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels'],\n        num_rows: 10\n    })\n    valid: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels'],\n        num_rows: 10\n    })\n})"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_dataset_dict = DatasetDict({\n",
    "    'train': dataset['train'].select(range(10)),\n",
    "    'valid': dataset['valid'].select(range(10))\n",
    "})\n",
    "small_dataset_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "[[769188, 769231, 769281, 770055, 770166, 770414],\n [55962],\n [951805, 951834],\n [34696, 36909],\n [39459, 39650, 39713, 39794, 39797, 39858, 40168, 40532, 40713, 40750, 41035],\n [139898, 139951, 140097],\n [223249, 224129],\n [181],\n [227032,\n  227315,\n  227702,\n  227839,\n  227841,\n  228178,\n  228288,\n  228420,\n  228457,\n  228582],\n [94817, 94880]]"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/data/rozen/home/e0833634/lama/protllama/batch_script/train_intermediate_checkpoint_batches_1000000.pkl', 'rb') as f:\n",
    "    batch_indices_train = pickle.load(f)\n",
    "batch_indices_train = batch_indices_train[:10]\n",
    "batch_indices_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class DynamicBatchingDataset(Dataset):\n",
    "    def __init__(self, dataset_dict, batch_indices):\n",
    "        self.dataset_dict = dataset_dict['train']\n",
    "        self.batch_indices = batch_indices  # This is mainly for informational purposes, if needed.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_dict['attention_mask'])  # Assuming each entry in dataset_dict represents a batch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #batch_idx = self.batch_indices[idx]\n",
    "        # Directly retrieve the batch using the index\n",
    "        \"\"\"returns [seq_number, token_length], return one batch at a time\"\"\"\n",
    "        attention_mask = torch.tensor(self.dataset_dict['attention_mask'][idx])\n",
    "        input_ids = torch.tensor(self.dataset_dict['input_ids'][idx])\n",
    "        label = torch.tensor(self.dataset_dict['labels'][idx])\n",
    "\n",
    "        return {\n",
    "            'attention_mask': attention_mask,\n",
    "            'input_ids': input_ids,\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_collate_fn(batch):\n",
    "        \"\"\"Return the first element of the batch because each element is already a batch.\n",
    "            (prevent auto batching from pytorch DataLoader, otherwise, the batch_size=1 will add another dimension during data retrieval)\n",
    "        \"\"\"\n",
    "        return batch[0]\n",
    "\n",
    "#batch_indices = [[181], [55, 266]]  # Your actual batch indices\n",
    "train_dataset = DynamicBatchingDataset(small_dataset_dict, batch_indices_train)\n",
    "\n",
    "# Batch size is set to 1 because your dataset itself is returning batches\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=DynamicBatchingDataset.custom_collate_fn)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[   1,  390,  817,  325,  881,  545, 4579, 2698, 3853,  400, 3222,  811,\n",
      "         5410,  757, 1580,  312, 2013, 1832,  689, 2295, 3041, 3967,  762, 5632,\n",
      "         2307, 4670, 3743,  300, 5307,  343, 2685, 3989, 5093, 4627, 4023,  771,\n",
      "         3396, 6661,  220,  379, 6834,  771, 3306, 1659, 4508, 5596, 2220, 2892,\n",
      "          461,    4, 2068, 1108, 1198, 2766, 2828, 1009,  585, 2566, 3824, 6189,\n",
      "          666, 1509, 5843, 1419,  172,  810, 4278,  931,  707, 4049,  470, 5006,\n",
      "         1181, 1208, 1271, 3306,  505, 3989, 4959, 3861,  356, 4102, 3154,  611],\n",
      "        [   1,  820, 1919, 4465, 1017, 1241, 4424, 2858, 1017,   53, 2874, 3400,\n",
      "         1248,  617,  737,   54,  409,  591,  924,   37, 4884, 6452, 1225, 3224,\n",
      "          378, 2301,  434, 6762,  663, 3534,   72, 4146,  464, 1098,   45, 1134,\n",
      "          348, 1982, 3944, 4862,  995,  485, 4882, 1472, 4226,   54,  995, 1209,\n",
      "         5196,  277, 1428,  582, 2211,  552,  784, 4702, 6476,  787, 3076,  236,\n",
      "          314, 5817, 1042, 1265,   61, 3873,  381, 2263, 5040,  330, 1234,  282,\n",
      "          742, 6747, 7776, 6029,  210, 4271, 5760,  907, 2704,  517, 5292, 1141],\n",
      "        [   1, 1171, 7708,   34, 4009, 1036, 3470,   74, 1039, 2396,  606, 2432,\n",
      "          337,  356,  402,  894, 1974,  212, 3452, 6420, 1611, 5894, 3857, 5713,\n",
      "         1353, 2464, 2909, 4248, 4887,   21, 5170, 2999, 5670, 3389,  729, 3524,\n",
      "         1726,  191,  337, 5388, 5772, 3095,  610,  946, 1610,  330,  339, 5085,\n",
      "         5794, 3690, 3926,  337, 1711, 7155, 6247,  306, 7910,    6,  845, 3122,\n",
      "         7642,  341,  846,  613,  416, 4251, 1209, 1946, 5173, 7705, 2679, 7705,\n",
      "          679, 1321, 4645, 1305,  716,  363,  790, 1968, 3273,  605,  101,  706],\n",
      "        [   1, 4052,   47,  516,   21, 1253, 1603, 1026, 1306, 4156, 4988,  661,\n",
      "         2514, 1590, 5005, 5831, 3966, 6830, 6751, 1786, 4531,  606, 2634, 1216,\n",
      "         1304, 4026, 4617,  383,  244,  412,  437,  947, 3502, 3457, 1408,  863,\n",
      "         2632, 1345,  383, 1905, 2024,  809, 1291, 5253, 2583, 1435,  903,   25,\n",
      "          804, 3076,  781, 2702,  137,   21,  500,  921,  891,  579, 1996, 3865,\n",
      "         1681, 1455,   22, 5327, 1399, 3981, 1424, 3171,   32, 2521, 1496,  838,\n",
      "         4514, 1324,  591, 2282,  483,  889, 7513,  276, 6409, 3009, 4022, 4962],\n",
      "        [   1,   28,  807, 6219,  114, 7396,  133, 4094, 2677,  791, 1501, 3151,\n",
      "         2209, 1532,  921,  867, 2447,  561, 2958, 7742, 1086, 5403, 3735, 4276,\n",
      "         5093,  998, 2591, 3703, 3297, 7128, 2520, 3234,  183, 2091, 2077,  970,\n",
      "          291,  519, 1742,  869, 2382,   33,  953, 2084, 2341,    3, 1365, 2507,\n",
      "         5127, 2217, 2430,  262, 3214, 4115, 3266,  381, 5441,  425, 5146, 4858,\n",
      "         1218,  426, 4543, 1727, 5532, 4994, 5937, 3778, 2912, 6576, 3879, 4238,\n",
      "         2751, 1493, 1522, 1090, 1214,  498, 6998,  137, 2846,  480,  366,  324],\n",
      "        [   1, 7322,  680,  648, 3056,  449,  634,  607, 1337, 3661, 1483,  599,\n",
      "          875, 1645, 3648,   47,  409, 3993, 2543, 1437,  317, 6480, 2885,  577,\n",
      "          478, 2763,  934, 1639, 4523, 1769, 7046, 3417, 1386,  620, 1788, 1711,\n",
      "         6623,  774, 6154, 1438, 3898,  505, 4123, 1321, 5935,   29, 1754,  878,\n",
      "         1755, 4279, 1447, 1102,  724, 1263, 5214,  618, 3792,  486,  342, 4051,\n",
      "         2855, 1510, 1531, 4306,  715,  136, 1182, 3362, 1651, 1342, 2957,  424,\n",
      "           65, 3007, 4999, 2133, 3529, 2606,  647, 1214, 3637, 2996, 5330, 6755]]), 'labels': tensor([[   1,  390,  817,  325,  881,  545, 4579, 2698, 3853,  400, 3222,  811,\n",
      "         5410,  757, 1580,  312, 2013, 1832,  689, 2295, 3041, 3967,  762, 5632,\n",
      "         2307, 4670, 3743,  300, 5307,  343, 2685, 3989, 5093, 4627, 4023,  771,\n",
      "         3396, 6661,  220,  379, 6834,  771, 3306, 1659, 4508, 5596, 2220, 2892,\n",
      "          461,    4, 2068, 1108, 1198, 2766, 2828, 1009,  585, 2566, 3824, 6189,\n",
      "          666, 1509, 5843, 1419,  172,  810, 4278,  931,  707, 4049,  470, 5006,\n",
      "         1181, 1208, 1271, 3306,  505, 3989, 4959, 3861,  356, 4102, 3154,  611],\n",
      "        [   1,  820, 1919, 4465, 1017, 1241, 4424, 2858, 1017,   53, 2874, 3400,\n",
      "         1248,  617,  737,   54,  409,  591,  924,   37, 4884, 6452, 1225, 3224,\n",
      "          378, 2301,  434, 6762,  663, 3534,   72, 4146,  464, 1098,   45, 1134,\n",
      "          348, 1982, 3944, 4862,  995,  485, 4882, 1472, 4226,   54,  995, 1209,\n",
      "         5196,  277, 1428,  582, 2211,  552,  784, 4702, 6476,  787, 3076,  236,\n",
      "          314, 5817, 1042, 1265,   61, 3873,  381, 2263, 5040,  330, 1234,  282,\n",
      "          742, 6747, 7776, 6029,  210, 4271, 5760,  907, 2704,  517, 5292, 1141],\n",
      "        [   1, 1171, 7708,   34, 4009, 1036, 3470,   74, 1039, 2396,  606, 2432,\n",
      "          337,  356,  402,  894, 1974,  212, 3452, 6420, 1611, 5894, 3857, 5713,\n",
      "         1353, 2464, 2909, 4248, 4887,   21, 5170, 2999, 5670, 3389,  729, 3524,\n",
      "         1726,  191,  337, 5388, 5772, 3095,  610,  946, 1610,  330,  339, 5085,\n",
      "         5794, 3690, 3926,  337, 1711, 7155, 6247,  306, 7910,    6,  845, 3122,\n",
      "         7642,  341,  846,  613,  416, 4251, 1209, 1946, 5173, 7705, 2679, 7705,\n",
      "          679, 1321, 4645, 1305,  716,  363,  790, 1968, 3273,  605,  101,  706],\n",
      "        [   1, 4052,   47,  516,   21, 1253, 1603, 1026, 1306, 4156, 4988,  661,\n",
      "         2514, 1590, 5005, 5831, 3966, 6830, 6751, 1786, 4531,  606, 2634, 1216,\n",
      "         1304, 4026, 4617,  383,  244,  412,  437,  947, 3502, 3457, 1408,  863,\n",
      "         2632, 1345,  383, 1905, 2024,  809, 1291, 5253, 2583, 1435,  903,   25,\n",
      "          804, 3076,  781, 2702,  137,   21,  500,  921,  891,  579, 1996, 3865,\n",
      "         1681, 1455,   22, 5327, 1399, 3981, 1424, 3171,   32, 2521, 1496,  838,\n",
      "         4514, 1324,  591, 2282,  483,  889, 7513,  276, 6409, 3009, 4022, 4962],\n",
      "        [   1,   28,  807, 6219,  114, 7396,  133, 4094, 2677,  791, 1501, 3151,\n",
      "         2209, 1532,  921,  867, 2447,  561, 2958, 7742, 1086, 5403, 3735, 4276,\n",
      "         5093,  998, 2591, 3703, 3297, 7128, 2520, 3234,  183, 2091, 2077,  970,\n",
      "          291,  519, 1742,  869, 2382,   33,  953, 2084, 2341,    3, 1365, 2507,\n",
      "         5127, 2217, 2430,  262, 3214, 4115, 3266,  381, 5441,  425, 5146, 4858,\n",
      "         1218,  426, 4543, 1727, 5532, 4994, 5937, 3778, 2912, 6576, 3879, 4238,\n",
      "         2751, 1493, 1522, 1090, 1214,  498, 6998,  137, 2846,  480,  366,  324],\n",
      "        [   1, 7322,  680,  648, 3056,  449,  634,  607, 1337, 3661, 1483,  599,\n",
      "          875, 1645, 3648,   47,  409, 3993, 2543, 1437,  317, 6480, 2885,  577,\n",
      "          478, 2763,  934, 1639, 4523, 1769, 7046, 3417, 1386,  620, 1788, 1711,\n",
      "         6623,  774, 6154, 1438, 3898,  505, 4123, 1321, 5935,   29, 1754,  878,\n",
      "         1755, 4279, 1447, 1102,  724, 1263, 5214,  618, 3792,  486,  342, 4051,\n",
      "         2855, 1510, 1531, 4306,  715,  136, 1182, 3362, 1651, 1342, 2957,  424,\n",
      "           65, 3007, 4999, 2133, 3529, 2606,  647, 1214, 3637, 2996, 5330, 6755]])}\n",
      "1 {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[   1,   28, 3312, 4266, 2308, 1219, 2665, 1052, 6285,  164,  411,  763,\n",
      "          760, 1737, 6201, 3094, 2154, 2344, 5860, 7077,  374,  472, 7350, 4553,\n",
      "         1061, 1254, 2222, 2739, 2237, 2823, 7070, 2058, 1121, 2672, 5592,  987,\n",
      "         2171,  751, 4233,  696,   55,  908, 2149, 3049,  928, 2605, 2578,  370,\n",
      "         4472, 1064, 1291,  224,  162, 1482, 5744, 7558,  983, 1940,  691, 7570,\n",
      "         1567,  436, 2667, 2250, 1060,  625, 2229, 4668,  949,  140, 3011, 1959,\n",
      "         5673,  798, 1685, 2393,  452,  670,  759, 1558, 2864, 1041, 5518,  119,\n",
      "          889,  711, 3631, 2562, 1294, 2593, 1484, 4434, 3037, 4208, 3096,  613,\n",
      "         5596, 3820,  192, 2169, 4229,  825,  596, 3919,  141, 1549, 4692, 2247,\n",
      "         1919, 1012,  648, 1870, 6303, 1648,  422, 4305, 1114,    6, 3947, 4162,\n",
      "         4547, 5194, 2131, 4992, 5694, 3414, 2910, 4514, 3348,   86, 3929,  565,\n",
      "         3140,  827,   61, 4195,  282,  478, 3656,   51, 1107,  996, 3010,  970,\n",
      "           83, 1158, 1357, 4239,  226,  408, 2684, 3768, 2607, 6537,  924, 3098,\n",
      "          292,  752, 6317,   84,  242,  309, 5158,  284, 1909, 1277,  803, 5547,\n",
      "          440, 2414, 1458, 2629, 4801,  118,  690, 4322, 3482, 4563, 5003, 5452,\n",
      "         1920, 2468, 2046,  307, 4248, 7985,   44, 1574, 4310, 5485,   41, 3735,\n",
      "         6127, 3588,  327, 4205, 2239, 3828,   62, 3605, 2813, 2334, 2274, 1975,\n",
      "          857,  773,  629, 2038, 1005, 5770, 1608, 4084,  132,  736, 2318, 4522,\n",
      "          162, 3868,  590, 5828, 4519, 2570,  577, 1805,  613, 1529, 1296, 2199,\n",
      "         7640, 2486, 4636,  102, 4170, 4427,   16,   95, 1205, 1364, 6847,  511,\n",
      "         2281, 4215,  346, 1653,  150, 4343,  691,  852,  955,  957, 6160,  120,\n",
      "         4641,  544, 6930, 7429, 1180, 6074, 6856,   16, 3996, 1799, 7919, 1045,\n",
      "         2887, 1452,   47, 6844, 1522, 3440,   49, 2498, 4131, 3835,  584,  350,\n",
      "          688, 6349, 2712,  428,  815,  982, 2392, 1195, 1824, 4168, 1368,  305,\n",
      "         3075, 1524, 2150, 7203,  891, 1318, 1633, 2186,  971, 1067, 3896, 2725,\n",
      "         2910,  799,  502, 2769, 4050, 2588, 1138,  341, 3523,   79, 4255, 1454,\n",
      "         1770,  963,  814,  380, 1321,   80, 3036, 6044, 2327, 3429,   61, 3494,\n",
      "         1928,  114, 1426, 1803,  311, 7235, 1686]]), 'labels': tensor([[   1,   28, 3312, 4266, 2308, 1219, 2665, 1052, 6285,  164,  411,  763,\n",
      "          760, 1737, 6201, 3094, 2154, 2344, 5860, 7077,  374,  472, 7350, 4553,\n",
      "         1061, 1254, 2222, 2739, 2237, 2823, 7070, 2058, 1121, 2672, 5592,  987,\n",
      "         2171,  751, 4233,  696,   55,  908, 2149, 3049,  928, 2605, 2578,  370,\n",
      "         4472, 1064, 1291,  224,  162, 1482, 5744, 7558,  983, 1940,  691, 7570,\n",
      "         1567,  436, 2667, 2250, 1060,  625, 2229, 4668,  949,  140, 3011, 1959,\n",
      "         5673,  798, 1685, 2393,  452,  670,  759, 1558, 2864, 1041, 5518,  119,\n",
      "          889,  711, 3631, 2562, 1294, 2593, 1484, 4434, 3037, 4208, 3096,  613,\n",
      "         5596, 3820,  192, 2169, 4229,  825,  596, 3919,  141, 1549, 4692, 2247,\n",
      "         1919, 1012,  648, 1870, 6303, 1648,  422, 4305, 1114,    6, 3947, 4162,\n",
      "         4547, 5194, 2131, 4992, 5694, 3414, 2910, 4514, 3348,   86, 3929,  565,\n",
      "         3140,  827,   61, 4195,  282,  478, 3656,   51, 1107,  996, 3010,  970,\n",
      "           83, 1158, 1357, 4239,  226,  408, 2684, 3768, 2607, 6537,  924, 3098,\n",
      "          292,  752, 6317,   84,  242,  309, 5158,  284, 1909, 1277,  803, 5547,\n",
      "          440, 2414, 1458, 2629, 4801,  118,  690, 4322, 3482, 4563, 5003, 5452,\n",
      "         1920, 2468, 2046,  307, 4248, 7985,   44, 1574, 4310, 5485,   41, 3735,\n",
      "         6127, 3588,  327, 4205, 2239, 3828,   62, 3605, 2813, 2334, 2274, 1975,\n",
      "          857,  773,  629, 2038, 1005, 5770, 1608, 4084,  132,  736, 2318, 4522,\n",
      "          162, 3868,  590, 5828, 4519, 2570,  577, 1805,  613, 1529, 1296, 2199,\n",
      "         7640, 2486, 4636,  102, 4170, 4427,   16,   95, 1205, 1364, 6847,  511,\n",
      "         2281, 4215,  346, 1653,  150, 4343,  691,  852,  955,  957, 6160,  120,\n",
      "         4641,  544, 6930, 7429, 1180, 6074, 6856,   16, 3996, 1799, 7919, 1045,\n",
      "         2887, 1452,   47, 6844, 1522, 3440,   49, 2498, 4131, 3835,  584,  350,\n",
      "          688, 6349, 2712,  428,  815,  982, 2392, 1195, 1824, 4168, 1368,  305,\n",
      "         3075, 1524, 2150, 7203,  891, 1318, 1633, 2186,  971, 1067, 3896, 2725,\n",
      "         2910,  799,  502, 2769, 4050, 2588, 1138,  341, 3523,   79, 4255, 1454,\n",
      "         1770,  963,  814,  380, 1321,   80, 3036, 6044, 2327, 3429,   61, 3494,\n",
      "         1928,  114, 1426, 1803,  311, 7235, 1686]])}\n",
      "2 {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[   1,   28,  425,  643,  481, 2329,  841,  144, 5480, 1528,  434,   98,\n",
      "          794, 3818, 4905,  435, 1665, 1354,  853, 1869, 5901, 1184, 7909,  664,\n",
      "         4766,   20,  357, 5219, 7122, 1484, 2861, 4958,  443, 7387, 1650, 1385,\n",
      "         2494, 1201, 3444, 1166, 3346, 2041, 1034,  302, 2817, 5065, 3014,  953,\n",
      "          864, 1296, 3378,  865,  589,  661, 3076, 1191, 6644, 2507,   86, 3464,\n",
      "          308, 1842,  672, 4501,   32, 5916, 3328,  862, 1782, 1561,   32, 1321,\n",
      "          680, 7365, 7170, 2108, 2438, 1479,  419, 4610,  762,   37, 3939, 2382,\n",
      "          159, 6184, 1541, 3693,  320, 3457,  346,  552, 2236, 4494, 2809, 2892,\n",
      "          817, 2169, 6473, 2224, 2074, 4095, 5720, 3350, 1689,  892, 4599, 3284,\n",
      "          850, 2144, 4824,  317, 1845, 4416, 1115, 3415, 1187, 3178, 1157, 6919,\n",
      "         3964, 3752,  932, 4710, 1584, 6405, 3557, 1531, 2983, 5914,  928, 6838,\n",
      "         7257,  263,  104, 1423, 1869, 1011,  778, 5890, 1412,  619,  403, 3467,\n",
      "         1016, 1475, 5699, 2724,   42,  792, 5948, 1623, 2916, 6502,  538,  912,\n",
      "         3671, 2882,  218,  145, 1034, 2768, 1014, 1369, 2761,    3, 2022, 3531,\n",
      "         2492, 1092, 1303, 5330, 3891, 2992,  766, 1939,  690, 4877, 1559, 2348,\n",
      "          197,   52, 2320,  806, 2137, 5183,  518, 6415,  824,  395,  529],\n",
      "        [   1,  667,  183,  427, 4931, 4515,  383,  127, 4004, 7008, 1099, 2971,\n",
      "         1460, 2081, 1614, 5265, 2630, 1079, 5505, 1199, 2008,   62, 1724, 6706,\n",
      "         1460, 1554, 2128,  742,  452, 7967, 1074, 3352,  862, 1342, 2072, 7917,\n",
      "          794,  380, 2052,  879, 1965, 1649, 5974, 1649, 2612, 3189, 5891, 1023,\n",
      "          416,  358, 2781,  145, 1723, 1659, 2938, 1906,  513,  957, 3266, 2834,\n",
      "         7124, 2112, 3046, 6730, 1717,  989, 7050, 1878, 2308,   25, 1936, 1154,\n",
      "         3612,  587, 4068, 7029,  169, 3412, 5170, 5163, 1640, 1213, 3753, 2472,\n",
      "         1213, 1044, 3035, 1113, 6615, 1063, 1472,  452,  197,  896, 3138,  439,\n",
      "          277,  711, 2035, 1217,   44, 1907, 5005, 1281, 1626,  441, 2954,  233,\n",
      "          210, 7176,  361,  961, 1119, 1437,  925, 3969, 3522, 1391, 1359,  718,\n",
      "         5415,  516, 2475, 5024,   42, 2188,  829,  269, 1249,  749, 4930, 2898,\n",
      "         1824, 1048, 2822,   74, 1585, 1227, 7317, 3419,  852,  600, 3209,  365,\n",
      "         4814, 2541, 1054, 5164, 1462, 1436,  401, 7612, 4203, 1383, 5827,  540,\n",
      "         6251,  913, 1212, 3902, 1081, 4888, 1622, 1322, 3445, 5657,  788, 1067,\n",
      "          475, 1615, 1710, 2788, 4594,  994,  603, 6322, 5661, 7699, 3936, 2681,\n",
      "         6660, 7329, 1575,  138,  296, 3051, 3906, 1483, 5301,    4, 1491]]), 'labels': tensor([[   1,   28,  425,  643,  481, 2329,  841,  144, 5480, 1528,  434,   98,\n",
      "          794, 3818, 4905,  435, 1665, 1354,  853, 1869, 5901, 1184, 7909,  664,\n",
      "         4766,   20,  357, 5219, 7122, 1484, 2861, 4958,  443, 7387, 1650, 1385,\n",
      "         2494, 1201, 3444, 1166, 3346, 2041, 1034,  302, 2817, 5065, 3014,  953,\n",
      "          864, 1296, 3378,  865,  589,  661, 3076, 1191, 6644, 2507,   86, 3464,\n",
      "          308, 1842,  672, 4501,   32, 5916, 3328,  862, 1782, 1561,   32, 1321,\n",
      "          680, 7365, 7170, 2108, 2438, 1479,  419, 4610,  762,   37, 3939, 2382,\n",
      "          159, 6184, 1541, 3693,  320, 3457,  346,  552, 2236, 4494, 2809, 2892,\n",
      "          817, 2169, 6473, 2224, 2074, 4095, 5720, 3350, 1689,  892, 4599, 3284,\n",
      "          850, 2144, 4824,  317, 1845, 4416, 1115, 3415, 1187, 3178, 1157, 6919,\n",
      "         3964, 3752,  932, 4710, 1584, 6405, 3557, 1531, 2983, 5914,  928, 6838,\n",
      "         7257,  263,  104, 1423, 1869, 1011,  778, 5890, 1412,  619,  403, 3467,\n",
      "         1016, 1475, 5699, 2724,   42,  792, 5948, 1623, 2916, 6502,  538,  912,\n",
      "         3671, 2882,  218,  145, 1034, 2768, 1014, 1369, 2761,    3, 2022, 3531,\n",
      "         2492, 1092, 1303, 5330, 3891, 2992,  766, 1939,  690, 4877, 1559, 2348,\n",
      "          197,   52, 2320,  806, 2137, 5183,  518, 6415,  824,  395,  529],\n",
      "        [   1,  667,  183,  427, 4931, 4515,  383,  127, 4004, 7008, 1099, 2971,\n",
      "         1460, 2081, 1614, 5265, 2630, 1079, 5505, 1199, 2008,   62, 1724, 6706,\n",
      "         1460, 1554, 2128,  742,  452, 7967, 1074, 3352,  862, 1342, 2072, 7917,\n",
      "          794,  380, 2052,  879, 1965, 1649, 5974, 1649, 2612, 3189, 5891, 1023,\n",
      "          416,  358, 2781,  145, 1723, 1659, 2938, 1906,  513,  957, 3266, 2834,\n",
      "         7124, 2112, 3046, 6730, 1717,  989, 7050, 1878, 2308,   25, 1936, 1154,\n",
      "         3612,  587, 4068, 7029,  169, 3412, 5170, 5163, 1640, 1213, 3753, 2472,\n",
      "         1213, 1044, 3035, 1113, 6615, 1063, 1472,  452,  197,  896, 3138,  439,\n",
      "          277,  711, 2035, 1217,   44, 1907, 5005, 1281, 1626,  441, 2954,  233,\n",
      "          210, 7176,  361,  961, 1119, 1437,  925, 3969, 3522, 1391, 1359,  718,\n",
      "         5415,  516, 2475, 5024,   42, 2188,  829,  269, 1249,  749, 4930, 2898,\n",
      "         1824, 1048, 2822,   74, 1585, 1227, 7317, 3419,  852,  600, 3209,  365,\n",
      "         4814, 2541, 1054, 5164, 1462, 1436,  401, 7612, 4203, 1383, 5827,  540,\n",
      "         6251,  913, 1212, 3902, 1081, 4888, 1622, 1322, 3445, 5657,  788, 1067,\n",
      "          475, 1615, 1710, 2788, 4594,  994,  603, 6322, 5661, 7699, 3936, 2681,\n",
      "         6660, 7329, 1575,  138,  296, 3051, 3906, 1483, 5301,    4, 1491]])}\n",
      "3 {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[   1,   28, 2321,  514,  561, 1289,  702,  495, 2762, 3921,  171, 1652,\n",
      "           34,  802,  961,   29,  492, 2122, 1324, 2131, 1554, 5040, 3060, 2877,\n",
      "          685, 1848,  928,  937, 4941, 2739, 3690, 3425,  392, 7477, 2214, 2106,\n",
      "         1854, 1987, 5807,  450, 1493, 1241, 2941,  361,  691, 2028,  492, 6679,\n",
      "          748, 4544,  229, 3967, 2387, 4470,  710,  660,  933,  176,  364, 7410,\n",
      "         4649,  803, 2486, 2297, 2030,  466, 2385, 5254, 1512, 1418, 3248, 2483,\n",
      "          364, 1808, 1796,  336,  423, 6562, 6562, 2982,  748, 3082,  905,  333,\n",
      "         1603, 2641, 1201, 1547, 1302,  130, 5460, 1039, 2971, 3357, 2585, 2393,\n",
      "         4415,  414,  717,  583, 1665, 1951, 3141, 4942, 3326,  209,  633, 6186,\n",
      "           66, 2123, 2257, 1426, 2598, 2486, 1260, 1324, 5258,  638, 1340,  748,\n",
      "         3957,  679, 1295, 4726,  288, 2222, 3739,  911,  459,  612, 1710, 1632,\n",
      "         2828,  949, 1215, 2652, 2965, 1573, 6856, 2937, 1326,   87,  722,  435,\n",
      "          848, 1785, 1233,  963, 1420, 3744, 1063, 1119, 3127, 1074, 2519, 2656,\n",
      "          526, 1380, 5201, 3414,  188, 1113, 1189,  828, 3545, 2893, 6681, 2248,\n",
      "          371, 4564, 2406, 6303, 2284, 3239, 1202, 4872, 1472, 2391, 2500,  129,\n",
      "         4229,  508, 2102, 2412, 3412, 5404,  171, 1805, 2265, 7535, 1079, 4429,\n",
      "          828, 1684,  526, 6691, 3138,   55,  788, 6385,  106, 3485, 3224, 6521,\n",
      "         2532,  391, 6766, 2891, 2714, 1285, 7305, 3259,  635, 2264, 1091,  698,\n",
      "          672,  311, 2205, 6358, 5468, 5827,  466,    3],\n",
      "        [   1,  252, 1552, 2911, 1804, 2075, 2301, 1258, 3541, 1045, 2831, 1083,\n",
      "            4, 6965,   26,  409, 1454,  917, 1056, 7617, 1875,   25, 4803, 3459,\n",
      "         2299,  524, 7206, 2859,  288,  860, 3506, 1924, 2965,  135, 3522, 7150,\n",
      "         1462, 2088, 2459, 2424, 5513, 4322,  463, 2473,  917, 4142, 1440, 2963,\n",
      "         7425, 1275, 1534,  881,  729, 1667, 4075, 3266, 3058, 5300, 1033, 1439,\n",
      "         7491,  183, 2280,  791, 3697, 1089, 2727, 4464, 1048,  926, 4707,  631,\n",
      "         3240,  467,  625, 1286,  125, 6919,  101,  429, 3018, 1079,  560, 4836,\n",
      "         4751, 6779,  311, 2017, 3397,  543, 4050, 2506,  759, 3573, 3916, 1564,\n",
      "         4243, 3876, 3241, 3400, 6128, 6929, 2233, 3467,  933,  750, 3051,  763,\n",
      "         3381, 4541,  676, 2053,  187, 2168, 1227, 7313,  287,  261,  287,  206,\n",
      "          478, 3610,  919, 2107, 1256, 5619, 7815,   89, 5837,  594, 3941, 3819,\n",
      "          257, 3172, 2926, 2632, 1800, 2674, 4278, 6165, 1086,  582,  984, 2360,\n",
      "         3067, 3535,  537, 2426, 1352,  229, 2160,  937,  386, 1857, 1293,  927,\n",
      "         2340, 7991,  715, 2856, 3934,  316, 2116, 1414,    3, 4446, 2735, 4426,\n",
      "         1455, 5565, 1356, 2091,  835, 3261,  899, 2693, 1348, 1667, 1133, 2924,\n",
      "          978, 2827,  985, 2611,    3,  844, 1728,  772, 6497, 2186, 1200, 2114,\n",
      "         7998, 7875, 2376, 4065, 6116, 3666,  970,  841,  581,  171, 4678, 1021,\n",
      "         2776, 1872,   95, 1311, 6226,  486, 1872, 1070, 4842, 7622,  867, 1098,\n",
      "         3590, 3115, 4715, 2670,  372,  744, 4323,   15]]), 'labels': tensor([[   1,   28, 2321,  514,  561, 1289,  702,  495, 2762, 3921,  171, 1652,\n",
      "           34,  802,  961,   29,  492, 2122, 1324, 2131, 1554, 5040, 3060, 2877,\n",
      "          685, 1848,  928,  937, 4941, 2739, 3690, 3425,  392, 7477, 2214, 2106,\n",
      "         1854, 1987, 5807,  450, 1493, 1241, 2941,  361,  691, 2028,  492, 6679,\n",
      "          748, 4544,  229, 3967, 2387, 4470,  710,  660,  933,  176,  364, 7410,\n",
      "         4649,  803, 2486, 2297, 2030,  466, 2385, 5254, 1512, 1418, 3248, 2483,\n",
      "          364, 1808, 1796,  336,  423, 6562, 6562, 2982,  748, 3082,  905,  333,\n",
      "         1603, 2641, 1201, 1547, 1302,  130, 5460, 1039, 2971, 3357, 2585, 2393,\n",
      "         4415,  414,  717,  583, 1665, 1951, 3141, 4942, 3326,  209,  633, 6186,\n",
      "           66, 2123, 2257, 1426, 2598, 2486, 1260, 1324, 5258,  638, 1340,  748,\n",
      "         3957,  679, 1295, 4726,  288, 2222, 3739,  911,  459,  612, 1710, 1632,\n",
      "         2828,  949, 1215, 2652, 2965, 1573, 6856, 2937, 1326,   87,  722,  435,\n",
      "          848, 1785, 1233,  963, 1420, 3744, 1063, 1119, 3127, 1074, 2519, 2656,\n",
      "          526, 1380, 5201, 3414,  188, 1113, 1189,  828, 3545, 2893, 6681, 2248,\n",
      "          371, 4564, 2406, 6303, 2284, 3239, 1202, 4872, 1472, 2391, 2500,  129,\n",
      "         4229,  508, 2102, 2412, 3412, 5404,  171, 1805, 2265, 7535, 1079, 4429,\n",
      "          828, 1684,  526, 6691, 3138,   55,  788, 6385,  106, 3485, 3224, 6521,\n",
      "         2532,  391, 6766, 2891, 2714, 1285, 7305, 3259,  635, 2264, 1091,  698,\n",
      "          672,  311, 2205, 6358, 5468, 5827,  466,    3],\n",
      "        [   1,  252, 1552, 2911, 1804, 2075, 2301, 1258, 3541, 1045, 2831, 1083,\n",
      "            4, 6965,   26,  409, 1454,  917, 1056, 7617, 1875,   25, 4803, 3459,\n",
      "         2299,  524, 7206, 2859,  288,  860, 3506, 1924, 2965,  135, 3522, 7150,\n",
      "         1462, 2088, 2459, 2424, 5513, 4322,  463, 2473,  917, 4142, 1440, 2963,\n",
      "         7425, 1275, 1534,  881,  729, 1667, 4075, 3266, 3058, 5300, 1033, 1439,\n",
      "         7491,  183, 2280,  791, 3697, 1089, 2727, 4464, 1048,  926, 4707,  631,\n",
      "         3240,  467,  625, 1286,  125, 6919,  101,  429, 3018, 1079,  560, 4836,\n",
      "         4751, 6779,  311, 2017, 3397,  543, 4050, 2506,  759, 3573, 3916, 1564,\n",
      "         4243, 3876, 3241, 3400, 6128, 6929, 2233, 3467,  933,  750, 3051,  763,\n",
      "         3381, 4541,  676, 2053,  187, 2168, 1227, 7313,  287,  261,  287,  206,\n",
      "          478, 3610,  919, 2107, 1256, 5619, 7815,   89, 5837,  594, 3941, 3819,\n",
      "          257, 3172, 2926, 2632, 1800, 2674, 4278, 6165, 1086,  582,  984, 2360,\n",
      "         3067, 3535,  537, 2426, 1352,  229, 2160,  937,  386, 1857, 1293,  927,\n",
      "         2340, 7991,  715, 2856, 3934,  316, 2116, 1414,    3, 4446, 2735, 4426,\n",
      "         1455, 5565, 1356, 2091,  835, 3261,  899, 2693, 1348, 1667, 1133, 2924,\n",
      "          978, 2827,  985, 2611,    3,  844, 1728,  772, 6497, 2186, 1200, 2114,\n",
      "         7998, 7875, 2376, 4065, 6116, 3666,  970,  841,  581,  171, 4678, 1021,\n",
      "         2776, 1872,   95, 1311, 6226,  486, 1872, 1070, 4842, 7622,  867, 1098,\n",
      "         3590, 3115, 4715, 2670,  372,  744, 4323,   15]])}\n",
      "4 {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[   1,  820,  913,  508,  398,  998, 5048, 7183, 4154,  359, 1084, 6002,\n",
      "         2395, 6218, 2849,  343,   60, 6066,  788, 5244, 3815,  641, 1205,   27,\n",
      "         1428, 5858, 6862,  884, 2088,  337,  570,  233, 7259, 6515, 6254, 1515,\n",
      "          343,  965,  362,  283, 4263, 1212,  123, 3569],\n",
      "        [   1,  820, 2303, 6106, 5525,  341,  168, 3634, 2631, 2993, 2178, 4621,\n",
      "         4678,  719,  743, 1207,  885,  343, 5187, 6536, 2521, 2934, 1929, 5842,\n",
      "          333,   33, 7059,  341,  550, 1973, 6494, 1453,  422, 2164, 5655, 2010,\n",
      "          708, 6512, 4200, 3976, 3115, 2070, 2635, 7548],\n",
      "        [   1,  667, 1265, 2223, 3870,  403,  555, 2655, 1510, 1315,  794,   98,\n",
      "         2059,  373,  489, 2248,  691,  509, 1327,  413, 2183, 3584, 4657, 4342,\n",
      "         1193, 1375,  753,  454, 1770, 1610, 1610,  683,  346, 1012,  698, 6129,\n",
      "         2301,  571, 6816,  488, 1375,  814,  866,  493],\n",
      "        [   1, 6550, 5506, 3160, 2745,   67,  928,  616, 5113,  132, 6687, 2497,\n",
      "         4918, 2684,   39,  892, 5317,  687, 6191, 3899, 1417, 3297, 3668, 1677,\n",
      "         4255, 1615, 1321, 1922, 3469, 1511,   97, 7122, 3122,  859,  919,  313,\n",
      "          201,  940,  947, 1897, 2002, 1184, 1033,   49],\n",
      "        [   1,   28,  651, 3282, 1713, 1247, 7217, 3463, 1254, 1474, 2146, 1657,\n",
      "         2706, 4474, 3480, 6580,   44, 4789, 2940, 1966, 2715, 6400, 2132, 7483,\n",
      "         1399,   63,  875, 2082,   66, 6981,  653, 1294, 1516, 3731, 2643, 3830,\n",
      "         1444,   40,   87, 4158, 2973,  570, 3838,  937],\n",
      "        [   1,   28, 6749, 6447,  535, 2132,  265, 3671, 1432,   85, 1912,  994,\n",
      "         1773, 4611, 4028,  980,  565, 3383,  164, 2992, 1883, 3733, 4009, 2965,\n",
      "          290, 7127,    6, 6862, 2378, 2157, 2247, 4021, 1551, 3434, 1011, 2341,\n",
      "         3880, 3735, 1347, 2911,  559, 2346,  393, 1233],\n",
      "        [   1,   28, 3653, 5485, 2150, 6396, 1365, 1611, 1321, 1676,  787, 6922,\n",
      "          900, 3706, 1124, 1543, 2467, 2135, 5709,  524, 2375, 1160, 3308,  708,\n",
      "         2272, 1094, 1296,  503,  242, 5581, 4763, 7507, 1418, 3498, 6407,  941,\n",
      "         1425, 2171, 3336, 4365,  683,  641,  163, 3654],\n",
      "        [   1,  252, 7988,  343,  503, 4885,  204,   37,  247, 3218,  432, 2717,\n",
      "         3715, 3087,  477, 2757, 1298, 3343,  281,  774, 6369, 1213,  453, 2083,\n",
      "          362, 4816, 3123, 3255, 5838, 6767,  618,  211, 6345,   15, 1707, 1130,\n",
      "         6674, 4743, 1341, 1877, 2033, 1020, 1589, 1553],\n",
      "        [   1,   28, 5290, 1259,  356, 4042, 1478,   70, 2403, 2553, 1964, 1036,\n",
      "          767,  262, 3424, 2422, 1259, 2299,  388,   47, 2576,  322,   15,  670,\n",
      "         5994, 1781, 3248, 1670, 2433, 1039, 7818, 4444, 2898, 1084,   43, 4253,\n",
      "         2126,  409,  286,  674,  383, 1340,  336, 3465],\n",
      "        [   1,  667, 1291, 2792,  158, 5780, 3501,  998, 3458, 1480, 1974, 1629,\n",
      "         3719, 1930,  695,  638, 2098, 2093,  177, 1909,  692, 1349,  875,  727,\n",
      "         1085, 1222, 4617,  905,  752, 2098,  838, 2857, 3288, 1703, 2038, 3635,\n",
      "         1004, 1641, 1693, 1202,  728,    6, 1025, 1149],\n",
      "        [   1, 3078, 1584, 2388,  733,  873, 3540, 4479, 7546, 4444,  555, 1737,\n",
      "          603,   21, 2033,  678, 2392,  722,  843, 1400, 2897, 1387, 1521,  774,\n",
      "         3762, 7458,  841, 1580, 1121, 4157,  613, 2171, 3109,  112, 6133, 1160,\n",
      "          461, 1800,  537,  423,   33, 2569,  364, 1204]]), 'labels': tensor([[   1,  820,  913,  508,  398,  998, 5048, 7183, 4154,  359, 1084, 6002,\n",
      "         2395, 6218, 2849,  343,   60, 6066,  788, 5244, 3815,  641, 1205,   27,\n",
      "         1428, 5858, 6862,  884, 2088,  337,  570,  233, 7259, 6515, 6254, 1515,\n",
      "          343,  965,  362,  283, 4263, 1212,  123, 3569],\n",
      "        [   1,  820, 2303, 6106, 5525,  341,  168, 3634, 2631, 2993, 2178, 4621,\n",
      "         4678,  719,  743, 1207,  885,  343, 5187, 6536, 2521, 2934, 1929, 5842,\n",
      "          333,   33, 7059,  341,  550, 1973, 6494, 1453,  422, 2164, 5655, 2010,\n",
      "          708, 6512, 4200, 3976, 3115, 2070, 2635, 7548],\n",
      "        [   1,  667, 1265, 2223, 3870,  403,  555, 2655, 1510, 1315,  794,   98,\n",
      "         2059,  373,  489, 2248,  691,  509, 1327,  413, 2183, 3584, 4657, 4342,\n",
      "         1193, 1375,  753,  454, 1770, 1610, 1610,  683,  346, 1012,  698, 6129,\n",
      "         2301,  571, 6816,  488, 1375,  814,  866,  493],\n",
      "        [   1, 6550, 5506, 3160, 2745,   67,  928,  616, 5113,  132, 6687, 2497,\n",
      "         4918, 2684,   39,  892, 5317,  687, 6191, 3899, 1417, 3297, 3668, 1677,\n",
      "         4255, 1615, 1321, 1922, 3469, 1511,   97, 7122, 3122,  859,  919,  313,\n",
      "          201,  940,  947, 1897, 2002, 1184, 1033,   49],\n",
      "        [   1,   28,  651, 3282, 1713, 1247, 7217, 3463, 1254, 1474, 2146, 1657,\n",
      "         2706, 4474, 3480, 6580,   44, 4789, 2940, 1966, 2715, 6400, 2132, 7483,\n",
      "         1399,   63,  875, 2082,   66, 6981,  653, 1294, 1516, 3731, 2643, 3830,\n",
      "         1444,   40,   87, 4158, 2973,  570, 3838,  937],\n",
      "        [   1,   28, 6749, 6447,  535, 2132,  265, 3671, 1432,   85, 1912,  994,\n",
      "         1773, 4611, 4028,  980,  565, 3383,  164, 2992, 1883, 3733, 4009, 2965,\n",
      "          290, 7127,    6, 6862, 2378, 2157, 2247, 4021, 1551, 3434, 1011, 2341,\n",
      "         3880, 3735, 1347, 2911,  559, 2346,  393, 1233],\n",
      "        [   1,   28, 3653, 5485, 2150, 6396, 1365, 1611, 1321, 1676,  787, 6922,\n",
      "          900, 3706, 1124, 1543, 2467, 2135, 5709,  524, 2375, 1160, 3308,  708,\n",
      "         2272, 1094, 1296,  503,  242, 5581, 4763, 7507, 1418, 3498, 6407,  941,\n",
      "         1425, 2171, 3336, 4365,  683,  641,  163, 3654],\n",
      "        [   1,  252, 7988,  343,  503, 4885,  204,   37,  247, 3218,  432, 2717,\n",
      "         3715, 3087,  477, 2757, 1298, 3343,  281,  774, 6369, 1213,  453, 2083,\n",
      "          362, 4816, 3123, 3255, 5838, 6767,  618,  211, 6345,   15, 1707, 1130,\n",
      "         6674, 4743, 1341, 1877, 2033, 1020, 1589, 1553],\n",
      "        [   1,   28, 5290, 1259,  356, 4042, 1478,   70, 2403, 2553, 1964, 1036,\n",
      "          767,  262, 3424, 2422, 1259, 2299,  388,   47, 2576,  322,   15,  670,\n",
      "         5994, 1781, 3248, 1670, 2433, 1039, 7818, 4444, 2898, 1084,   43, 4253,\n",
      "         2126,  409,  286,  674,  383, 1340,  336, 3465],\n",
      "        [   1,  667, 1291, 2792,  158, 5780, 3501,  998, 3458, 1480, 1974, 1629,\n",
      "         3719, 1930,  695,  638, 2098, 2093,  177, 1909,  692, 1349,  875,  727,\n",
      "         1085, 1222, 4617,  905,  752, 2098,  838, 2857, 3288, 1703, 2038, 3635,\n",
      "         1004, 1641, 1693, 1202,  728,    6, 1025, 1149],\n",
      "        [   1, 3078, 1584, 2388,  733,  873, 3540, 4479, 7546, 4444,  555, 1737,\n",
      "          603,   21, 2033,  678, 2392,  722,  843, 1400, 2897, 1387, 1521,  774,\n",
      "         3762, 7458,  841, 1580, 1121, 4157,  613, 2171, 3109,  112, 6133, 1160,\n",
      "          461, 1800,  537,  423,   33, 2569,  364, 1204]])}\n",
      "5 {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[   1,  438, 3434, 2157, 7517, 1279, 2090, 6246, 3077, 1066,  734, 2369,\n",
      "         2968, 4480, 1765, 5634, 2336, 2158,  960, 6131, 1402, 1919,  832,  953,\n",
      "         3836,  118, 2473, 2620, 2030, 1643, 5503, 6367, 1704,   47, 4966,  217,\n",
      "         3400, 2003,  118,  551, 2775, 2989, 2625,  119, 1901, 7799, 6021, 2110,\n",
      "         3620, 2188, 4845,  708, 7906, 3629, 2224, 1609, 1045, 1085, 5331, 1965,\n",
      "          670, 1378,  793,  566, 1867, 7401, 5548,  756, 3251,  103,  573, 2087,\n",
      "           52,   65,  873, 6454, 1654, 3556,    6, 3612,  612, 2623,  734,  538,\n",
      "         3151, 1060, 1947, 1132, 1292, 7302, 1177, 7830, 2621,  300, 1277,  947,\n",
      "         3220,  717, 7890, 3009, 2149,   16, 3487, 2059, 5846, 1561,  254,  378,\n",
      "          343,  121, 1500, 2774, 1378, 2307,  214,  324, 1127,  727, 1523, 1728,\n",
      "         1023, 1651, 7608,  932,   58, 4432,  547,  977, 7821,  669, 5006,  591,\n",
      "         1281, 4381, 2660, 3579, 2295, 2741, 1459,  921,  379, 2691, 2267],\n",
      "        [   1,  438, 1867, 1244, 2359, 1522, 5144,  905,  154,  312, 6240,   73,\n",
      "         1663, 2386, 2153, 2497,  533, 3590,  796, 5269,   34, 2145,  724, 5354,\n",
      "         1880, 2062, 4266, 3815, 5771, 5101, 1345, 1121,  809, 7928,  993, 3713,\n",
      "          509, 2989, 2158, 2288, 1599, 7536,  118, 1264, 4516, 7255, 1330,  237,\n",
      "         3116, 3216,  285,  489,  737,  431, 1920, 3650, 1295,  603, 1633,  964,\n",
      "         3458, 1212,  804, 1110, 1771,   41,  982, 1035,  507, 1827, 1194, 4278,\n",
      "         5028, 3811, 1508, 4874, 1982, 6413, 3180, 3738, 1599,  444, 4613, 3484,\n",
      "         1495, 2205,  247, 1036, 3837, 1253, 3101, 2709, 1744,  414, 2075, 4174,\n",
      "         2226,  277,  623,  756, 1674, 2490,  793,  398, 2061,  163, 2190, 7165,\n",
      "          333,   25,  282,  951, 2892, 1572, 1893,  810,  171, 5957,  598, 4046,\n",
      "          790, 6026,  714, 2442, 4905, 1178, 2211, 1695, 6691, 3214, 4688,  815,\n",
      "          495, 1964, 5919, 6300, 2825, 2126, 2933, 3347, 4800, 7191,  104],\n",
      "        [   1,  438, 3162, 1295, 1032, 1700, 1603,  308, 3233,  691,  887,  628,\n",
      "         5681,  972, 1934, 3091,  608, 6753, 1105, 1569, 7193,  383,  375, 1616,\n",
      "         2731, 2386, 3722,  471, 3796, 6672, 6589,  530,  585,  453, 3991, 7498,\n",
      "          730, 3281,  774,  112, 3804,   56, 6230,  308,  739, 1122, 2425, 1982,\n",
      "          464,  757, 1612,  559, 1236, 6038, 7380, 4732, 7317,  760, 1740, 2057,\n",
      "         1199,  364,  654, 1945, 5722,  512, 5276,  684, 5320, 1357, 5955, 1941,\n",
      "         1464,  365, 2673, 6384, 6791, 1528,  971, 1073, 3583, 1396, 1467, 3295,\n",
      "         1642, 6389, 7020, 1534,  364, 2800, 4232,  612, 1823, 2240,  578, 1317,\n",
      "         6093,  726, 1472,  541,  142, 1100, 3991, 3711, 2391, 3876,  402, 2712,\n",
      "         1661,  870,   42, 1911,  495, 3359, 1393, 7668, 1224,  643, 1956,  403,\n",
      "         1535,  669, 2119, 2388,  357, 4679, 3272,  885,  623, 1621,  731, 5082,\n",
      "         7114,  640,  492,  804, 2464, 1395, 1628,  885,  790, 5236, 1330]]), 'labels': tensor([[   1,  438, 3434, 2157, 7517, 1279, 2090, 6246, 3077, 1066,  734, 2369,\n",
      "         2968, 4480, 1765, 5634, 2336, 2158,  960, 6131, 1402, 1919,  832,  953,\n",
      "         3836,  118, 2473, 2620, 2030, 1643, 5503, 6367, 1704,   47, 4966,  217,\n",
      "         3400, 2003,  118,  551, 2775, 2989, 2625,  119, 1901, 7799, 6021, 2110,\n",
      "         3620, 2188, 4845,  708, 7906, 3629, 2224, 1609, 1045, 1085, 5331, 1965,\n",
      "          670, 1378,  793,  566, 1867, 7401, 5548,  756, 3251,  103,  573, 2087,\n",
      "           52,   65,  873, 6454, 1654, 3556,    6, 3612,  612, 2623,  734,  538,\n",
      "         3151, 1060, 1947, 1132, 1292, 7302, 1177, 7830, 2621,  300, 1277,  947,\n",
      "         3220,  717, 7890, 3009, 2149,   16, 3487, 2059, 5846, 1561,  254,  378,\n",
      "          343,  121, 1500, 2774, 1378, 2307,  214,  324, 1127,  727, 1523, 1728,\n",
      "         1023, 1651, 7608,  932,   58, 4432,  547,  977, 7821,  669, 5006,  591,\n",
      "         1281, 4381, 2660, 3579, 2295, 2741, 1459,  921,  379, 2691, 2267],\n",
      "        [   1,  438, 1867, 1244, 2359, 1522, 5144,  905,  154,  312, 6240,   73,\n",
      "         1663, 2386, 2153, 2497,  533, 3590,  796, 5269,   34, 2145,  724, 5354,\n",
      "         1880, 2062, 4266, 3815, 5771, 5101, 1345, 1121,  809, 7928,  993, 3713,\n",
      "          509, 2989, 2158, 2288, 1599, 7536,  118, 1264, 4516, 7255, 1330,  237,\n",
      "         3116, 3216,  285,  489,  737,  431, 1920, 3650, 1295,  603, 1633,  964,\n",
      "         3458, 1212,  804, 1110, 1771,   41,  982, 1035,  507, 1827, 1194, 4278,\n",
      "         5028, 3811, 1508, 4874, 1982, 6413, 3180, 3738, 1599,  444, 4613, 3484,\n",
      "         1495, 2205,  247, 1036, 3837, 1253, 3101, 2709, 1744,  414, 2075, 4174,\n",
      "         2226,  277,  623,  756, 1674, 2490,  793,  398, 2061,  163, 2190, 7165,\n",
      "          333,   25,  282,  951, 2892, 1572, 1893,  810,  171, 5957,  598, 4046,\n",
      "          790, 6026,  714, 2442, 4905, 1178, 2211, 1695, 6691, 3214, 4688,  815,\n",
      "          495, 1964, 5919, 6300, 2825, 2126, 2933, 3347, 4800, 7191,  104],\n",
      "        [   1,  438, 3162, 1295, 1032, 1700, 1603,  308, 3233,  691,  887,  628,\n",
      "         5681,  972, 1934, 3091,  608, 6753, 1105, 1569, 7193,  383,  375, 1616,\n",
      "         2731, 2386, 3722,  471, 3796, 6672, 6589,  530,  585,  453, 3991, 7498,\n",
      "          730, 3281,  774,  112, 3804,   56, 6230,  308,  739, 1122, 2425, 1982,\n",
      "          464,  757, 1612,  559, 1236, 6038, 7380, 4732, 7317,  760, 1740, 2057,\n",
      "         1199,  364,  654, 1945, 5722,  512, 5276,  684, 5320, 1357, 5955, 1941,\n",
      "         1464,  365, 2673, 6384, 6791, 1528,  971, 1073, 3583, 1396, 1467, 3295,\n",
      "         1642, 6389, 7020, 1534,  364, 2800, 4232,  612, 1823, 2240,  578, 1317,\n",
      "         6093,  726, 1472,  541,  142, 1100, 3991, 3711, 2391, 3876,  402, 2712,\n",
      "         1661,  870,   42, 1911,  495, 3359, 1393, 7668, 1224,  643, 1956,  403,\n",
      "         1535,  669, 2119, 2388,  357, 4679, 3272,  885,  623, 1621,  731, 5082,\n",
      "         7114,  640,  492,  804, 2464, 1395, 1628,  885,  790, 5236, 1330]])}\n",
      "6 {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]]), 'input_ids': tensor([[   1,  438, 3967, 3031, 2037,  644, 4525, 2853, 1595, 7655, 2122, 3882,\n",
      "         1948,  827, 3159,  233, 7655, 1388,  155,  402, 1702, 7473, 1425, 6283,\n",
      "           87, 2633, 7887, 1619, 5549, 1394, 4520, 2760, 1609,  776, 5306, 5830,\n",
      "         4472,  116, 1497,  700, 1339,  817, 2097, 1284, 3985, 1787,  453,  798,\n",
      "         1424,   16,  574,  373, 4954,  129,  352, 5230,  355,  322, 1892, 5268,\n",
      "          276,  462, 2340,  672, 2771, 2802,   83, 3842, 2067,  627, 2938,   91,\n",
      "          585,  262, 6913, 5515, 5746, 7924, 3154,  371, 2948,  581, 1380, 3107,\n",
      "         3767, 4351, 2564, 1598, 3932,   75,  831, 1049, 5327, 1058, 4681, 2163,\n",
      "         1704, 6553, 3401, 1015,  610,  572, 6446, 2597,  994, 5837, 5524,  415,\n",
      "         6872,  621,  735,  795, 3080, 6009, 3758, 1938, 6646,   51, 1397, 1023,\n",
      "         1676, 4256, 2426,  577, 1488, 1573, 1417,  885,  210,  626, 3796, 5908,\n",
      "         1104, 1312, 1971, 5161,  309, 6629, 5292, 2234,  799,  522, 7668, 4861,\n",
      "         3905,  117,  763, 6219,  183,  472, 6945, 4057, 6506,   39, 5424, 1354,\n",
      "         4799, 1052,  665,  641, 7602, 5994,  773, 4737, 3263,    3, 6532,  714,\n",
      "         7097, 4873],\n",
      "        [   1,   28,  730,  740, 1090, 1136,  456,  473,   53,   42, 1509, 1948,\n",
      "         1188, 1134, 1846, 3613, 2297,  724, 4365,  552,  285, 7945,   62,  763,\n",
      "          722, 2370, 1116, 4173,  262,  679, 1541, 1076, 2865, 4940, 1099,    3,\n",
      "          511, 4147,  871,  778, 3304, 7170, 5460,  127, 1553, 2108, 2105, 2914,\n",
      "          983,  650, 3314, 2623, 3387, 4530,  609,  524,  669,  271, 1657, 1808,\n",
      "           64, 6392, 7275,  829, 7329, 7441, 7915, 1197, 5498, 2501, 1270,  380,\n",
      "          691, 2264, 3666, 1374,  220, 7253, 4945, 2410,  567,  258,  269, 1990,\n",
      "         4138, 1195, 1421, 1126, 2807, 2976, 1573, 5702,  577,  483, 2485, 3873,\n",
      "         3253, 3953, 1706, 6510, 4490, 1884,  189, 5677, 2305, 2279, 1288, 3747,\n",
      "          676, 2067,  111, 1289, 3937, 3891,  855, 1626,  590,   86, 4889,  217,\n",
      "          177, 1776,  385, 1193, 5198, 4376, 4843, 2571, 1813,  968, 7224, 2886,\n",
      "         2930, 2786, 5226, 1809, 1753, 1953, 1456, 3478, 3170, 2104, 4041,  598,\n",
      "          850, 1919, 1419,  706, 2453,  384,  746,  871,   55,  381, 1433, 1942,\n",
      "         5937, 2755, 2674,   96, 1546,  830, 7247,   65, 3566, 5072, 1244, 4582,\n",
      "          519, 5604]]), 'labels': tensor([[   1,  438, 3967, 3031, 2037,  644, 4525, 2853, 1595, 7655, 2122, 3882,\n",
      "         1948,  827, 3159,  233, 7655, 1388,  155,  402, 1702, 7473, 1425, 6283,\n",
      "           87, 2633, 7887, 1619, 5549, 1394, 4520, 2760, 1609,  776, 5306, 5830,\n",
      "         4472,  116, 1497,  700, 1339,  817, 2097, 1284, 3985, 1787,  453,  798,\n",
      "         1424,   16,  574,  373, 4954,  129,  352, 5230,  355,  322, 1892, 5268,\n",
      "          276,  462, 2340,  672, 2771, 2802,   83, 3842, 2067,  627, 2938,   91,\n",
      "          585,  262, 6913, 5515, 5746, 7924, 3154,  371, 2948,  581, 1380, 3107,\n",
      "         3767, 4351, 2564, 1598, 3932,   75,  831, 1049, 5327, 1058, 4681, 2163,\n",
      "         1704, 6553, 3401, 1015,  610,  572, 6446, 2597,  994, 5837, 5524,  415,\n",
      "         6872,  621,  735,  795, 3080, 6009, 3758, 1938, 6646,   51, 1397, 1023,\n",
      "         1676, 4256, 2426,  577, 1488, 1573, 1417,  885,  210,  626, 3796, 5908,\n",
      "         1104, 1312, 1971, 5161,  309, 6629, 5292, 2234,  799,  522, 7668, 4861,\n",
      "         3905,  117,  763, 6219,  183,  472, 6945, 4057, 6506,   39, 5424, 1354,\n",
      "         4799, 1052,  665,  641, 7602, 5994,  773, 4737, 3263,    3, 6532,  714,\n",
      "         7097, 4873],\n",
      "        [   1,   28,  730,  740, 1090, 1136,  456,  473,   53,   42, 1509, 1948,\n",
      "         1188, 1134, 1846, 3613, 2297,  724, 4365,  552,  285, 7945,   62,  763,\n",
      "          722, 2370, 1116, 4173,  262,  679, 1541, 1076, 2865, 4940, 1099,    3,\n",
      "          511, 4147,  871,  778, 3304, 7170, 5460,  127, 1553, 2108, 2105, 2914,\n",
      "          983,  650, 3314, 2623, 3387, 4530,  609,  524,  669,  271, 1657, 1808,\n",
      "           64, 6392, 7275,  829, 7329, 7441, 7915, 1197, 5498, 2501, 1270,  380,\n",
      "          691, 2264, 3666, 1374,  220, 7253, 4945, 2410,  567,  258,  269, 1990,\n",
      "         4138, 1195, 1421, 1126, 2807, 2976, 1573, 5702,  577,  483, 2485, 3873,\n",
      "         3253, 3953, 1706, 6510, 4490, 1884,  189, 5677, 2305, 2279, 1288, 3747,\n",
      "          676, 2067,  111, 1289, 3937, 3891,  855, 1626,  590,   86, 4889,  217,\n",
      "          177, 1776,  385, 1193, 5198, 4376, 4843, 2571, 1813,  968, 7224, 2886,\n",
      "         2930, 2786, 5226, 1809, 1753, 1953, 1456, 3478, 3170, 2104, 4041,  598,\n",
      "          850, 1919, 1419,  706, 2453,  384,  746,  871,   55,  381, 1433, 1942,\n",
      "         5937, 2755, 2674,   96, 1546,  830, 7247,   65, 3566, 5072, 1244, 4582,\n",
      "          519, 5604]])}\n",
      "7 {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[   1,  751,  855,  217, 2630, 1881, 3427, 4958, 1496, 1530, 2928, 1378,\n",
      "         2683, 1126, 4691,  996,  841, 2491, 4510, 1366, 3253, 5474,  482,  475,\n",
      "         5959, 2726,  325,  345, 4674,  828,  850, 2479, 6148, 4195,  209,  701,\n",
      "         6150, 1222,  905,  351,  840, 1552, 2412, 6531,  606, 5471, 4757, 1355,\n",
      "          224, 3629, 4623, 5882,  949, 7967, 1034,  607,  823,  386,   27,  652,\n",
      "         5514, 2947, 1477,  461, 4314, 3804,  249, 1781,  447, 3170,  869, 2308,\n",
      "         1755, 1936, 4467, 7093,  364, 1830,  819, 5489,  480, 2022, 1424,  905,\n",
      "          681,  861, 3684, 1439, 6446, 2226, 2442, 6190, 4745, 3887, 7257, 3268,\n",
      "          250, 2178,  485,  124, 7774, 1173,  923, 5284, 2786,  498, 1494,  360,\n",
      "          527, 2294, 4307, 6773, 7596,   57, 2367,  671,  881, 1322,  648, 2144,\n",
      "         3081, 2131, 1027,  518, 2447, 3157, 5483, 6527, 2761, 5259, 4470,   40,\n",
      "         1045,   53, 6417,   40,  739,  521, 1515, 2478, 2915, 2293,  800, 1060,\n",
      "         5948, 4110,   53, 1120, 1607, 2884,  514, 7507,  153, 3389, 2426,   37,\n",
      "          990, 1269, 3298, 4042, 3278, 3391,  859, 1865,  986, 4061,  485, 5034,\n",
      "            9,  345, 2148,  144, 1706, 1332,  851, 5272,  507, 4777,  639, 2370,\n",
      "         1376, 1706, 1165,  431, 5666,  313, 1321, 1907, 7629, 2438, 1018,  165,\n",
      "         1768, 1700, 3495, 6750, 4205, 1091, 1548, 2443,  669, 1556,  883,  818,\n",
      "         1932, 7387,  550,    5,  909, 3176, 6706,  235, 1705,  637, 2185, 3505,\n",
      "           54, 2521, 2384,  132, 3264, 1278,  749,  519, 5060, 3883,  223, 6629,\n",
      "         1891, 1830, 4373,  542,   16,  559,  701, 3037, 1932, 6282, 5494, 1290,\n",
      "         4589,  589, 1056, 4412, 7332, 4080,    6,  341, 1573, 6438, 1970, 4163,\n",
      "         3108, 3608, 1266, 7943, 1621,  970,   62, 1342,  240, 2371,  674, 1276,\n",
      "          823, 1589, 5783, 2204,  698, 4603,   39,  938, 2208, 2806, 1825, 7305,\n",
      "         6808, 6205, 1424,  251, 2016, 1878, 2263, 2319,  247,  925, 1174, 1656,\n",
      "         1884,  558, 4784,  213, 4277, 1109,  465,  788, 7230,  683,  736,  732,\n",
      "           16, 3608, 1619,  141, 2023,  596, 6075, 1348, 6035, 2641,  578,  268,\n",
      "           15,  524, 1585, 1789, 1229, 7187,  524,   91,  580, 7583, 3004,  298,\n",
      "          354, 6055,   88,  702, 6111,  409, 6372, 3162, 3259, 3791, 4687, 2395,\n",
      "         7882,   26, 7681,  149,   32, 6110, 1944, 1228, 2592,   41, 7666,  328,\n",
      "          389, 2593, 6585, 4728, 3908,  733, 2879, 1784, 2660, 5664,  707, 4646,\n",
      "         2328, 3114, 3534,  562,   67, 2532,  595, 7527,  364, 2903, 1064, 1160,\n",
      "         1622, 2042, 2608, 1622,  802,  440,  418, 6708,  411, 5478, 1390,  927,\n",
      "         2016,  482, 3820, 3447, 7464, 1931, 6325, 1859, 6760, 1506, 1780,  859,\n",
      "         7091,  493, 2693, 1726, 2747,   27,  796, 1993, 2616, 5090,  575, 1983,\n",
      "         7878, 6619, 2606,  564,   46, 6336, 4535, 4435, 2323, 2291, 1293,  846,\n",
      "         1737,  334,   42, 2115,  690, 1686,  690, 3573,  889, 2863, 2960, 5288,\n",
      "         6919, 1575,  466, 5678, 4465, 5072, 5180, 2209, 3739, 1062, 1561,  431,\n",
      "         1876,  702, 2039, 2216,  276, 4652,   96, 3176,  683, 3034,  684,  731,\n",
      "         7620, 3148, 2159, 1877,  516,  204, 1758, 1476, 1819, 2746, 1945, 2406,\n",
      "          853,  695, 1684,  795,  565, 1443, 1692, 1945, 5395, 5282,  855, 1223,\n",
      "         3511, 2982, 4654, 1677, 6411,  564, 4463, 5277, 2243, 4609, 4522, 1417,\n",
      "         4251,   25,  553,  841, 2736,  817, 5338,  856,  384, 4174, 3886,  681,\n",
      "           31, 5574, 6978, 3292, 4436, 2397, 5454, 2552]]), 'labels': tensor([[   1,  751,  855,  217, 2630, 1881, 3427, 4958, 1496, 1530, 2928, 1378,\n",
      "         2683, 1126, 4691,  996,  841, 2491, 4510, 1366, 3253, 5474,  482,  475,\n",
      "         5959, 2726,  325,  345, 4674,  828,  850, 2479, 6148, 4195,  209,  701,\n",
      "         6150, 1222,  905,  351,  840, 1552, 2412, 6531,  606, 5471, 4757, 1355,\n",
      "          224, 3629, 4623, 5882,  949, 7967, 1034,  607,  823,  386,   27,  652,\n",
      "         5514, 2947, 1477,  461, 4314, 3804,  249, 1781,  447, 3170,  869, 2308,\n",
      "         1755, 1936, 4467, 7093,  364, 1830,  819, 5489,  480, 2022, 1424,  905,\n",
      "          681,  861, 3684, 1439, 6446, 2226, 2442, 6190, 4745, 3887, 7257, 3268,\n",
      "          250, 2178,  485,  124, 7774, 1173,  923, 5284, 2786,  498, 1494,  360,\n",
      "          527, 2294, 4307, 6773, 7596,   57, 2367,  671,  881, 1322,  648, 2144,\n",
      "         3081, 2131, 1027,  518, 2447, 3157, 5483, 6527, 2761, 5259, 4470,   40,\n",
      "         1045,   53, 6417,   40,  739,  521, 1515, 2478, 2915, 2293,  800, 1060,\n",
      "         5948, 4110,   53, 1120, 1607, 2884,  514, 7507,  153, 3389, 2426,   37,\n",
      "          990, 1269, 3298, 4042, 3278, 3391,  859, 1865,  986, 4061,  485, 5034,\n",
      "            9,  345, 2148,  144, 1706, 1332,  851, 5272,  507, 4777,  639, 2370,\n",
      "         1376, 1706, 1165,  431, 5666,  313, 1321, 1907, 7629, 2438, 1018,  165,\n",
      "         1768, 1700, 3495, 6750, 4205, 1091, 1548, 2443,  669, 1556,  883,  818,\n",
      "         1932, 7387,  550,    5,  909, 3176, 6706,  235, 1705,  637, 2185, 3505,\n",
      "           54, 2521, 2384,  132, 3264, 1278,  749,  519, 5060, 3883,  223, 6629,\n",
      "         1891, 1830, 4373,  542,   16,  559,  701, 3037, 1932, 6282, 5494, 1290,\n",
      "         4589,  589, 1056, 4412, 7332, 4080,    6,  341, 1573, 6438, 1970, 4163,\n",
      "         3108, 3608, 1266, 7943, 1621,  970,   62, 1342,  240, 2371,  674, 1276,\n",
      "          823, 1589, 5783, 2204,  698, 4603,   39,  938, 2208, 2806, 1825, 7305,\n",
      "         6808, 6205, 1424,  251, 2016, 1878, 2263, 2319,  247,  925, 1174, 1656,\n",
      "         1884,  558, 4784,  213, 4277, 1109,  465,  788, 7230,  683,  736,  732,\n",
      "           16, 3608, 1619,  141, 2023,  596, 6075, 1348, 6035, 2641,  578,  268,\n",
      "           15,  524, 1585, 1789, 1229, 7187,  524,   91,  580, 7583, 3004,  298,\n",
      "          354, 6055,   88,  702, 6111,  409, 6372, 3162, 3259, 3791, 4687, 2395,\n",
      "         7882,   26, 7681,  149,   32, 6110, 1944, 1228, 2592,   41, 7666,  328,\n",
      "          389, 2593, 6585, 4728, 3908,  733, 2879, 1784, 2660, 5664,  707, 4646,\n",
      "         2328, 3114, 3534,  562,   67, 2532,  595, 7527,  364, 2903, 1064, 1160,\n",
      "         1622, 2042, 2608, 1622,  802,  440,  418, 6708,  411, 5478, 1390,  927,\n",
      "         2016,  482, 3820, 3447, 7464, 1931, 6325, 1859, 6760, 1506, 1780,  859,\n",
      "         7091,  493, 2693, 1726, 2747,   27,  796, 1993, 2616, 5090,  575, 1983,\n",
      "         7878, 6619, 2606,  564,   46, 6336, 4535, 4435, 2323, 2291, 1293,  846,\n",
      "         1737,  334,   42, 2115,  690, 1686,  690, 3573,  889, 2863, 2960, 5288,\n",
      "         6919, 1575,  466, 5678, 4465, 5072, 5180, 2209, 3739, 1062, 1561,  431,\n",
      "         1876,  702, 2039, 2216,  276, 4652,   96, 3176,  683, 3034,  684,  731,\n",
      "         7620, 3148, 2159, 1877,  516,  204, 1758, 1476, 1819, 2746, 1945, 2406,\n",
      "          853,  695, 1684,  795,  565, 1443, 1692, 1945, 5395, 5282,  855, 1223,\n",
      "         3511, 2982, 4654, 1677, 6411,  564, 4463, 5277, 2243, 4609, 4522, 1417,\n",
      "         4251,   25,  553,  841, 2736,  817, 5338,  856,  384, 4174, 3886,  681,\n",
      "           31, 5574, 6978, 3292, 4436, 2397, 5454, 2552]])}\n",
      "8 {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]]), 'input_ids': tensor([[   1,   28, 6453,  680,   22, 1880, 1565, 7397,   21,  467,  172, 4011,\n",
      "         2840,   65, 1716, 4144, 5341, 7153,   95, 1302, 3134, 1218,   40,  569,\n",
      "         3901, 4377, 7080,  671,  839, 3562, 5169, 1489,  788, 3453, 1937, 2937,\n",
      "          917, 2994, 5936, 3429,   61, 2597,  192, 4080, 1778, 3501, 7105, 1284,\n",
      "         6087],\n",
      "        [   1,  820,   80, 7064, 3923, 4372,  320, 6875, 7776, 4440,  271,  367,\n",
      "          262, 2421, 1530, 1618, 1060,  618, 1038, 2575, 4182, 1162, 2899,  595,\n",
      "          253, 4150, 4739, 5450, 7959, 5157, 6346, 3391,  978, 1253, 6479, 1141,\n",
      "          569, 3869, 1097, 1741, 5074, 6452, 7739,  334, 4875, 6081, 4308, 1212,\n",
      "         1167],\n",
      "        [   1,  390,  446,  476, 2412,  329, 1170, 2164, 2501, 5297, 2520, 3721,\n",
      "         5549, 2832,  770, 7088, 2869, 4413,  621, 1395,  780, 1160, 1486,  258,\n",
      "         3808, 1937,  959, 3733, 2714,  889, 1910, 4976,  650,  310,  871, 2465,\n",
      "         2343, 1210,  546,  480,  467, 3437, 7975,  650, 4941,   47,  970, 7840,\n",
      "           69],\n",
      "        [   1, 3078, 3020, 2102, 3616, 4687, 3915, 4859,   61, 7021,  973, 2545,\n",
      "         2532, 6420, 5078,  174,  992, 1651, 2104, 4456, 4350,  785, 1979,   85,\n",
      "         1077, 1995, 5383, 6986, 5160, 5274,  429,  863, 3349, 1607,  383,  596,\n",
      "         1077,  933, 1515,    6, 2183, 3906, 3372, 4913,  644, 2185, 6256, 6112,\n",
      "          796],\n",
      "        [   1,  886, 2741, 3274,   58,  412,  588,   47, 6804, 2403,  145, 4424,\n",
      "         2965, 3697, 1187, 5727, 4186, 1941, 7871, 4077, 3459, 2420, 4931, 1487,\n",
      "         7903, 5315, 1220, 1653, 4119,  309,  726, 1356, 2087, 2357,  533,  361,\n",
      "         5701,  689,  378, 4889, 3398, 1445, 3697, 3574, 7881, 5323,  333, 2548,\n",
      "          395],\n",
      "        [   1,  260, 2607,  834,  349, 5409, 6418,  888,   21, 1247, 2745,  682,\n",
      "          224, 2038,  884, 3371, 2836,   39, 4109, 4946, 2359,   50,  498,   32,\n",
      "          892, 1241,   57, 2351, 2164,    6, 5959, 2300, 2991, 2220,  650, 5385,\n",
      "         1463,  337, 1816, 4961, 2861,  444,  516,  787, 2535,   59, 1015,  705,\n",
      "          395],\n",
      "        [   1,   28,  253,  986, 4484, 1477, 1451, 3395,  442, 2673, 1135, 1387,\n",
      "         1753, 3334,  811,  607, 1813,  624, 3739, 5673, 2284, 1481,  325,   88,\n",
      "          276, 1423,  837,  798,   61, 1717, 3282, 7161, 4641,  466,  503, 1295,\n",
      "         2178, 7566, 3086,  565, 2620,   43, 3166, 1253, 6649, 7219, 2632, 5411,\n",
      "         2761],\n",
      "        [   1,  390, 4496,  701,  465, 3589,  389,  816,  377, 1051, 3079,  360,\n",
      "         2144, 2073, 2852, 4445, 1770,  209,  526, 2056, 1546, 4386,  553, 1767,\n",
      "         3784, 1017, 2394, 1127, 4836,  888, 1310, 3268, 1428, 7126, 2287,  107,\n",
      "          636, 2194, 1620, 2763, 1211, 6391,  620, 4312, 2536,  730, 1136, 4439,\n",
      "         4809],\n",
      "        [   1,  260, 6524,  351,  579, 4149, 1110,  169, 5018,  519, 1225, 2944,\n",
      "         1034, 2241,  573, 1788,  101,  339,  348,  913, 1823, 2956,  217, 3258,\n",
      "          368, 6056,  869, 3740,  297, 2320, 6386,  955,  761,  360, 1719, 3396,\n",
      "         5062,  123, 5822, 1873,  223,  788, 4514,  836,  437, 1157, 5403,  467,\n",
      "         2695],\n",
      "        [   1,  252, 1057, 4695, 7152, 2314,   31,  597, 1678,  739, 5682, 1810,\n",
      "          191,  612,   66, 7621, 1708, 7091, 6328,   27,   25, 1133,  741, 5691,\n",
      "         2227, 3296, 4949,    3, 1633,  625, 7528, 3659, 7814, 1689,  586, 3808,\n",
      "         1996, 3191, 1745,  427, 2256,  427,   52,  324, 5150,  816, 1547, 1520,\n",
      "          453]]), 'labels': tensor([[   1,   28, 6453,  680,   22, 1880, 1565, 7397,   21,  467,  172, 4011,\n",
      "         2840,   65, 1716, 4144, 5341, 7153,   95, 1302, 3134, 1218,   40,  569,\n",
      "         3901, 4377, 7080,  671,  839, 3562, 5169, 1489,  788, 3453, 1937, 2937,\n",
      "          917, 2994, 5936, 3429,   61, 2597,  192, 4080, 1778, 3501, 7105, 1284,\n",
      "         6087],\n",
      "        [   1,  820,   80, 7064, 3923, 4372,  320, 6875, 7776, 4440,  271,  367,\n",
      "          262, 2421, 1530, 1618, 1060,  618, 1038, 2575, 4182, 1162, 2899,  595,\n",
      "          253, 4150, 4739, 5450, 7959, 5157, 6346, 3391,  978, 1253, 6479, 1141,\n",
      "          569, 3869, 1097, 1741, 5074, 6452, 7739,  334, 4875, 6081, 4308, 1212,\n",
      "         1167],\n",
      "        [   1,  390,  446,  476, 2412,  329, 1170, 2164, 2501, 5297, 2520, 3721,\n",
      "         5549, 2832,  770, 7088, 2869, 4413,  621, 1395,  780, 1160, 1486,  258,\n",
      "         3808, 1937,  959, 3733, 2714,  889, 1910, 4976,  650,  310,  871, 2465,\n",
      "         2343, 1210,  546,  480,  467, 3437, 7975,  650, 4941,   47,  970, 7840,\n",
      "           69],\n",
      "        [   1, 3078, 3020, 2102, 3616, 4687, 3915, 4859,   61, 7021,  973, 2545,\n",
      "         2532, 6420, 5078,  174,  992, 1651, 2104, 4456, 4350,  785, 1979,   85,\n",
      "         1077, 1995, 5383, 6986, 5160, 5274,  429,  863, 3349, 1607,  383,  596,\n",
      "         1077,  933, 1515,    6, 2183, 3906, 3372, 4913,  644, 2185, 6256, 6112,\n",
      "          796],\n",
      "        [   1,  886, 2741, 3274,   58,  412,  588,   47, 6804, 2403,  145, 4424,\n",
      "         2965, 3697, 1187, 5727, 4186, 1941, 7871, 4077, 3459, 2420, 4931, 1487,\n",
      "         7903, 5315, 1220, 1653, 4119,  309,  726, 1356, 2087, 2357,  533,  361,\n",
      "         5701,  689,  378, 4889, 3398, 1445, 3697, 3574, 7881, 5323,  333, 2548,\n",
      "          395],\n",
      "        [   1,  260, 2607,  834,  349, 5409, 6418,  888,   21, 1247, 2745,  682,\n",
      "          224, 2038,  884, 3371, 2836,   39, 4109, 4946, 2359,   50,  498,   32,\n",
      "          892, 1241,   57, 2351, 2164,    6, 5959, 2300, 2991, 2220,  650, 5385,\n",
      "         1463,  337, 1816, 4961, 2861,  444,  516,  787, 2535,   59, 1015,  705,\n",
      "          395],\n",
      "        [   1,   28,  253,  986, 4484, 1477, 1451, 3395,  442, 2673, 1135, 1387,\n",
      "         1753, 3334,  811,  607, 1813,  624, 3739, 5673, 2284, 1481,  325,   88,\n",
      "          276, 1423,  837,  798,   61, 1717, 3282, 7161, 4641,  466,  503, 1295,\n",
      "         2178, 7566, 3086,  565, 2620,   43, 3166, 1253, 6649, 7219, 2632, 5411,\n",
      "         2761],\n",
      "        [   1,  390, 4496,  701,  465, 3589,  389,  816,  377, 1051, 3079,  360,\n",
      "         2144, 2073, 2852, 4445, 1770,  209,  526, 2056, 1546, 4386,  553, 1767,\n",
      "         3784, 1017, 2394, 1127, 4836,  888, 1310, 3268, 1428, 7126, 2287,  107,\n",
      "          636, 2194, 1620, 2763, 1211, 6391,  620, 4312, 2536,  730, 1136, 4439,\n",
      "         4809],\n",
      "        [   1,  260, 6524,  351,  579, 4149, 1110,  169, 5018,  519, 1225, 2944,\n",
      "         1034, 2241,  573, 1788,  101,  339,  348,  913, 1823, 2956,  217, 3258,\n",
      "          368, 6056,  869, 3740,  297, 2320, 6386,  955,  761,  360, 1719, 3396,\n",
      "         5062,  123, 5822, 1873,  223,  788, 4514,  836,  437, 1157, 5403,  467,\n",
      "         2695],\n",
      "        [   1,  252, 1057, 4695, 7152, 2314,   31,  597, 1678,  739, 5682, 1810,\n",
      "          191,  612,   66, 7621, 1708, 7091, 6328,   27,   25, 1133,  741, 5691,\n",
      "         2227, 3296, 4949,    3, 1633,  625, 7528, 3659, 7814, 1689,  586, 3808,\n",
      "         1996, 3191, 1745,  427, 2256,  427,   52,  324, 5150,  816, 1547, 1520,\n",
      "          453]])}\n",
      "9 {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[   1,  252, 2448,  322, 2548, 1447,  700,  393,  756, 3601, 5972, 1739,\n",
      "          901, 2215, 1292,  287, 1219,  989, 6094,  928,  322, 1984, 1839,   83,\n",
      "         3678, 2752, 4697, 5776, 3200,   47, 1802, 2047, 1309,   82, 5076,  552,\n",
      "         3502, 1864,  871, 4538, 2236, 1689, 7283, 2391,  914, 6851,  971, 3847,\n",
      "         1658,  808,  871, 1714, 1048, 3512, 1542, 2340, 1923, 7950, 1234, 1068,\n",
      "         2594, 2250, 2353, 4228, 6540,  679, 1352, 2474, 1637,   22, 2467, 2880,\n",
      "         1039, 3372, 5574, 2538, 1451, 3265, 2146, 3724, 1284, 1356, 3160,  107,\n",
      "          763, 1109, 3183, 1116, 2139,  785, 2500, 1214, 2184, 1128, 4757, 4125,\n",
      "         1131,   26, 4087,  428, 2951, 2347, 1161,  388,  647, 7583,  625, 1762,\n",
      "         1708, 7135, 4529, 4424, 2968, 3529,  374, 2581,  191, 3397, 2882, 1950,\n",
      "         7919,  499, 4320, 3517, 5354, 2955, 3494,  903, 3280,  539, 7703, 6277,\n",
      "         1053,  373, 2033, 2482,  947, 3198, 1631, 1206, 1732, 5663,   25,  380,\n",
      "         1994,  492, 5350,  595,  443,  922, 7953, 4667,  217,  346, 3140, 3193,\n",
      "           30, 2567, 1698, 3900, 6880,  933, 1848,  793,  317, 3810,   54,  232,\n",
      "         3791, 2670, 1954, 1659, 2566, 5826, 2670, 1118, 2674, 2903, 3955, 1075,\n",
      "         1317,  406, 2158, 1380,   24, 3685, 5946,  721,  362,  793, 1872, 1511,\n",
      "         3632,  416, 5327, 2389,    3, 3779,  125, 1380, 3066, 5993,  183, 3026,\n",
      "         1761, 1670, 5368,  784, 1036, 6462, 1827, 2817,  555, 1256, 4201, 3995,\n",
      "          842, 1666,  726, 2282, 7013, 1358, 1296,  383, 2810, 1050, 1942, 2412],\n",
      "        [   1,  252, 2281,  326,  883, 1261, 2581, 1143,  776, 2936, 2467, 4494,\n",
      "         1906, 4745,   87,  258, 1200, 4555, 4555, 5092,  768, 3331,  803, 1861,\n",
      "         2185,  189, 1952, 6817, 7962, 1877, 3331,  239,  519, 6137,  572, 1823,\n",
      "         1189, 1581, 2070, 3031, 1881, 2176, 2072, 5513,   80, 1321, 3990,  976,\n",
      "         2781,  581,  578,  514, 2427,  794, 4555, 1105,  829, 2282, 1790,  566,\n",
      "         4466,  496, 2220, 5306, 2116, 1352, 2029, 2807, 1606, 3385,  863,  107,\n",
      "          961, 1043, 2648, 6007, 2215,  793, 4637,  741, 4849, 1719, 4524, 2460,\n",
      "         1067, 1794, 1461, 3585, 1510, 6820,  509, 5268, 2930,  409, 3769, 2208,\n",
      "         1577, 7240,  739, 4279, 6148, 1297,  136, 1909, 2914, 7211, 4755, 3313,\n",
      "         2928, 4053, 7194, 4905, 3081, 3916, 1912, 1668,  885, 3066,  997, 1565,\n",
      "         2357, 5253, 3166, 1354,  742, 4306,   61, 4070, 2430,  976,  256,  453,\n",
      "         1836, 2291, 2223,  575,  900, 1091, 3218, 2611, 4611,  557, 3357, 1526,\n",
      "         1176,  109, 1894, 4384, 3352,  494, 4340,  310, 2022, 3627, 1291, 1700,\n",
      "          614, 2568, 2124, 1929, 1969, 3437, 3467, 2700, 3259, 1599, 2302,  686,\n",
      "         4573,  509,  725, 5244, 6126, 2647, 1877,  845, 1877, 1552, 4880, 2263,\n",
      "         3073, 1253, 1156, 6368,  395, 4171, 3397, 4554,  282, 5591, 1345,  644,\n",
      "          802, 3805, 3792, 3434, 3953,  776, 3234, 1009, 1403, 6756, 2083, 3791,\n",
      "         1361,  129, 1877, 2419, 3050, 3004, 1553, 4552, 1239, 1716, 1477, 4238,\n",
      "         3125, 5266, 7945, 5489, 1684,  827, 3156, 3558,    6, 3485, 1589, 3212]]), 'labels': tensor([[   1,  252, 2448,  322, 2548, 1447,  700,  393,  756, 3601, 5972, 1739,\n",
      "          901, 2215, 1292,  287, 1219,  989, 6094,  928,  322, 1984, 1839,   83,\n",
      "         3678, 2752, 4697, 5776, 3200,   47, 1802, 2047, 1309,   82, 5076,  552,\n",
      "         3502, 1864,  871, 4538, 2236, 1689, 7283, 2391,  914, 6851,  971, 3847,\n",
      "         1658,  808,  871, 1714, 1048, 3512, 1542, 2340, 1923, 7950, 1234, 1068,\n",
      "         2594, 2250, 2353, 4228, 6540,  679, 1352, 2474, 1637,   22, 2467, 2880,\n",
      "         1039, 3372, 5574, 2538, 1451, 3265, 2146, 3724, 1284, 1356, 3160,  107,\n",
      "          763, 1109, 3183, 1116, 2139,  785, 2500, 1214, 2184, 1128, 4757, 4125,\n",
      "         1131,   26, 4087,  428, 2951, 2347, 1161,  388,  647, 7583,  625, 1762,\n",
      "         1708, 7135, 4529, 4424, 2968, 3529,  374, 2581,  191, 3397, 2882, 1950,\n",
      "         7919,  499, 4320, 3517, 5354, 2955, 3494,  903, 3280,  539, 7703, 6277,\n",
      "         1053,  373, 2033, 2482,  947, 3198, 1631, 1206, 1732, 5663,   25,  380,\n",
      "         1994,  492, 5350,  595,  443,  922, 7953, 4667,  217,  346, 3140, 3193,\n",
      "           30, 2567, 1698, 3900, 6880,  933, 1848,  793,  317, 3810,   54,  232,\n",
      "         3791, 2670, 1954, 1659, 2566, 5826, 2670, 1118, 2674, 2903, 3955, 1075,\n",
      "         1317,  406, 2158, 1380,   24, 3685, 5946,  721,  362,  793, 1872, 1511,\n",
      "         3632,  416, 5327, 2389,    3, 3779,  125, 1380, 3066, 5993,  183, 3026,\n",
      "         1761, 1670, 5368,  784, 1036, 6462, 1827, 2817,  555, 1256, 4201, 3995,\n",
      "          842, 1666,  726, 2282, 7013, 1358, 1296,  383, 2810, 1050, 1942, 2412],\n",
      "        [   1,  252, 2281,  326,  883, 1261, 2581, 1143,  776, 2936, 2467, 4494,\n",
      "         1906, 4745,   87,  258, 1200, 4555, 4555, 5092,  768, 3331,  803, 1861,\n",
      "         2185,  189, 1952, 6817, 7962, 1877, 3331,  239,  519, 6137,  572, 1823,\n",
      "         1189, 1581, 2070, 3031, 1881, 2176, 2072, 5513,   80, 1321, 3990,  976,\n",
      "         2781,  581,  578,  514, 2427,  794, 4555, 1105,  829, 2282, 1790,  566,\n",
      "         4466,  496, 2220, 5306, 2116, 1352, 2029, 2807, 1606, 3385,  863,  107,\n",
      "          961, 1043, 2648, 6007, 2215,  793, 4637,  741, 4849, 1719, 4524, 2460,\n",
      "         1067, 1794, 1461, 3585, 1510, 6820,  509, 5268, 2930,  409, 3769, 2208,\n",
      "         1577, 7240,  739, 4279, 6148, 1297,  136, 1909, 2914, 7211, 4755, 3313,\n",
      "         2928, 4053, 7194, 4905, 3081, 3916, 1912, 1668,  885, 3066,  997, 1565,\n",
      "         2357, 5253, 3166, 1354,  742, 4306,   61, 4070, 2430,  976,  256,  453,\n",
      "         1836, 2291, 2223,  575,  900, 1091, 3218, 2611, 4611,  557, 3357, 1526,\n",
      "         1176,  109, 1894, 4384, 3352,  494, 4340,  310, 2022, 3627, 1291, 1700,\n",
      "          614, 2568, 2124, 1929, 1969, 3437, 3467, 2700, 3259, 1599, 2302,  686,\n",
      "         4573,  509,  725, 5244, 6126, 2647, 1877,  845, 1877, 1552, 4880, 2263,\n",
      "         3073, 1253, 1156, 6368,  395, 4171, 3397, 4554,  282, 5591, 1345,  644,\n",
      "          802, 3805, 3792, 3434, 3953,  776, 3234, 1009, 1403, 6756, 2083, 3791,\n",
      "         1361,  129, 1877, 2419, 3050, 3004, 1553, 4552, 1239, 1716, 1477, 4238,\n",
      "         3125, 5266, 7945, 5489, 1684,  827, 3156, 3558,    6, 3485, 1589, 3212]])}\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(train_dataloader):\n",
    "    print(idx, batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 228])"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(batch['attention_mask'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers.models.llama.configuration_llama import LlamaConfig\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "import sentencepiece as spm\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, dataset_dict, batch_indices_train, batch_size=1):\n",
    "        super().__init__()\n",
    "        self.dataset_dict = dataset_dict\n",
    "        self.batch_indices_train = batch_indices_train\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.tokenizer = self.tokenizer_generation('protein', '8k')\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_generation(target, vocab_size):\n",
    "        if target == 'original':\n",
    "            tokenizer = LlamaTokenizer.from_pretrained('hf-internal-testing/llama-tokenizer')\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "            return tokenizer\n",
    "        elif target == 'protein':\n",
    "            tokenizer_path = '/data/rozen/home/e0833634/lama/protllama/batch_script/'\n",
    "            tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path + \"protein_%s.model\" % (vocab_size))\n",
    "            return tokenizer\n",
    "        else:\n",
    "            raise ValueError('Have not prepared tokenizer for this target')\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Possibly download data, set transforms, etc.\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        self.train_dataset = DynamicBatchingDataset(self.dataset_dict, self.batch_indices_train)\n",
    "        # Repeat similar steps for validation and test datasets if needed\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=DynamicBatchingDataset.custom_collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import transformers\n",
    "from transformers.models.llama.configuration_llama import LlamaConfig\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import os\n",
    "import logging as log\n",
    "import glob\n",
    "from argparse import ArgumentParser\n",
    "from protllama.bin.data import PretrainDataset\n",
    "import torch\n",
    "class pretrainLlama(pl.LightningModule):\n",
    "    def __init__(self, hparam) -> None:\n",
    "        super(pretrainLlama, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.hparam = hparam  # need to contain epoch, target, date, learning rate, batch_size, num_frozen_epochs\n",
    "        self.MODEL_CONFIGS = self.retrieve_config()\n",
    "        self.__build_model()\n",
    "        self.tokenizer = self.tokenizer_generation('protein', '8k')\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_generation(target, vocab_size):\n",
    "        if target == 'original':\n",
    "            tokenizer = LlamaTokenizer.from_pretrained('hf-internal-testing/llama-tokenizer')\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "            return tokenizer\n",
    "        elif target == 'protein':\n",
    "            tokenizer_path = '/data/rozen/home/e0833634/lama/protllama/batch_script/'\n",
    "            tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path + \"protein_%s.model\" % (vocab_size))\n",
    "            return tokenizer\n",
    "        else:\n",
    "            raise ValueError('Have not prepared tokenizer for this target')\n",
    "\n",
    "    def retrieve_config(self):\n",
    "        \"\"\" return transformers DATASET object\"\"\"\n",
    "        if self.hparam.target == 'original':\n",
    "            config_dict = {'7b': LlamaConfig(max_position_embeddings=self.hparam.max_position_embeddings,\n",
    "                                             hidden_size=self.hparam.hidden_size,\n",
    "                                             intermediate_size=self.hparam.intermediate_size)}\n",
    "            return config_dict['7b']\n",
    "        elif self.hparam.target == 'protein':\n",
    "            config_dict = {\n",
    "                'protllama2': LlamaConfig(max_position_embeddings=self.hparam.max_position_embeddings,  # maximum length\n",
    "                                          hidden_size=self.hparam.hidden_size,\n",
    "                                          transformers_version=transformers.__version__,\n",
    "                                          intermediate_size=self.hparam.intermediate_size,\n",
    "                                          vocab_size=int(self.hparam.vocab_size.rstrip('k')) * 1000)}\n",
    "            print(config_dict['protllama2'])\n",
    "            return config_dict['protllama2']\n",
    "        else:\n",
    "            raise ValueError('Have not prepared dataset for this target')\n",
    "\n",
    "    def __build_model(self) -> None:\n",
    "        \"\"\"start model building, can add customized classification head\"\"\"\n",
    "        self.model = LlamaForCausalLM(self.MODEL_CONFIGS)\n",
    "        print(self.model.lm_head.weight)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"set learning rates\"\"\"\n",
    "        if self.hparam.scheduler == 'linear':\n",
    "            parameters = self.model.parameters()\n",
    "            optimizer = AdamW(parameters, lr=self.hparam.learning_rate, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "            lr_schedulers = {\n",
    "                \"scheduler\": get_linear_schedule_with_warmup(optimizer,\n",
    "                                                            num_warmup_steps=100,\n",
    "                                                            num_training_steps=self.hparam.epoch * self.hparam.train_dataset_length),\n",
    "                \"name\": 'learning_rate_logs'\n",
    "            }\n",
    "            return [optimizer], [lr_schedulers]\n",
    "        elif self.hparam.scheduler == 'cosine':\n",
    "            \"\"\"llama behavior, end learning rate matches 10% of the maximum learning rate\n",
    "                hard-coded to be 10% first\n",
    "            \"\"\"\n",
    "            parameters = self.model.parameters()\n",
    "            optimizer = AdamW(parameters, lr=self.hparam.learning_rate, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "            lr_schedulers = {\n",
    "                \"scheduler\": get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=100,\n",
    "                                                            num_training_steps=self.hparam.epoch * self.hparam.train_dataset_length,\n",
    "                                                            num_cycles=0.39758361765,\n",
    "                                                            # number of waves in the cosine schedule - e.g. 0.5 for period 2 cos wave means take 0 to 1\n",
    "                                                            last_epoch=-1  # index of the last epoch when resuming training\n",
    "                                                            ),\n",
    "                \"name\": 'learning_rate_logs'\n",
    "            }\n",
    "            return [optimizer], [lr_schedulers]\n",
    "        else:\n",
    "            raise ValueError('You need to specify a scheduler first. Default is linear')\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        \"\"\" Pytorch forward function\n",
    "        Returns:\n",
    "        dict with model outputs (loss, logits, hidden layer, attention)\n",
    "        \"\"\"\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_nb: int, verbose=True):\n",
    "        outputs = self.forward(**batch)\n",
    "        loss_train = outputs[0]\n",
    "\n",
    "        # Compute the perplexity\n",
    "        perplexity = torch.exp(outputs[0].cpu())  # Ensure outputs are on CPU\n",
    "\n",
    "        # Accuracy computation\n",
    "        # Shifting\n",
    "        shift_logits = outputs[1][..., :-1, :].contiguous().argmax(\n",
    "            dim=-1).cpu()  # Ensure outputs and argmax result are on CPU\n",
    "        if verbose:\n",
    "            print('model predict?')\n",
    "            print(shift_logits)\n",
    "\n",
    "        # Assuming 'labels' is a key in batch containing true token IDs\n",
    "        shift_labels = batch['labels'][..., 1:].contiguous().cpu()  # Move labels to CPU\n",
    "        if verbose:\n",
    "            print('model true?')\n",
    "            print(shift_labels)\n",
    "\n",
    "        non_padding_mask = shift_labels != -100\n",
    "\n",
    "        # Compare predictions to true labels, but only for non-padding tokens\n",
    "        acc_train = ((shift_logits == shift_labels) & non_padding_mask).sum().item() / non_padding_mask.sum().item()\n",
    "        if verbose:\n",
    "            print(acc_train)\n",
    "\n",
    "        # Log\n",
    "        self.log('train_loss', loss_train, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_perplexity', perplexity, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_accuracy', acc_train, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss_train\n",
    "\n",
    "    def validation_step(self, batch, batch_nb: int):\n",
    "        \"\"\" Similar to the training step but with the model in eval mode.\n",
    "        Returns:\n",
    "            - dictionary passed to the validation_end function.\n",
    "        \"\"\"\n",
    "        outputs = self.forward(**batch)\n",
    "        loss_val = outputs[0].cpu()\n",
    "\n",
    "        # Compute the perplexity\n",
    "        perplexity = torch.exp(loss_val)  # Ensure outputs are on CPU\n",
    "\n",
    "        # Accuracy computation\n",
    "        # Shifting\n",
    "        shift_logits = outputs[1][..., :-1, :].contiguous().argmax(\n",
    "            dim=-1).cpu()  # Ensure outputs and argmax result are on CPU\n",
    "\n",
    "        # Assuming 'labels' is a key in batch containing true token IDs\n",
    "        shift_labels = batch['labels'][..., 1:].contiguous().cpu()  # Move labels to CPU\n",
    "\n",
    "        non_padding_mask = shift_labels != -100\n",
    "\n",
    "        # Compare predictions to true labels, but only for non-padding tokens\n",
    "        acc_val = ((shift_logits == shift_labels) & non_padding_mask).sum().item() / non_padding_mask.sum().item()\n",
    "\n",
    "        # Log\n",
    "        self.log('val_loss', loss_val, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_perplexity', perplexity, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_accuracy', acc_val, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss_val\n",
    "\n",
    "    @classmethod\n",
    "    def add_model_specific_args(cls, parser: ArgumentParser):\n",
    "        \"\"\"parser for hyperparameters\"\"\"\n",
    "        parser.add_argument('--learning_rate', type=float, default=3e-4, help='Learning rate for Adam optimizer')\n",
    "        parser.add_argument('--scheduler', type=str, default='linear', help='Learning rate scheduler, either linear '\n",
    "                                                                            'or cosine')\n",
    "        parser.add_argument('--epoch', type=int, default=1, help='number of epochs for the training')\n",
    "        #parser.add_argument('--batch_size', type=int, default=2, help='Batch sizes, sequence number per batch')\n",
    "        return parser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 640,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1720,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.33.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.2041e-02,  9.0041e-03, -3.4151e-03,  ..., -7.1120e-05,\n",
      "          2.6942e-02,  5.4405e-03],\n",
      "        [ 2.2820e-02,  4.9283e-03, -1.2922e-02,  ...,  1.2707e-02,\n",
      "         -9.8123e-03, -4.3455e-03],\n",
      "        [ 6.4636e-03,  8.7062e-03,  4.9835e-02,  ...,  4.0261e-03,\n",
      "         -3.7080e-04,  9.0960e-04],\n",
      "        ...,\n",
      "        [ 2.2562e-02, -8.0692e-04, -7.5625e-03,  ...,  1.4612e-02,\n",
      "         -2.3076e-02, -5.0668e-03],\n",
      "        [ 5.5981e-03, -3.8602e-03, -3.7671e-02,  ...,  1.4204e-02,\n",
      "          2.7897e-02, -5.4544e-03],\n",
      "        [-6.7332e-05,  2.3461e-02,  1.0129e-02,  ..., -2.0542e-02,\n",
      "          1.8716e-02,  4.7361e-02]], requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-d4342c04-7329-6ec9-9124-b61038ac3411,GPU-0958174f-473c-d99c-c8b5-c9c88c212a45]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | LlamaForCausalLM | 168 M \n",
      "-------------------------------------------\n",
      "168 M     Trainable params\n",
      "0         Non-trainable params\n",
      "168 M     Total params\n",
      "673.549   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84d08f9b7ff04c9592ae51f9b48b6069"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predict?\n",
      "tensor([[5876, 2453, 2453, 2453, 2453, 2453, 2453, 2453, 2453,  547,  547,  547,\n",
      "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
      "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
      "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
      "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
      "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
      "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547],\n",
      "        [5876, 5876, 7949, 7949, 7949, 7949, 7949, 7896, 7896, 7896, 7896, 7896,\n",
      "         7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896,\n",
      "         7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896,\n",
      "         7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 4726, 7896, 7896,\n",
      "         7896,  987, 7896,  987,  987, 7896,  987, 7896, 4679,  583,  987, 4679,\n",
      "          583,  583,  987,  583,  583, 4679, 4679, 7251,  987,  583,  583,  583,\n",
      "         4679, 2795, 7251, 2795, 7251,  583, 7251, 7251, 7251, 7251, 7251],\n",
      "        [5876, 2453, 5876, 5876, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896,\n",
      "         7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896, 7896,\n",
      "         7896, 7755, 7896, 7896, 7896, 7755, 7755, 4955, 7755, 7755, 7755, 7755,\n",
      "         7755, 7755, 7755, 7755, 7755, 7755, 7778, 7755, 7755, 7755, 7755, 7755,\n",
      "         7755, 7755, 7755, 7755, 7755, 7755, 7755, 7755, 2192, 7755, 7755, 4679,\n",
      "         2192, 7755, 7755, 2192, 2192, 2192, 7755, 7755, 7755, 7755, 7755, 7755,\n",
      "         2192, 2192, 7755, 2192, 2192, 7755, 2192, 2192, 7755, 2192, 7755],\n",
      "        [5876, 5876, 5876, 5876, 5876, 2376, 2376, 2376, 2376, 2376, 2376, 2376,\n",
      "         2376, 2376, 5394, 2376, 2376, 2376, 5394,  547, 5394,  547,  547,  547,\n",
      "          547,  547, 5662,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
      "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
      "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
      "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
      "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547],\n",
      "        [5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876,\n",
      "         5876, 7251, 5876, 7251, 5876, 5876, 7949, 7949, 7949, 7949, 7949, 7949,\n",
      "         5876, 5876, 7949, 7949, 7949, 7949, 7949, 7949, 7949, 7949, 7949, 7949,\n",
      "         7949, 7949, 7949, 7949, 7949, 7949, 1334, 1334, 7949, 7949, 7949, 1334,\n",
      "         1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334,\n",
      "         1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334,\n",
      "         1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334],\n",
      "        [5876, 5876, 4955, 4955, 4955, 4955, 4955, 4955, 4955, 4955, 4955, 4955,\n",
      "         4955, 4955, 4955, 4955, 4955, 4955, 4955, 4955, 4955, 3117, 3117, 3117,\n",
      "         3117, 3117, 3117, 3117, 3117, 3117, 3117, 3117, 3117, 6431, 3117, 3117,\n",
      "         3117, 3117, 3117, 3117, 3117, 3117, 3117, 6431, 3117, 3117, 3117, 3117,\n",
      "         3117, 6431, 6431, 6431, 6431, 6431, 6431, 6431, 6431, 6431, 3117, 6431,\n",
      "         6431, 6431, 6431, 6431, 6431, 3117, 6431, 6431, 6431, 6431, 6431, 6431,\n",
      "         6431, 3117, 6431, 6431, 6431, 6431, 3117, 6431, 6431, 6431, 6431]])\n",
      "model true?\n",
      "tensor([[ 390,  817,  325,  881,  545, 4579, 2698, 3853,  400, 3222,  811, 5410,\n",
      "          757, 1580,  312, 2013, 1832,  689, 2295, 3041, 3967,  762, 5632, 2307,\n",
      "         4670, 3743,  300, 5307,  343, 2685, 3989, 5093, 4627, 4023,  771, 3396,\n",
      "         6661,  220,  379, 6834,  771, 3306, 1659, 4508, 5596, 2220, 2892,  461,\n",
      "            4, 2068, 1108, 1198, 2766, 2828, 1009,  585, 2566, 3824, 6189,  666,\n",
      "         1509, 5843, 1419,  172,  810, 4278,  931,  707, 4049,  470, 5006, 1181,\n",
      "         1208, 1271, 3306,  505, 3989, 4959, 3861,  356, 4102, 3154,  611],\n",
      "        [ 820, 1919, 4465, 1017, 1241, 4424, 2858, 1017,   53, 2874, 3400, 1248,\n",
      "          617,  737,   54,  409,  591,  924,   37, 4884, 6452, 1225, 3224,  378,\n",
      "         2301,  434, 6762,  663, 3534,   72, 4146,  464, 1098,   45, 1134,  348,\n",
      "         1982, 3944, 4862,  995,  485, 4882, 1472, 4226,   54,  995, 1209, 5196,\n",
      "          277, 1428,  582, 2211,  552,  784, 4702, 6476,  787, 3076,  236,  314,\n",
      "         5817, 1042, 1265,   61, 3873,  381, 2263, 5040,  330, 1234,  282,  742,\n",
      "         6747, 7776, 6029,  210, 4271, 5760,  907, 2704,  517, 5292, 1141],\n",
      "        [1171, 7708,   34, 4009, 1036, 3470,   74, 1039, 2396,  606, 2432,  337,\n",
      "          356,  402,  894, 1974,  212, 3452, 6420, 1611, 5894, 3857, 5713, 1353,\n",
      "         2464, 2909, 4248, 4887,   21, 5170, 2999, 5670, 3389,  729, 3524, 1726,\n",
      "          191,  337, 5388, 5772, 3095,  610,  946, 1610,  330,  339, 5085, 5794,\n",
      "         3690, 3926,  337, 1711, 7155, 6247,  306, 7910,    6,  845, 3122, 7642,\n",
      "          341,  846,  613,  416, 4251, 1209, 1946, 5173, 7705, 2679, 7705,  679,\n",
      "         1321, 4645, 1305,  716,  363,  790, 1968, 3273,  605,  101,  706],\n",
      "        [4052,   47,  516,   21, 1253, 1603, 1026, 1306, 4156, 4988,  661, 2514,\n",
      "         1590, 5005, 5831, 3966, 6830, 6751, 1786, 4531,  606, 2634, 1216, 1304,\n",
      "         4026, 4617,  383,  244,  412,  437,  947, 3502, 3457, 1408,  863, 2632,\n",
      "         1345,  383, 1905, 2024,  809, 1291, 5253, 2583, 1435,  903,   25,  804,\n",
      "         3076,  781, 2702,  137,   21,  500,  921,  891,  579, 1996, 3865, 1681,\n",
      "         1455,   22, 5327, 1399, 3981, 1424, 3171,   32, 2521, 1496,  838, 4514,\n",
      "         1324,  591, 2282,  483,  889, 7513,  276, 6409, 3009, 4022, 4962],\n",
      "        [  28,  807, 6219,  114, 7396,  133, 4094, 2677,  791, 1501, 3151, 2209,\n",
      "         1532,  921,  867, 2447,  561, 2958, 7742, 1086, 5403, 3735, 4276, 5093,\n",
      "          998, 2591, 3703, 3297, 7128, 2520, 3234,  183, 2091, 2077,  970,  291,\n",
      "          519, 1742,  869, 2382,   33,  953, 2084, 2341,    3, 1365, 2507, 5127,\n",
      "         2217, 2430,  262, 3214, 4115, 3266,  381, 5441,  425, 5146, 4858, 1218,\n",
      "          426, 4543, 1727, 5532, 4994, 5937, 3778, 2912, 6576, 3879, 4238, 2751,\n",
      "         1493, 1522, 1090, 1214,  498, 6998,  137, 2846,  480,  366,  324],\n",
      "        [7322,  680,  648, 3056,  449,  634,  607, 1337, 3661, 1483,  599,  875,\n",
      "         1645, 3648,   47,  409, 3993, 2543, 1437,  317, 6480, 2885,  577,  478,\n",
      "         2763,  934, 1639, 4523, 1769, 7046, 3417, 1386,  620, 1788, 1711, 6623,\n",
      "          774, 6154, 1438, 3898,  505, 4123, 1321, 5935,   29, 1754,  878, 1755,\n",
      "         4279, 1447, 1102,  724, 1263, 5214,  618, 3792,  486,  342, 4051, 2855,\n",
      "         1510, 1531, 4306,  715,  136, 1182, 3362, 1651, 1342, 2957,  424,   65,\n",
      "         3007, 4999, 2133, 3529, 2606,  647, 1214, 3637, 2996, 5330, 6755]])\n",
      "0.0\n",
      "model predict?\n",
      "tensor([[5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 1573, 5876, 5876, 1573,\n",
      "         5876, 5876, 5876, 5876, 7949, 7949, 7949, 7949, 7949, 7949, 7949, 7949,\n",
      "         7949, 7949, 7949, 7949, 7949, 7949, 7949, 7949, 5424, 7949, 7949, 5424,\n",
      "         5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 7949, 5424, 2858,\n",
      "         5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424,\n",
      "         5424, 5424, 1648, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424,\n",
      "         5424, 1648, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424,\n",
      "         5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424,\n",
      "         5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424,\n",
      "         5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 5424, 1648,\n",
      "         1125, 5424, 5424, 5424, 5424, 5424, 1125, 5424, 5424, 1125, 1125, 5424,\n",
      "         5424, 1125, 5424, 5424,  771, 5424, 5424, 5424,  771, 5424, 5424, 5424,\n",
      "         5851, 5424, 1648, 5424, 5424, 5424, 5424, 1125, 5424, 5424, 5424, 5424,\n",
      "         5851, 5424, 1125, 5424, 5424, 5424,  771,  771, 1648, 1125, 1125, 1125,\n",
      "         5424,  771, 5424, 5851, 1125, 1125, 5424, 5424,  771, 1125, 1125, 1648,\n",
      "         5424,  771, 5424,  771, 5424, 1648,  771, 5424, 5424,  771,  771,  771,\n",
      "          771, 5424, 1125, 5424,  771, 5424, 1125, 5424, 1125, 5851,  771,  771,\n",
      "         1125, 5424, 5424, 5424, 1125, 1125, 1125,  771, 5424, 1125, 1125, 5424,\n",
      "         5851,  771,  771, 5424, 1125, 1125,  771,  771,  771,  771,  771, 5424,\n",
      "         5424,  771, 1125, 1648,  771,  771,  771,  771, 1125, 5424, 1125, 5424,\n",
      "          771,  771, 5851, 5851,  771,  771, 5424, 5851, 1125, 1648, 5424, 5424,\n",
      "         1125,  771, 1125, 5424,  771,  771,  771,  771, 5424, 5424,  771, 5424,\n",
      "         5424,  771, 5424, 5424, 5424, 1125, 1125, 5424, 5424, 1125, 1125, 1125,\n",
      "          771, 1125, 5424, 1125, 5424, 1125, 5424,  771, 1125, 5424, 1125, 1125,\n",
      "         1125,  771, 5424,  771, 1125,  771, 1125, 1125, 5424, 5424, 5424,  771,\n",
      "         1125, 1125, 5424,  771, 1125, 4406, 1125, 5424, 1125, 5424, 5424, 5424,\n",
      "         1125, 1125, 5424, 5424, 1125, 1125,  771, 1125,  771, 5424, 1125, 1125,\n",
      "         1125, 1125, 1125, 1125, 1125, 1125]])\n",
      "model true?\n",
      "tensor([[  28, 3312, 4266, 2308, 1219, 2665, 1052, 6285,  164,  411,  763,  760,\n",
      "         1737, 6201, 3094, 2154, 2344, 5860, 7077,  374,  472, 7350, 4553, 1061,\n",
      "         1254, 2222, 2739, 2237, 2823, 7070, 2058, 1121, 2672, 5592,  987, 2171,\n",
      "          751, 4233,  696,   55,  908, 2149, 3049,  928, 2605, 2578,  370, 4472,\n",
      "         1064, 1291,  224,  162, 1482, 5744, 7558,  983, 1940,  691, 7570, 1567,\n",
      "          436, 2667, 2250, 1060,  625, 2229, 4668,  949,  140, 3011, 1959, 5673,\n",
      "          798, 1685, 2393,  452,  670,  759, 1558, 2864, 1041, 5518,  119,  889,\n",
      "          711, 3631, 2562, 1294, 2593, 1484, 4434, 3037, 4208, 3096,  613, 5596,\n",
      "         3820,  192, 2169, 4229,  825,  596, 3919,  141, 1549, 4692, 2247, 1919,\n",
      "         1012,  648, 1870, 6303, 1648,  422, 4305, 1114,    6, 3947, 4162, 4547,\n",
      "         5194, 2131, 4992, 5694, 3414, 2910, 4514, 3348,   86, 3929,  565, 3140,\n",
      "          827,   61, 4195,  282,  478, 3656,   51, 1107,  996, 3010,  970,   83,\n",
      "         1158, 1357, 4239,  226,  408, 2684, 3768, 2607, 6537,  924, 3098,  292,\n",
      "          752, 6317,   84,  242,  309, 5158,  284, 1909, 1277,  803, 5547,  440,\n",
      "         2414, 1458, 2629, 4801,  118,  690, 4322, 3482, 4563, 5003, 5452, 1920,\n",
      "         2468, 2046,  307, 4248, 7985,   44, 1574, 4310, 5485,   41, 3735, 6127,\n",
      "         3588,  327, 4205, 2239, 3828,   62, 3605, 2813, 2334, 2274, 1975,  857,\n",
      "          773,  629, 2038, 1005, 5770, 1608, 4084,  132,  736, 2318, 4522,  162,\n",
      "         3868,  590, 5828, 4519, 2570,  577, 1805,  613, 1529, 1296, 2199, 7640,\n",
      "         2486, 4636,  102, 4170, 4427,   16,   95, 1205, 1364, 6847,  511, 2281,\n",
      "         4215,  346, 1653,  150, 4343,  691,  852,  955,  957, 6160,  120, 4641,\n",
      "          544, 6930, 7429, 1180, 6074, 6856,   16, 3996, 1799, 7919, 1045, 2887,\n",
      "         1452,   47, 6844, 1522, 3440,   49, 2498, 4131, 3835,  584,  350,  688,\n",
      "         6349, 2712,  428,  815,  982, 2392, 1195, 1824, 4168, 1368,  305, 3075,\n",
      "         1524, 2150, 7203,  891, 1318, 1633, 2186,  971, 1067, 3896, 2725, 2910,\n",
      "          799,  502, 2769, 4050, 2588, 1138,  341, 3523,   79, 4255, 1454, 1770,\n",
      "          963,  814,  380, 1321,   80, 3036, 6044, 2327, 3429,   61, 3494, 1928,\n",
      "          114, 1426, 1803,  311, 7235, 1686]])\n",
      "0.0\n",
      "model predict?\n",
      "tensor([[5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876,\n",
      "         5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876,\n",
      "         5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876, 5876,\n",
      "         5876, 5876, 5876, 5876, 5876, 5876, 5394, 5876, 5876, 5394, 5876, 5876,\n",
      "         5394, 5394, 5876, 5394, 5394, 5394, 5394, 5876, 5394, 5394, 5394, 5394,\n",
      "         5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394,\n",
      "         5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394,\n",
      "         5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394, 5394,\n",
      "         5394, 1494, 1812, 5394, 5394, 5394, 5394, 5394, 1812, 5394, 5394, 1812,\n",
      "         5394, 1812, 1812, 1494, 1812, 5394, 5394, 5394, 2975, 1812, 1812, 5394,\n",
      "         5394, 5394, 1812, 1812, 1812, 5394, 1812, 1812, 5394, 1812, 5394, 1812,\n",
      "         1812, 1812, 2639, 5394, 4655, 1812, 7008, 4655, 1812, 1812, 1812, 1812,\n",
      "         1812, 1812, 1812, 1812, 5394, 1812,  833, 1812, 1812, 5394, 1812, 1812,\n",
      "         1812, 1812, 1812, 1812, 1812, 1812, 1812, 1812, 1812, 1812, 1812, 1812,\n",
      "         1812, 1812, 1812, 1812, 1812, 1812, 1812, 1812, 1812, 1812, 1812, 1812,\n",
      "         1812, 1812, 1812, 1812, 1812, 1812, 1812, 1812,  833, 1812],\n",
      "        [5876, 5876, 5876,  547,  547,  547, 2515,  547,  547,  547,  547,  547,\n",
      "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
      "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
      "          547,  547,  547,  547,  547,  547,  547, 4578,  547, 1125, 1125,  547,\n",
      "         1125, 1125, 2827, 1125, 4578, 4578,  547, 4578, 2827, 2827, 1125, 4578,\n",
      "         1125, 1125, 2827, 2827, 2827,  547, 2827, 1125, 2827, 1125, 2827, 1125,\n",
      "         4578, 4578, 1125, 2827, 4578, 2827,  547, 2827, 1125, 2827, 4578, 2827,\n",
      "         4578, 1125, 2827, 2827, 2827, 4578, 2827, 2827, 2827, 4578, 4578, 2827,\n",
      "         2827, 2827, 1125, 2827, 2827, 4578, 2827, 1125, 1125, 2827, 2827, 2827,\n",
      "         2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827,\n",
      "         2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827,\n",
      "         2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827,\n",
      "         2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827,\n",
      "         2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827,\n",
      "         2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827,\n",
      "         2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827, 2827]])\n",
      "model true?\n",
      "tensor([[  28,  425,  643,  481, 2329,  841,  144, 5480, 1528,  434,   98,  794,\n",
      "         3818, 4905,  435, 1665, 1354,  853, 1869, 5901, 1184, 7909,  664, 4766,\n",
      "           20,  357, 5219, 7122, 1484, 2861, 4958,  443, 7387, 1650, 1385, 2494,\n",
      "         1201, 3444, 1166, 3346, 2041, 1034,  302, 2817, 5065, 3014,  953,  864,\n",
      "         1296, 3378,  865,  589,  661, 3076, 1191, 6644, 2507,   86, 3464,  308,\n",
      "         1842,  672, 4501,   32, 5916, 3328,  862, 1782, 1561,   32, 1321,  680,\n",
      "         7365, 7170, 2108, 2438, 1479,  419, 4610,  762,   37, 3939, 2382,  159,\n",
      "         6184, 1541, 3693,  320, 3457,  346,  552, 2236, 4494, 2809, 2892,  817,\n",
      "         2169, 6473, 2224, 2074, 4095, 5720, 3350, 1689,  892, 4599, 3284,  850,\n",
      "         2144, 4824,  317, 1845, 4416, 1115, 3415, 1187, 3178, 1157, 6919, 3964,\n",
      "         3752,  932, 4710, 1584, 6405, 3557, 1531, 2983, 5914,  928, 6838, 7257,\n",
      "          263,  104, 1423, 1869, 1011,  778, 5890, 1412,  619,  403, 3467, 1016,\n",
      "         1475, 5699, 2724,   42,  792, 5948, 1623, 2916, 6502,  538,  912, 3671,\n",
      "         2882,  218,  145, 1034, 2768, 1014, 1369, 2761,    3, 2022, 3531, 2492,\n",
      "         1092, 1303, 5330, 3891, 2992,  766, 1939,  690, 4877, 1559, 2348,  197,\n",
      "           52, 2320,  806, 2137, 5183,  518, 6415,  824,  395,  529],\n",
      "        [ 667,  183,  427, 4931, 4515,  383,  127, 4004, 7008, 1099, 2971, 1460,\n",
      "         2081, 1614, 5265, 2630, 1079, 5505, 1199, 2008,   62, 1724, 6706, 1460,\n",
      "         1554, 2128,  742,  452, 7967, 1074, 3352,  862, 1342, 2072, 7917,  794,\n",
      "          380, 2052,  879, 1965, 1649, 5974, 1649, 2612, 3189, 5891, 1023,  416,\n",
      "          358, 2781,  145, 1723, 1659, 2938, 1906,  513,  957, 3266, 2834, 7124,\n",
      "         2112, 3046, 6730, 1717,  989, 7050, 1878, 2308,   25, 1936, 1154, 3612,\n",
      "          587, 4068, 7029,  169, 3412, 5170, 5163, 1640, 1213, 3753, 2472, 1213,\n",
      "         1044, 3035, 1113, 6615, 1063, 1472,  452,  197,  896, 3138,  439,  277,\n",
      "          711, 2035, 1217,   44, 1907, 5005, 1281, 1626,  441, 2954,  233,  210,\n",
      "         7176,  361,  961, 1119, 1437,  925, 3969, 3522, 1391, 1359,  718, 5415,\n",
      "          516, 2475, 5024,   42, 2188,  829,  269, 1249,  749, 4930, 2898, 1824,\n",
      "         1048, 2822,   74, 1585, 1227, 7317, 3419,  852,  600, 3209,  365, 4814,\n",
      "         2541, 1054, 5164, 1462, 1436,  401, 7612, 4203, 1383, 5827,  540, 6251,\n",
      "          913, 1212, 3902, 1081, 4888, 1622, 1322, 3445, 5657,  788, 1067,  475,\n",
      "         1615, 1710, 2788, 4594,  994,  603, 6322, 5661, 7699, 3936, 2681, 6660,\n",
      "         7329, 1575,  138,  296, 3051, 3906, 1483, 5301,    4, 1491]])\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from protllama.bin.data import PretrainDataset\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, TQDMProgressBar, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# set wandb offline on HPC\n",
    "os.environ['WANDB_MODE'] = \"offline\"\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "hparam = SimpleNamespace(\n",
    "    date='Oct_11',\n",
    "    target='protein',\n",
    "    max_position_embeddings=512,\n",
    "    vocab_size='8k',\n",
    "    hidden_size=640,\n",
    "    intermediate_size=1720,\n",
    "    save_top_k=1,\n",
    "    scheduler='linear',\n",
    "    learning_rate=3e-4,\n",
    "    epoch=1\n",
    "    #... add all other arguments similarly\n",
    ")\n",
    "\n",
    "\n",
    "#dm = PretrainDataset(target=hparam.target,\n",
    "                     #max_sequence_length=hparam.max_position_embeddings)\n",
    "data_module = CustomDataModule(dataset_dict=small_dataset_dict, batch_indices_train=batch_indices_train, batch_size=1)\n",
    "\n",
    "# make sure dataset has \"training\" key\n",
    "hparam.train_dataset_length = 10\n",
    "training_log_path = str('protllama/pl_logs/')\n",
    "if not os.path.exists(training_log_path):\n",
    "    os.makedirs(training_log_path)\n",
    "logger = WandbLogger(project=\"protllama2\",\n",
    "                     name=f\"{hparam.target}_{hparam.date}_pre-training_log\", #display on the web\n",
    "                     save_dir='protllama/pl_logs/',\n",
    "                     job_type='model-training',\n",
    "                     group=f'pretrain_protllama2_{hparam.vocab_size}_{hparam.max_position_embeddings}',\n",
    "                     id='version_%s' % str(1))\n",
    "seed_everything(42)\n",
    "model = pretrainLlama(hparam)\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"loss\",\n",
    "    min_delta=0.0,\n",
    "    patience=0,  # number of epoch with no improvement\n",
    "    verbose=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "training_model_path = str('protllama/pl_model_cache/')\n",
    "if not os.path.exists(training_model_path):\n",
    "    os.makedirs(training_model_path)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=training_model_path,\n",
    "    filename=\"{epoch}-{train_loss:.2f}-{val_loss:.2f}-%s_%s_%s_%s\" % (hparam.target, hparam.date, hparam.vocab_size, hparam.max_position_embeddings),\n",
    "    save_top_k=hparam.save_top_k,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(\n",
    "    logging_interval='epoch'\n",
    ")\n",
    "trainer = Trainer(\n",
    "    devices=1,\n",
    "    accelerator='gpu',\n",
    "    limit_train_batches=3,\n",
    "    max_epochs=1,\n",
    "    logger=logger,\n",
    "    # max_epochs=1,\n",
    "    # min_epochs=1,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=10), lr_monitor],\n",
    "    deterministic=True,\n",
    "    enable_model_summary=True\n",
    ")\n",
    "\n",
    "# automatic garbage collection\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5876, 5876, 5876, 5876, 5876, 2376, 2376, 2376, 2376, 2376, 2376,\n        2376, 2376, 2376, 5394, 2376, 2376, 2376, 5394,  547, 5394,  547,\n         547,  547,  547,  547, 5662,  547,  547,  547,  547,  547,  547,\n         547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n         547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n         547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n         547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n         547,  547,  547,  547,  547,  547]])"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = np.array([[5876, 5876, 5876, 5876, 5876, 2376, 2376, 2376, 2376, 2376, 2376, 2376,\n",
    "         2376, 2376, 5394, 2376, 2376, 2376, 5394,  547, 5394,  547,  547,  547,\n",
    "          547,  547, 5662,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
    "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
    "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
    "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547,\n",
    "          547,  547,  547,  547,  547,  547,  547,  547,  547,  547,  547]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "        shift_logits = outputs[1][..., :-1, :].contiguous().argmax(\n",
    "            dim=-1).cpu()  # Ensure outputs and argmax result are on CPU\n",
    "        if verbose:\n",
    "            print('model predict?')\n",
    "            print(shift_logits)\n",
    "\n",
    "        # Assuming 'labels' is a key in batch containing true token IDs\n",
    "        shift_labels = batch['labels'][..., 1:].contiguous().cpu()  # Move labels to CPU\n",
    "        if verbose:\n",
    "            print('model true?')\n",
    "            print(shift_logits)\n",
    "\n",
    "        non_padding_mask = shift_labels != -100\n",
    "\n",
    "        # Compare predictions to true labels, but only for non-padding tokens\n",
    "        acc_train = ((shift_logits == shift_labels) & non_padding_mask).sum().item() / non_padding_mask.sum().item()\n",
    "        if verbose:\n",
    "            print(acc_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch indices\n",
    "with open('/data/rozen/home/e0833634/lama/protllama/batch_script/train_intermediate_checkpoint_batches_1000000.pkl', 'rb') as f:\n",
    "    batch_indices_train = pickle.load(f)\n",
    "# batch indices\n",
    "with open('/data/rozen/home/e0833634/lama/protllama/batch_script/valid_intermediate_checkpoint_batches_1000000.pkl', 'rb') as f:\n",
    "    batch_indices_valid = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "253440"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_indices_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "253440"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_indices_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "[[181],\n [816],\n [327],\n [790],\n [227],\n [896],\n [125],\n [435, 618, 756, 881],\n [55, 266],\n [325, 345],\n [823],\n [895],\n [975],\n [643],\n [18],\n [471],\n [558, 647, 653, 681, 697, 764, 864, 882, 917],\n [24],\n [17],\n [35],\n [108, 150, 194, 300, 368, 672],\n [609],\n [955],\n [755],\n [855],\n [617],\n [122, 298],\n [829],\n [51],\n [487, 657],\n [490],\n [365],\n [976],\n [470, 775],\n [730],\n [85, 322],\n [182],\n [530, 541, 543, 548, 712, 785],\n [151, 216, 230, 483, 549, 608, 711, 760],\n [447],\n [250],\n [234],\n [269, 443, 452, 515],\n [313, 956],\n [691],\n [102],\n [935],\n [584, 592],\n [614, 640, 934],\n [843],\n [926],\n [544, 734, 876],\n [696, 731, 736, 773, 965],\n [101, 255, 312, 680],\n [662],\n [572, 604, 690],\n [699, 827, 922],\n [723, 873, 905],\n [212, 219, 655],\n [62],\n [892],\n [299],\n [283],\n [115],\n [840],\n [98],\n [87],\n [0],\n [25],\n [480],\n [522],\n [251],\n [692],\n [525, 605, 978],\n [792],\n [256],\n [163, 781],\n [114],\n [126, 382],\n [82],\n [4, 80, 103],\n [113, 143, 274],\n [794],\n [364],\n [76, 204, 286, 547, 629],\n [613, 997],\n [479],\n [718, 741, 783],\n [521],\n [120],\n [237, 306, 500],\n [197],\n [223, 437],\n [31],\n [503, 885],\n [411, 465, 491, 524, 687, 706, 786],\n [184],\n [119, 725, 820],\n [27],\n [277, 570, 573, 648],\n [602],\n [8],\n [53],\n [918],\n [880],\n [214],\n [369],\n [203, 590],\n [213],\n [831],\n [268],\n [538, 678, 744],\n [186],\n [144, 376],\n [740, 800, 899],\n [342],\n [787],\n [63, 287, 563, 743, 907, 929],\n [453],\n [838],\n [360, 428, 588, 728],\n [383, 626, 825],\n [506, 735],\n [459, 494],\n [199, 341],\n [20, 532, 582],\n [180],\n [693],\n [448, 578, 585, 619],\n [138],\n [278, 909],\n [202],\n [65],\n [56],\n [695, 769],\n [445],\n [418, 473, 474, 507, 571, 684],\n [185],\n [362, 371],\n [580, 721, 772, 777],\n [594],\n [91],\n [139],\n [545],\n [344],\n [57],\n [384],\n [821],\n [441, 477],\n [273, 639, 663],\n [66],\n [252, 338, 346, 423],\n [702],\n [759],\n [984],\n [49],\n [482],\n [6],\n [36],\n [830],\n [141],\n [309, 412, 516],\n [132, 938],\n [349, 398, 460, 713, 931],\n [518, 561],\n [883],\n [142],\n [463],\n [7, 498, 555],\n [836],\n [352, 485, 941],\n [127, 900, 919],\n [914, 985],\n [611],\n [393],\n [140],\n [233],\n [50],\n [239],\n [377],\n [462],\n [69, 394, 566, 733, 802, 866],\n [336],\n [844],\n [224, 433],\n [319],\n [162, 331, 858],\n [381, 509],\n [54],\n [553, 980],\n [14, 400, 484],\n [16],\n [628],\n [910],\n [644],\n [343, 936],\n [796],\n [397],\n [282, 335, 601],\n [111],\n [136],\n [763, 806],\n [804, 828, 867],\n [416],\n [324, 603]]"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build smaller test set\n",
    "small_batches = [batch for batch in batch_indices_train if all(idx < 1000 for idx in batch)]\n",
    "small_batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_indices = small_batches\n",
    "train_dataset = CustomDataset(dataset, batch_indices)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=None, shuffle=False)  # Note that batch_size is None because your dataset itself is returning batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [107]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain_dataset\u001B[49m\n",
      "Input \u001B[0;32mIn [107]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain_dataset\u001B[49m\n",
      "File \u001B[0;32m~/pycharm-2021.3.2/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:747\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    745\u001B[0m \u001B[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001B[39;00m\n\u001B[1;32m    746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND:\n\u001B[0;32m--> 747\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;66;03m# No need to reset frame.f_trace to keep the same trace function.\u001B[39;00m\n\u001B[1;32m    749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrace_dispatch\n",
      "File \u001B[0;32m~/pycharm-2021.3.2/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:144\u001B[0m, in \u001B[0;36mPyDBFrame.do_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pycharm-2021.3.2/plugins/python/helpers/pydev/pydevd.py:1147\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1144\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1147\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pycharm-2021.3.2/plugins/python/helpers/pydev/pydevd.py:1162\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1159\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1161\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1162\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1166\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [103]\u001B[0m, in \u001B[0;36m<cell line: 37>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     34\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(BatchedDataset(small_batches), batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, collate_fn\u001B[38;5;241m=\u001B[39mcollate_fn)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Usage\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28mprint\u001B[39m(batch)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:2807\u001B[0m, in \u001B[0;36mDataset.__getitems__\u001B[0;34m(self, keys)\u001B[0m\n\u001B[1;32m   2805\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitems__\u001B[39m(\u001B[38;5;28mself\u001B[39m, keys: List) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List:\n\u001B[1;32m   2806\u001B[0m     \u001B[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 2807\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getitem__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2808\u001B[0m     n_examples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch[\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(batch))])\n\u001B[1;32m   2809\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [{col: array[i] \u001B[38;5;28;01mfor\u001B[39;00m col, array \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mitems()} \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_examples)]\n",
      "Input \u001B[0;32mIn [103]\u001B[0m, in \u001B[0;36mBatchedDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[0;32m---> 31\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_indices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "# Flatten the batch indices to select the data from the original dataset\n",
    "flat_indices = [idx for batch in small_batches for idx in batch]\n",
    "\n",
    "# Create the shrunken dataset using the flattened indices\n",
    "shrunken_dataset_train = dataset[\"train\"].select(flat_indices)\n",
    "\n",
    "# Create a mapping from original indices to their position in the shrunken dataset\n",
    "index_mapping = {original_idx: shrunken_idx for shrunken_idx, original_idx in enumerate(flat_indices)}\n",
    "\n",
    "# Define the custom collate function\n",
    "def collate_fn(batch_indices_list):\n",
    "    # Convert the original indices to shrunken dataset indices\n",
    "    mapped_indices = [index_mapping[idx] for idx_list in batch_indices_list for idx in idx_list]\n",
    "\n",
    "    batch_ = {\n",
    "        'attention_mask': [shrunken_dataset_train['attention_mask'][i] for i in mapped_indices],\n",
    "        'input_ids': [shrunken_dataset_train['input_ids'][i] for i in mapped_indices],\n",
    "        'labels': [shrunken_dataset_train['labels'][i] for i in mapped_indices]\n",
    "    }\n",
    "    return batch_\n",
    "\n",
    "# Custom Dataset to wrap the batch indices\n",
    "class BatchedDataset(Dataset):\n",
    "    def __init__(self, batch_indices):\n",
    "        self.batch_indices = batch_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.batch_indices[idx]\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(BatchedDataset(small_batches), batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Usage\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "205"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_batches)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "{181: 0,\n 816: 1,\n 327: 2,\n 790: 3,\n 227: 4,\n 896: 5,\n 125: 6,\n 435: 7,\n 618: 8,\n 756: 9,\n 881: 10,\n 55: 11,\n 266: 12,\n 325: 13,\n 345: 14,\n 823: 15,\n 895: 16,\n 975: 17,\n 643: 18,\n 18: 19,\n 471: 20,\n 558: 21,\n 647: 22,\n 653: 23,\n 681: 24,\n 697: 25,\n 764: 26,\n 864: 27,\n 882: 28,\n 917: 29,\n 24: 30,\n 17: 31,\n 35: 32,\n 108: 33,\n 150: 34,\n 194: 35,\n 300: 36,\n 368: 37,\n 672: 38,\n 609: 39,\n 955: 40,\n 755: 41,\n 855: 42,\n 617: 43,\n 122: 44,\n 298: 45,\n 829: 46,\n 51: 47,\n 487: 48,\n 657: 49,\n 490: 50,\n 365: 51,\n 976: 52,\n 470: 53,\n 775: 54,\n 730: 55,\n 85: 56,\n 322: 57,\n 182: 58,\n 530: 59,\n 541: 60,\n 543: 61,\n 548: 62,\n 712: 63,\n 785: 64,\n 151: 65,\n 216: 66,\n 230: 67,\n 483: 68,\n 549: 69,\n 608: 70,\n 711: 71,\n 760: 72,\n 447: 73,\n 250: 74,\n 234: 75,\n 269: 76,\n 443: 77,\n 452: 78,\n 515: 79,\n 313: 80,\n 956: 81,\n 691: 82,\n 102: 83,\n 935: 84,\n 584: 85,\n 592: 86,\n 614: 87,\n 640: 88,\n 934: 89,\n 843: 90,\n 926: 91,\n 544: 92,\n 734: 93,\n 876: 94,\n 696: 95,\n 731: 96,\n 736: 97,\n 773: 98,\n 965: 99,\n 101: 100,\n 255: 101,\n 312: 102,\n 680: 103,\n 662: 104,\n 572: 105,\n 604: 106,\n 690: 107,\n 699: 108,\n 827: 109,\n 922: 110,\n 723: 111,\n 873: 112,\n 905: 113,\n 212: 114,\n 219: 115,\n 655: 116,\n 62: 117,\n 892: 118,\n 299: 119,\n 283: 120,\n 115: 121,\n 840: 122,\n 98: 123,\n 87: 124,\n 0: 125,\n 25: 126,\n 480: 127,\n 522: 128,\n 251: 129,\n 692: 130,\n 525: 131,\n 605: 132,\n 978: 133,\n 792: 134,\n 256: 135,\n 163: 136,\n 781: 137,\n 114: 138,\n 126: 139,\n 382: 140,\n 82: 141,\n 4: 142,\n 80: 143,\n 103: 144,\n 113: 145,\n 143: 146,\n 274: 147,\n 794: 148,\n 364: 149,\n 76: 150,\n 204: 151,\n 286: 152,\n 547: 153,\n 629: 154,\n 613: 155,\n 997: 156,\n 479: 157,\n 718: 158,\n 741: 159,\n 783: 160,\n 521: 161,\n 120: 162,\n 237: 163,\n 306: 164,\n 500: 165,\n 197: 166,\n 223: 167,\n 437: 168,\n 31: 169,\n 503: 170,\n 885: 171,\n 411: 172,\n 465: 173,\n 491: 174,\n 524: 175,\n 687: 176,\n 706: 177,\n 786: 178,\n 184: 179,\n 119: 180,\n 725: 181,\n 820: 182,\n 27: 183,\n 277: 184,\n 570: 185,\n 573: 186,\n 648: 187,\n 602: 188,\n 8: 189,\n 53: 190,\n 918: 191,\n 880: 192,\n 214: 193,\n 369: 194,\n 203: 195,\n 590: 196,\n 213: 197,\n 831: 198,\n 268: 199,\n 538: 200,\n 678: 201,\n 744: 202,\n 186: 203,\n 144: 204,\n 376: 205,\n 740: 206,\n 800: 207,\n 899: 208,\n 342: 209,\n 787: 210,\n 63: 211,\n 287: 212,\n 563: 213,\n 743: 214,\n 907: 215,\n 929: 216,\n 453: 217,\n 838: 218,\n 360: 219,\n 428: 220,\n 588: 221,\n 728: 222,\n 383: 223,\n 626: 224,\n 825: 225,\n 506: 226,\n 735: 227,\n 459: 228,\n 494: 229,\n 199: 230,\n 341: 231,\n 20: 232,\n 532: 233,\n 582: 234,\n 180: 235,\n 693: 236,\n 448: 237,\n 578: 238,\n 585: 239,\n 619: 240,\n 138: 241,\n 278: 242,\n 909: 243,\n 202: 244,\n 65: 245,\n 56: 246,\n 695: 247,\n 769: 248,\n 445: 249,\n 418: 250,\n 473: 251,\n 474: 252,\n 507: 253,\n 571: 254,\n 684: 255,\n 185: 256,\n 362: 257,\n 371: 258,\n 580: 259,\n 721: 260,\n 772: 261,\n 777: 262,\n 594: 263,\n 91: 264,\n 139: 265,\n 545: 266,\n 344: 267,\n 57: 268,\n 384: 269,\n 821: 270,\n 441: 271,\n 477: 272,\n 273: 273,\n 639: 274,\n 663: 275,\n 66: 276,\n 252: 277,\n 338: 278,\n 346: 279,\n 423: 280,\n 702: 281,\n 759: 282,\n 984: 283,\n 49: 284,\n 482: 285,\n 6: 286,\n 36: 287,\n 830: 288,\n 141: 289,\n 309: 290,\n 412: 291,\n 516: 292,\n 132: 293,\n 938: 294,\n 349: 295,\n 398: 296,\n 460: 297,\n 713: 298,\n 931: 299,\n 518: 300,\n 561: 301,\n 883: 302,\n 142: 303,\n 463: 304,\n 7: 305,\n 498: 306,\n 555: 307,\n 836: 308,\n 352: 309,\n 485: 310,\n 941: 311,\n 127: 312,\n 900: 313,\n 919: 314,\n 914: 315,\n 985: 316,\n 611: 317,\n 393: 318,\n 140: 319,\n 233: 320,\n 50: 321,\n 239: 322,\n 377: 323,\n 462: 324,\n 69: 325,\n 394: 326,\n 566: 327,\n 733: 328,\n 802: 329,\n 866: 330,\n 336: 331,\n 844: 332,\n 224: 333,\n 433: 334,\n 319: 335,\n 162: 336,\n 331: 337,\n 858: 338,\n 381: 339,\n 509: 340,\n 54: 341,\n 553: 342,\n 980: 343,\n 14: 344,\n 400: 345,\n 484: 346,\n 16: 347,\n 628: 348,\n 910: 349,\n 644: 350,\n 343: 351,\n 936: 352,\n 796: 353,\n 397: 354,\n 282: 355,\n 335: 356,\n 601: 357,\n 111: 358,\n 136: 359,\n 763: 360,\n 806: 361,\n 804: 362,\n 828: 363,\n 867: 364,\n 416: 365,\n 324: 366,\n 603: 367}"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_list = [num for sublist in small_batches for num in sublist]\n",
    "mapping_dict = {num: idx for idx, num in enumerate(flattened_list)}\n",
    "mapping_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "368"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "[[920],\n [787],\n [475],\n [885],\n [256],\n [374, 469],\n [0],\n [819],\n [64, 410, 491],\n [9],\n [78, 129, 399, 505, 667],\n [697],\n [527],\n [174, 197],\n [561, 564, 682, 738, 852, 856, 957],\n [535],\n [234, 873],\n [861],\n [319, 341, 604, 616, 650],\n [725, 946],\n [128, 607, 829, 967],\n [547],\n [179, 267, 344, 409, 423, 433, 460, 468, 486, 669, 692, 731, 752, 871, 884],\n [921],\n [334, 662, 791, 979],\n [242, 597],\n [960],\n [538],\n [65, 314],\n [329],\n [20],\n [349],\n [514, 892, 912],\n [617],\n [434],\n [168],\n [618, 645],\n [761],\n [259],\n [377],\n [886],\n [777],\n [39],\n [707],\n [769],\n [21, 204, 285, 357, 489, 898, 951],\n [750],\n [257],\n [272],\n [931],\n [250, 336],\n [940],\n [524],\n [690],\n [1],\n [33],\n [325, 508, 649, 658],\n [260],\n [376],\n [360],\n [860],\n [798],\n [571],\n [236, 400, 583, 610],\n [258],\n [843, 923],\n [466],\n [663],\n [86, 88, 211, 387],\n [574],\n [316],\n [942],\n [563],\n [762],\n [153, 201, 215, 365, 452, 529, 606, 894, 917],\n [596],\n [366, 455, 502],\n [117],\n [266, 503],\n [331],\n [708],\n [269, 534],\n [451, 507, 665],\n [318],\n [254, 262],\n [698, 927],\n [415],\n [323, 474, 970, 978],\n [151, 159, 356],\n [996],\n [402],\n [69,\n  167,\n  171,\n  244,\n  292,\n  317,\n  390,\n  432,\n  449,\n  537,\n  567,\n  687,\n  810,\n  818,\n  820,\n  840,\n  845,\n  847],\n [71, 73],\n [103, 205, 263, 333, 392],\n [593, 753, 857, 903],\n [116, 170, 789],\n [362],\n [381],\n [471, 817],\n [91, 275, 470, 523, 528, 605, 724, 747, 765],\n [137, 218, 288, 370, 632, 641, 755],\n [930],\n [573],\n [264, 398, 401, 717, 754, 757, 850, 859],\n [879],\n [130],\n [7, 113, 346, 544],\n [611],\n [383, 439, 494, 646],\n [81]]"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build smaller test set\n",
    "small_batches_valid = [batch for batch in batch_indices_valid if all(idx < 1000 for idx in batch)]\n",
    "small_batches_valid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "110"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_batches_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "{920: 0,\n 787: 1,\n 475: 2,\n 885: 3,\n 256: 4,\n 374: 5,\n 469: 6,\n 0: 7,\n 819: 8,\n 64: 9,\n 410: 10,\n 491: 11,\n 9: 12,\n 78: 13,\n 129: 14,\n 399: 15,\n 505: 16,\n 667: 17,\n 697: 18,\n 527: 19,\n 174: 20,\n 197: 21,\n 561: 22,\n 564: 23,\n 682: 24,\n 738: 25,\n 852: 26,\n 856: 27,\n 957: 28,\n 535: 29,\n 234: 30,\n 873: 31,\n 861: 32,\n 319: 33,\n 341: 34,\n 604: 35,\n 616: 36,\n 650: 37,\n 725: 38,\n 946: 39,\n 128: 40,\n 607: 41,\n 829: 42,\n 967: 43,\n 547: 44,\n 179: 45,\n 267: 46,\n 344: 47,\n 409: 48,\n 423: 49,\n 433: 50,\n 460: 51,\n 468: 52,\n 486: 53,\n 669: 54,\n 692: 55,\n 731: 56,\n 752: 57,\n 871: 58,\n 884: 59,\n 921: 60,\n 334: 61,\n 662: 62,\n 791: 63,\n 979: 64,\n 242: 65,\n 597: 66,\n 960: 67,\n 538: 68,\n 65: 69,\n 314: 70,\n 329: 71,\n 20: 72,\n 349: 73,\n 514: 74,\n 892: 75,\n 912: 76,\n 617: 77,\n 434: 78,\n 168: 79,\n 618: 80,\n 645: 81,\n 761: 82,\n 259: 83,\n 377: 84,\n 886: 85,\n 777: 86,\n 39: 87,\n 707: 88,\n 769: 89,\n 21: 90,\n 204: 91,\n 285: 92,\n 357: 93,\n 489: 94,\n 898: 95,\n 951: 96,\n 750: 97,\n 257: 98,\n 272: 99,\n 931: 100,\n 250: 101,\n 336: 102,\n 940: 103,\n 524: 104,\n 690: 105,\n 1: 106,\n 33: 107,\n 325: 108,\n 508: 109,\n 649: 110,\n 658: 111,\n 260: 112,\n 376: 113,\n 360: 114,\n 860: 115,\n 798: 116,\n 571: 117,\n 236: 118,\n 400: 119,\n 583: 120,\n 610: 121,\n 258: 122,\n 843: 123,\n 923: 124,\n 466: 125,\n 663: 126,\n 86: 127,\n 88: 128,\n 211: 129,\n 387: 130,\n 574: 131,\n 316: 132,\n 942: 133,\n 563: 134,\n 762: 135,\n 153: 136,\n 201: 137,\n 215: 138,\n 365: 139,\n 452: 140,\n 529: 141,\n 606: 142,\n 894: 143,\n 917: 144,\n 596: 145,\n 366: 146,\n 455: 147,\n 502: 148,\n 117: 149,\n 266: 150,\n 503: 151,\n 331: 152,\n 708: 153,\n 269: 154,\n 534: 155,\n 451: 156,\n 507: 157,\n 665: 158,\n 318: 159,\n 254: 160,\n 262: 161,\n 698: 162,\n 927: 163,\n 415: 164,\n 323: 165,\n 474: 166,\n 970: 167,\n 978: 168,\n 151: 169,\n 159: 170,\n 356: 171,\n 996: 172,\n 402: 173,\n 69: 174,\n 167: 175,\n 171: 176,\n 244: 177,\n 292: 178,\n 317: 179,\n 390: 180,\n 432: 181,\n 449: 182,\n 537: 183,\n 567: 184,\n 687: 185,\n 810: 186,\n 818: 187,\n 820: 188,\n 840: 189,\n 845: 190,\n 847: 191,\n 71: 192,\n 73: 193,\n 103: 194,\n 205: 195,\n 263: 196,\n 333: 197,\n 392: 198,\n 593: 199,\n 753: 200,\n 857: 201,\n 903: 202,\n 116: 203,\n 170: 204,\n 789: 205,\n 362: 206,\n 381: 207,\n 471: 208,\n 817: 209,\n 91: 210,\n 275: 211,\n 470: 212,\n 523: 213,\n 528: 214,\n 605: 215,\n 724: 216,\n 747: 217,\n 765: 218,\n 137: 219,\n 218: 220,\n 288: 221,\n 370: 222,\n 632: 223,\n 641: 224,\n 755: 225,\n 930: 226,\n 573: 227,\n 264: 228,\n 398: 229,\n 401: 230,\n 717: 231,\n 754: 232,\n 757: 233,\n 850: 234,\n 859: 235,\n 879: 236,\n 130: 237,\n 7: 238,\n 113: 239,\n 346: 240,\n 544: 241,\n 611: 242,\n 383: 243,\n 439: 244,\n 494: 245,\n 646: 246,\n 81: 247}"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_list_valid = [num for sublist in small_batches_valid for num in sublist]\n",
    "mapping_dict_valid = {num: idx for idx, num in enumerate(flattened_list_valid)}\n",
    "mapping_dict_valid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_dict_valid[787]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[181]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[816]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[327]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[790]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[227]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[896]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[125]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 4\n",
      "})\n",
      "[435, 618, 756, 881]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[55, 266]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[325, 345]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[823]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[895]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[975]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[643]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[18]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[471]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 9\n",
      "})\n",
      "[558, 647, 653, 681, 697, 764, 864, 882, 917]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[24]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[17]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[35]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 6\n",
      "})\n",
      "[108, 150, 194, 300, 368, 672]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[609]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[955]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[755]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[855]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[617]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[122, 298]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[829]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[51]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[487, 657]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[490]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[365]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[976]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[470, 775]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[730]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[85, 322]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[182]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 6\n",
      "})\n",
      "[530, 541, 543, 548, 712, 785]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 8\n",
      "})\n",
      "[151, 216, 230, 483, 549, 608, 711, 760]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[447]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[250]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[234]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 4\n",
      "})\n",
      "[269, 443, 452, 515]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[313, 956]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[691]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[102]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[935]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[584, 592]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[614, 640, 934]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[843]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[926]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[544, 734, 876]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 5\n",
      "})\n",
      "[696, 731, 736, 773, 965]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 4\n",
      "})\n",
      "[101, 255, 312, 680]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[662]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[572, 604, 690]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[699, 827, 922]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[723, 873, 905]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[212, 219, 655]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[62]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[892]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[299]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[283]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[115]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[840]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[98]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[87]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[0]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[25]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[480]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[522]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[251]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[692]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[525, 605, 978]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[792]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[256]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[163, 781]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[114]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[126, 382]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[82]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[4, 80, 103]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[113, 143, 274]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[794]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[364]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 5\n",
      "})\n",
      "[76, 204, 286, 547, 629]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[613, 997]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[479]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[718, 741, 783]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[521]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[120]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[237, 306, 500]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[197]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[223, 437]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[31]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[503, 885]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 7\n",
      "})\n",
      "[411, 465, 491, 524, 687, 706, 786]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[184]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[119, 725, 820]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[27]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 4\n",
      "})\n",
      "[277, 570, 573, 648]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[602]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[8]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[53]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[918]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[880]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[214]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[369]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[203, 590]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[213]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[831]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[268]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[538, 678, 744]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[186]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[144, 376]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[740, 800, 899]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[342]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[787]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 6\n",
      "})\n",
      "[63, 287, 563, 743, 907, 929]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[453]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[838]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 4\n",
      "})\n",
      "[360, 428, 588, 728]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[383, 626, 825]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[506, 735]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[459, 494]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[199, 341]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[20, 532, 582]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[180]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[693]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 4\n",
      "})\n",
      "[448, 578, 585, 619]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[138]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[278, 909]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[202]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[65]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[56]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[695, 769]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[445]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 6\n",
      "})\n",
      "[418, 473, 474, 507, 571, 684]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[185]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[362, 371]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 4\n",
      "})\n",
      "[580, 721, 772, 777]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[594]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[91]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[139]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[545]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[344]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[57]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[384]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[821]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[441, 477]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[273, 639, 663]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[66]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 4\n",
      "})\n",
      "[252, 338, 346, 423]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[702]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[759]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[984]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[49]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[482]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[6]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[36]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[830]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[141]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[309, 412, 516]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[132, 938]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 5\n",
      "})\n",
      "[349, 398, 460, 713, 931]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[518, 561]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[883]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[142]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[463]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[7, 498, 555]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[836]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[352, 485, 941]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[127, 900, 919]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[914, 985]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[611]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[393]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[140]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[233]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[50]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[239]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[377]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[462]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 6\n",
      "})\n",
      "[69, 394, 566, 733, 802, 866]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[336]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[844]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[224, 433]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[319]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[162, 331, 858]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[381, 509]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[54]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[553, 980]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[14, 400, 484]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[16]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[628]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[910]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[644]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[343, 936]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[796]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[397]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[282, 335, 601]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[111]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[136]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[763, 806]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[804, 828, 867]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "[416]\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels'],\n",
      "    num_rows: 2\n",
      "})\n",
      "[324, 603]\n"
     ]
    }
   ],
   "source": [
    "for batch in small_batches:\n",
    "    print(dataset[\"train\"].select(batch))\n",
    "    print(batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "small_datasets_train = [dataset[\"train\"].select(batch) for batch in small_batches]\n",
    "small_datasets_valid = [dataset[\"valid\"].select(batch) for batch in small_batches_valid]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "small_train_dataset = concatenate_datasets(small_datasets_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "\n",
    "small_valid_dataset = concatenate_datasets(small_datasets_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels'],\n        num_rows: 368\n    })\n    valid: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels'],\n        num_rows: 248\n    })\n})"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use list comprehension to get all small datasets and then concatenate them\n",
    "dataset_ = DatasetDict({\n",
    "            'train': small_train_dataset,\n",
    "            'valid': small_valid_dataset\n",
    "        })\n",
    "dataset_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "[181]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_batches[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "[181,\n 816,\n 327,\n 790,\n 227,\n 896,\n 125,\n 435,\n 618,\n 756,\n 881,\n 55,\n 266,\n 325,\n 345,\n 823,\n 895,\n 975,\n 643,\n 18,\n 471,\n 558,\n 647,\n 653,\n 681,\n 697,\n 764,\n 864,\n 882,\n 917,\n 24,\n 17,\n 35,\n 108,\n 150,\n 194,\n 300,\n 368,\n 672,\n 609,\n 955,\n 755,\n 855,\n 617,\n 122,\n 298,\n 829,\n 51,\n 487,\n 657,\n 490,\n 365,\n 976,\n 470,\n 775,\n 730,\n 85,\n 322,\n 182,\n 530,\n 541,\n 543,\n 548,\n 712,\n 785,\n 151,\n 216,\n 230,\n 483,\n 549,\n 608,\n 711,\n 760,\n 447,\n 250,\n 234,\n 269,\n 443,\n 452,\n 515,\n 313,\n 956,\n 691,\n 102,\n 935,\n 584,\n 592,\n 614,\n 640,\n 934,\n 843,\n 926,\n 544,\n 734,\n 876,\n 696,\n 731,\n 736,\n 773,\n 965,\n 101,\n 255,\n 312,\n 680,\n 662,\n 572,\n 604,\n 690,\n 699,\n 827,\n 922,\n 723,\n 873,\n 905,\n 212,\n 219,\n 655,\n 62,\n 892,\n 299,\n 283,\n 115,\n 840,\n 98,\n 87,\n 0,\n 25,\n 480,\n 522,\n 251,\n 692,\n 525,\n 605,\n 978,\n 792,\n 256,\n 163,\n 781,\n 114,\n 126,\n 382,\n 82,\n 4,\n 80,\n 103,\n 113,\n 143,\n 274,\n 794,\n 364,\n 76,\n 204,\n 286,\n 547,\n 629,\n 613,\n 997,\n 479,\n 718,\n 741,\n 783,\n 521,\n 120,\n 237,\n 306,\n 500,\n 197,\n 223,\n 437,\n 31,\n 503,\n 885,\n 411,\n 465,\n 491,\n 524,\n 687,\n 706,\n 786,\n 184,\n 119,\n 725,\n 820,\n 27,\n 277,\n 570,\n 573,\n 648,\n 602,\n 8,\n 53,\n 918,\n 880,\n 214,\n 369,\n 203,\n 590,\n 213,\n 831,\n 268,\n 538,\n 678,\n 744,\n 186,\n 144,\n 376,\n 740,\n 800,\n 899,\n 342,\n 787,\n 63,\n 287,\n 563,\n 743,\n 907,\n 929,\n 453,\n 838,\n 360,\n 428,\n 588,\n 728,\n 383,\n 626,\n 825,\n 506,\n 735,\n 459,\n 494,\n 199,\n 341,\n 20,\n 532,\n 582,\n 180,\n 693,\n 448,\n 578,\n 585,\n 619,\n 138,\n 278,\n 909,\n 202,\n 65,\n 56,\n 695,\n 769,\n 445,\n 418,\n 473,\n 474,\n 507,\n 571,\n 684,\n 185,\n 362,\n 371,\n 580,\n 721,\n 772,\n 777,\n 594,\n 91,\n 139,\n 545,\n 344,\n 57,\n 384,\n 821,\n 441,\n 477,\n 273,\n 639,\n 663,\n 66,\n 252,\n 338,\n 346,\n 423,\n 702,\n 759,\n 984,\n 49,\n 482,\n 6,\n 36,\n 830,\n 141,\n 309,\n 412,\n 516,\n 132,\n 938,\n 349,\n 398,\n 460,\n 713,\n 931,\n 518,\n 561,\n 883,\n 142,\n 463,\n 7,\n 498,\n 555,\n 836,\n 352,\n 485,\n 941,\n 127,\n 900,\n 919,\n 914,\n 985,\n 611,\n 393,\n 140,\n 233,\n 50,\n 239,\n 377,\n 462,\n 69,\n 394,\n 566,\n 733,\n 802,\n 866,\n 336,\n 844,\n 224,\n 433,\n 319,\n 162,\n 331,\n 858,\n 381,\n 509,\n 54,\n 553,\n 980,\n 14,\n 400,\n 484,\n 16,\n 628,\n 910,\n 644,\n 343,\n 936,\n 796,\n 397,\n 282,\n 335,\n 601,\n 111,\n 136,\n 763,\n 806,\n 804,\n 828,\n 867,\n 416,\n 324,\n 603]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_['train']['original_index']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "for i in dataset_['train']['original_index']:\n",
    "    if i == 819:\n",
    "        print(dataset_['train']['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "[181]"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_batches[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "725"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_table[181]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "[725]"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_indices = [mapping_table[idx] for idx in small_batches[0]]\n",
    "original_indices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "[1]"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_indices = [mapping_dict[idx] for idx in small_batches[1]]\n",
    "original_indices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "[816]"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_batches[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_dict[181]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "11"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_['train']['attention_mask'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[dataset_[split_name]['input_ids'][i] for i in small_batches[0]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def collate_fn(batch_indices, split_name):\n",
    "    # Use the mapping table to get the original indices\n",
    "    mapped_indices = [mapping_dict[idx] for idx in batch_indices[0]]\n",
    "\n",
    "    batch_ = {\n",
    "        'attention_mask': [dataset_[split_name]['attention_mask'][i] for i in mapped_indices],\n",
    "        'input_ids': [dataset_[split_name]['input_ids'][i] for i in mapped_indices],\n",
    "        'labels': [dataset_[split_name]['labels'][i] for i in mapped_indices]\n",
    "    }\n",
    "    return batch_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def collate_fn(batch_indices, split_name):\n",
    "        # Given a list of indices, retrieve the corresponding batch from your HuggingFace Dataset\n",
    "    batch_ = {\n",
    "        'attention_mask': [dataset[split_name]['attention_mask'][i] for i in batch_indices[0]],\n",
    "        'input_ids': [dataset[split_name]['input_ids'][i] for i in batch_indices[0]],\n",
    "        'labels': [dataset[split_name]['labels'][i] for i in batch_indices[0]]\n",
    "    }\n",
    "    return batch_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "[[1,\n  404,\n  5798,\n  453,\n  1204,\n  376,\n  1015,\n  2602,\n  1296,\n  2756,\n  3843,\n  7725,\n  2407,\n  817,\n  6581,\n  2230,\n  422,\n  6352,\n  885,\n  6007,\n  675,\n  1421,\n  7029,\n  1524,\n  5810,\n  6630,\n  1354,\n  5256,\n  4,\n  1522,\n  754,\n  1211,\n  2065,\n  5927,\n  378,\n  648,\n  3065,\n  2837,\n  1133,\n  3620,\n  3543,\n  1023,\n  5723,\n  1538,\n  1688,\n  97,\n  461,\n  842,\n  5150,\n  2267,\n  3944,\n  2036,\n  4756,\n  25,\n  572,\n  6245],\n [1,\n  260,\n  38,\n  3356,\n  1121,\n  4169,\n  600,\n  1744,\n  3275,\n  778,\n  479,\n  796,\n  3,\n  3425,\n  2062,\n  6486,\n  6838,\n  3822,\n  1074,\n  3752,\n  5262,\n  7456,\n  6896,\n  1203,\n  3211,\n  2529,\n  6704,\n  1300,\n  7382,\n  749,\n  2155,\n  4669,\n  2297,\n  3726,\n  2400,\n  580,\n  3565,\n  2068,\n  140,\n  1545,\n  3300,\n  350,\n  1920,\n  7455,\n  170,\n  61,\n  525,\n  597,\n  3127,\n  1688,\n  2077,\n  352,\n  1290,\n  2318,\n  56,\n  2662],\n [1,\n  820,\n  602,\n  5306,\n  141,\n  2374,\n  996,\n  1810,\n  2340,\n  459,\n  697,\n  40,\n  4029,\n  2869,\n  492,\n  1481,\n  1055,\n  36,\n  4597,\n  1258,\n  107,\n  724,\n  296,\n  1011,\n  4338,\n  3,\n  636,\n  772,\n  526,\n  1650,\n  463,\n  547,\n  1522,\n  7770,\n  871,\n  2131,\n  429,\n  618,\n  5,\n  5027,\n  375,\n  2760,\n  4295,\n  2631,\n  32,\n  7840,\n  5483,\n  1280,\n  122,\n  4860,\n  1760,\n  685,\n  577,\n  837,\n  4380,\n  3],\n [1,\n  28,\n  1426,\n  5972,\n  974,\n  796,\n  6298,\n  420,\n  1314,\n  3156,\n  735,\n  1062,\n  2037,\n  1198,\n  461,\n  1861,\n  993,\n  961,\n  3166,\n  156,\n  3282,\n  640,\n  1625,\n  3723,\n  1581,\n  2291,\n  1899,\n  31,\n  827,\n  1210,\n  1048,\n  301,\n  1557,\n  2116,\n  27,\n  2532,\n  957,\n  1796,\n  923,\n  961,\n  493,\n  1260,\n  2191,\n  1197,\n  1423,\n  3617,\n  2009,\n  721,\n  6064,\n  2671,\n  1332,\n  3107,\n  60,\n  1142,\n  62,\n  1094],\n [1,\n  820,\n  5787,\n  2286,\n  1792,\n  86,\n  611,\n  5764,\n  300,\n  2933,\n  1904,\n  806,\n  758,\n  2022,\n  2683,\n  296,\n  3988,\n  1679,\n  1119,\n  7085,\n  1156,\n  748,\n  3184,\n  3553,\n  643,\n  7007,\n  1202,\n  3523,\n  954,\n  3091,\n  848,\n  1098,\n  6579,\n  2521,\n  2937,\n  4988,\n  893,\n  5826,\n  3131,\n  2479,\n  2409,\n  5591,\n  4638,\n  4533,\n  441,\n  920,\n  1885,\n  4524,\n  615,\n  2415,\n  25,\n  1141,\n  367,\n  1476,\n  619,\n  3924],\n [1,\n  28,\n  7324,\n  3266,\n  1519,\n  2233,\n  5436,\n  2044,\n  3714,\n  3618,\n  1854,\n  5268,\n  3062,\n  3607,\n  486,\n  53,\n  728,\n  582,\n  78,\n  5221,\n  114,\n  852,\n  503,\n  387,\n  1990,\n  2278,\n  422,\n  490,\n  1652,\n  6263,\n  3510,\n  7529,\n  490,\n  1652,\n  6263,\n  1416,\n  795,\n  348,\n  4223,\n  1104,\n  6193,\n  687,\n  1990,\n  873,\n  349,\n  687,\n  2276,\n  7529,\n  490,\n  1652,\n  6263,\n  7440,\n  6193,\n  3770,\n  1416,\n  795],\n [1,\n  969,\n  1034,\n  1153,\n  801,\n  806,\n  7239,\n  5478,\n  2128,\n  2534,\n  2210,\n  397,\n  2356,\n  1254,\n  3573,\n  116,\n  388,\n  1173,\n  2278,\n  3552,\n  5219,\n  3845,\n  54,\n  595,\n  2939,\n  4595,\n  66,\n  938,\n  5396,\n  2440,\n  5068,\n  1221,\n  3183,\n  4383,\n  1203,\n  1987,\n  4986,\n  1464,\n  4550,\n  2790,\n  2344,\n  803,\n  1958,\n  3178,\n  1112,\n  1463,\n  6465,\n  754,\n  124,\n  1946,\n  3874,\n  7029,\n  42,\n  787,\n  1218,\n  214],\n [1,\n  1171,\n  1683,\n  1041,\n  5583,\n  3452,\n  4463,\n  716,\n  2535,\n  1260,\n  369,\n  5400,\n  2612,\n  571,\n  1080,\n  3513,\n  3066,\n  30,\n  512,\n  7549,\n  2642,\n  1948,\n  4939,\n  1324,\n  1484,\n  3884,\n  1265,\n  3455,\n  5529,\n  2034,\n  1756,\n  7777,\n  1129,\n  4575,\n  38,\n  2140,\n  4370,\n  4584,\n  2097,\n  5760,\n  3981,\n  3469,\n  1398,\n  4474,\n  6922,\n  1206,\n  648,\n  182,\n  2980,\n  4192,\n  3384,\n  887,\n  1195,\n  7479,\n  3225,\n  171]]"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['input_ids'][435]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.utils.data.dataloader.DataLoader at 0x2aac87608df0>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "train_dataloader = DataLoader(small_batches, batch_size=1, pin_memory=True, shuffle=False, num_workers=1,\n",
    "                                    collate_fn=partial(collate_fn, split_name='train'))\n",
    "train_dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'attention_mask': [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]], 'input_ids': [[[1, 404, 6008, 1021, 1774, 1600, 1025, 1108, 3735, 7706, 2597, 3204, 33, 3301, 4265, 2103, 2847, 413, 684, 21, 6563, 1735, 3381, 2638, 4214, 277, 3051, 771, 31, 5480, 3082, 1692, 1477, 7202, 5247, 3082, 1313, 4322, 1837, 662, 1215, 2341, 1174, 1118], [1, 28, 1809, 2144, 410, 559, 740, 2264, 3353, 3403, 1391, 2981, 3407, 3747, 2596, 1361, 1548, 3965, 798, 6363, 4957, 423, 2432, 2613, 3218, 4805, 2096, 647, 217, 2957, 508, 600, 3790, 2920, 2513, 1957, 655, 4053, 1523, 5029, 2156, 6086, 459, 956], [1, 7656, 53, 552, 1766, 1073, 696, 616, 1076, 4015, 1650, 42, 727, 462, 4118, 2808, 5368, 1074, 148, 1290, 409, 428, 191, 671, 1619, 2994, 1591, 65, 1308, 660, 802, 1008, 2629, 282, 2553, 1144, 485, 3710, 1712, 1233, 275, 5635, 2152, 1759], [1, 404, 6, 2063, 734, 6919, 5357, 4597, 640, 7240, 3907, 3375, 4178, 5116, 358, 3249, 6109, 768, 2675, 2861, 3576, 603, 3589, 603, 3566, 2822, 3122, 768, 7598, 43, 177, 1304, 516, 4974, 170, 565, 2458, 696, 869, 338, 29, 4804, 1314, 2572], [1, 1449, 6235, 2709, 3088, 7677, 4481, 2746, 3862, 7870, 1635, 2199, 6660, 7808, 728, 3388, 4743, 1199, 3789, 467, 122, 896, 858, 907, 2402, 60, 1156, 1972, 403, 7138, 3624, 1082, 1361, 6437, 5597, 454, 1199, 693, 34, 2601, 239, 1261, 1455, 2462], [1, 854, 53, 5820, 1182, 2928, 3578, 183, 3462, 6945, 366, 2416, 1486, 2758, 1682, 1992, 613, 7622, 2501, 2055, 985, 1534, 533, 2158, 4401, 443, 1023, 2174, 1841, 2646, 4680, 1142, 1802, 977, 673, 973, 1463, 253, 6022, 572, 2797, 2221, 248, 4894], [1, 854, 3360, 3739, 4011, 1203, 3040, 2503, 2650, 2900, 733, 1163, 2605, 513, 4592, 524, 920, 3999, 1247, 1515, 4547, 183, 1002, 946, 5490, 3610, 1515, 1091, 1034, 4818, 2387, 363, 2187, 4921, 2155, 548, 7298, 2678, 1947, 385, 1721, 3066, 3582, 803], [1, 6850, 2677, 1205, 5077, 4825, 4, 432, 589, 1833, 296, 2532, 1816, 2875, 2315, 954, 1205, 3705, 2913, 2473, 2702, 1803, 6182, 1303, 1604, 681, 2931, 941, 4775, 2711, 512, 1808, 4109, 1685, 5295, 1947, 4428, 467, 2042, 5842, 2012, 5840, 1166, 2919], [1, 667, 2396, 644, 6107, 594, 1966, 1233, 1484, 365, 2268, 612, 2270, 3309, 730, 848, 4100, 1870, 1482, 2011, 666, 2494, 673, 1277, 3136, 3052, 937, 1339, 1569, 1595, 32, 1495, 414, 6291, 3869, 817, 4256, 2082, 5475, 24, 1997, 1535, 1566, 140], [1, 438, 5660, 1184, 414, 800, 5997, 246, 1734, 2788, 1410, 3739, 3519, 1541, 7166, 149, 2727, 5147, 1614, 3360, 7618, 1634, 6538, 3129, 2418, 2834, 7239, 2584, 498, 397, 5106, 7437, 4076, 3760, 4300, 1463, 2322, 392, 4196, 3514, 104, 1020, 2174, 5386], [1, 390, 1081, 4474, 1515, 4667, 5675, 1495, 6167, 1738, 590, 4513, 2994, 880, 7636, 401, 5479, 545, 764, 2637, 5635, 6860, 29, 54, 1686, 7986, 2413, 2497, 516, 1006, 7665, 1677, 2377, 1119, 1989, 4396, 1221, 5176, 5094, 1521, 464, 970, 866, 44]]], 'labels': [[[1, 404, 6008, 1021, 1774, 1600, 1025, 1108, 3735, 7706, 2597, 3204, 33, 3301, 4265, 2103, 2847, 413, 684, 21, 6563, 1735, 3381, 2638, 4214, 277, 3051, 771, 31, 5480, 3082, 1692, 1477, 7202, 5247, 3082, 1313, 4322, 1837, 662, 1215, 2341, 1174, 1118], [1, 28, 1809, 2144, 410, 559, 740, 2264, 3353, 3403, 1391, 2981, 3407, 3747, 2596, 1361, 1548, 3965, 798, 6363, 4957, 423, 2432, 2613, 3218, 4805, 2096, 647, 217, 2957, 508, 600, 3790, 2920, 2513, 1957, 655, 4053, 1523, 5029, 2156, 6086, 459, 956], [1, 7656, 53, 552, 1766, 1073, 696, 616, 1076, 4015, 1650, 42, 727, 462, 4118, 2808, 5368, 1074, 148, 1290, 409, 428, 191, 671, 1619, 2994, 1591, 65, 1308, 660, 802, 1008, 2629, 282, 2553, 1144, 485, 3710, 1712, 1233, 275, 5635, 2152, 1759], [1, 404, 6, 2063, 734, 6919, 5357, 4597, 640, 7240, 3907, 3375, 4178, 5116, 358, 3249, 6109, 768, 2675, 2861, 3576, 603, 3589, 603, 3566, 2822, 3122, 768, 7598, 43, 177, 1304, 516, 4974, 170, 565, 2458, 696, 869, 338, 29, 4804, 1314, 2572], [1, 1449, 6235, 2709, 3088, 7677, 4481, 2746, 3862, 7870, 1635, 2199, 6660, 7808, 728, 3388, 4743, 1199, 3789, 467, 122, 896, 858, 907, 2402, 60, 1156, 1972, 403, 7138, 3624, 1082, 1361, 6437, 5597, 454, 1199, 693, 34, 2601, 239, 1261, 1455, 2462], [1, 854, 53, 5820, 1182, 2928, 3578, 183, 3462, 6945, 366, 2416, 1486, 2758, 1682, 1992, 613, 7622, 2501, 2055, 985, 1534, 533, 2158, 4401, 443, 1023, 2174, 1841, 2646, 4680, 1142, 1802, 977, 673, 973, 1463, 253, 6022, 572, 2797, 2221, 248, 4894], [1, 854, 3360, 3739, 4011, 1203, 3040, 2503, 2650, 2900, 733, 1163, 2605, 513, 4592, 524, 920, 3999, 1247, 1515, 4547, 183, 1002, 946, 5490, 3610, 1515, 1091, 1034, 4818, 2387, 363, 2187, 4921, 2155, 548, 7298, 2678, 1947, 385, 1721, 3066, 3582, 803], [1, 6850, 2677, 1205, 5077, 4825, 4, 432, 589, 1833, 296, 2532, 1816, 2875, 2315, 954, 1205, 3705, 2913, 2473, 2702, 1803, 6182, 1303, 1604, 681, 2931, 941, 4775, 2711, 512, 1808, 4109, 1685, 5295, 1947, 4428, 467, 2042, 5842, 2012, 5840, 1166, 2919], [1, 667, 2396, 644, 6107, 594, 1966, 1233, 1484, 365, 2268, 612, 2270, 3309, 730, 848, 4100, 1870, 1482, 2011, 666, 2494, 673, 1277, 3136, 3052, 937, 1339, 1569, 1595, 32, 1495, 414, 6291, 3869, 817, 4256, 2082, 5475, 24, 1997, 1535, 1566, 140], [1, 438, 5660, 1184, 414, 800, 5997, 246, 1734, 2788, 1410, 3739, 3519, 1541, 7166, 149, 2727, 5147, 1614, 3360, 7618, 1634, 6538, 3129, 2418, 2834, 7239, 2584, 498, 397, 5106, 7437, 4076, 3760, 4300, 1463, 2322, 392, 4196, 3514, 104, 1020, 2174, 5386], [1, 390, 1081, 4474, 1515, 4667, 5675, 1495, 6167, 1738, 590, 4513, 2994, 880, 7636, 401, 5479, 545, 764, 2637, 5635, 6860, 29, 54, 1686, 7986, 2413, 2497, 516, 1006, 7665, 1677, 2377, 1119, 1989, 4396, 1221, 5176, 5094, 1521, 464, 970, 866, 44]]]})\n"
     ]
    }
   ],
   "source": [
    "for batch in enumerate(train_dataloader):\n",
    "    print(batch)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}