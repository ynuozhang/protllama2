{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "checkpoint = torch.load('/data/rozen/home/e0833634/lama/protllama/pl_model_cache/epoch=23-train_perplexity=1.161-val_perplexity=255.593-ppi_10_26_10k_2048.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "{'hparam': Namespace(accumulate_grad_batches=10, attempts='1', batch_size=1, date='10_26', devices=8, epoch=30, flash_attention=True, hidden_size=1280, input_dataset_path='/home/a03-yzhang/projects/protllama2_data/data/ppi/ppi_8000', intermediate_size=3440, learning_rate=0.0001, max_position_embeddings=2048, num_attention_heads=20, num_hidden_layers=30, num_key_value_heads=20, num_workers=8, output_dataset_path='/home/a03-yzhang/projects/protllama2_output/ppi_8000', save_top_k=3, scheduler='cosine', strategy='ddp', target='ppi', tokenizer_path='/home/a03-yzhang/projects/protllama2_data/data/tokenizers/', train_dataloader_length=22735, vocab_size='10k')}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_parameters = checkpoint[\"hyper_parameters\"]\n",
    "hyper_parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-24 03:34:54,580] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 03:34:59.168100: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../bin')\n",
    "from model import pretrainLlama"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hparam': Namespace(accumulate_grad_batches=10, attempts='1', batch_size=1, date='10_26', devices=8, epoch=30, flash_attention=True, hidden_size=1280, input_dataset_path='/home/a03-yzhang/projects/protllama2_data/data/ppi/ppi_8000', intermediate_size=3440, learning_rate=0.0001, max_position_embeddings=2048, num_attention_heads=20, num_hidden_layers=30, num_key_value_heads=20, num_workers=8, output_dataset_path='/home/a03-yzhang/projects/protllama2_output/ppi_8000', save_top_k=3, scheduler='cosine', strategy='ddp', target='ppi', tokenizer_path='/data/rozen/home/e0833634/lama/protllama/batch_script/', train_dataloader_length=22735, vocab_size='10k')}\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "# Namespace is immutable\n",
    "\n",
    "# Assuming you have the original Namespace object\n",
    "original_hparam = hyper_parameters['hparam']\n",
    "\n",
    "# Create a new Namespace object with the updated tokenizer_path\n",
    "new_hparam = Namespace(\n",
    "    accumulate_grad_batches=original_hparam.accumulate_grad_batches,\n",
    "    attempts=original_hparam.attempts,\n",
    "    batch_size=original_hparam.batch_size,\n",
    "    date=original_hparam.date,\n",
    "    devices=original_hparam.devices,\n",
    "    epoch=original_hparam.epoch,\n",
    "    flash_attention=original_hparam.flash_attention,\n",
    "    hidden_size=original_hparam.hidden_size,\n",
    "    input_dataset_path=original_hparam.input_dataset_path,\n",
    "    intermediate_size=original_hparam.intermediate_size,\n",
    "    learning_rate=original_hparam.learning_rate,\n",
    "    max_position_embeddings=original_hparam.max_position_embeddings,\n",
    "    num_attention_heads=original_hparam.num_attention_heads,\n",
    "    num_hidden_layers=original_hparam.num_hidden_layers,\n",
    "    num_key_value_heads=original_hparam.num_key_value_heads,\n",
    "    num_workers=original_hparam.num_workers,\n",
    "    output_dataset_path=original_hparam.output_dataset_path,\n",
    "    save_top_k=original_hparam.save_top_k,\n",
    "    scheduler=original_hparam.scheduler,\n",
    "    strategy=original_hparam.strategy,\n",
    "    target=original_hparam.target,\n",
    "    tokenizer_path='/data/rozen/home/e0833634/lama/protllama/batch_script/',  # Update the tokenizer_path here\n",
    "    train_dataloader_length=original_hparam.train_dataloader_length,\n",
    "    vocab_size=original_hparam.vocab_size\n",
    ")\n",
    "\n",
    "# Update the hyper_parameters with the new Namespace\n",
    "hyper_parameters['hparam'] = new_hparam\n",
    "\n",
    "# Print the updated hyper_parameters to confirm the change\n",
    "print(hyper_parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1280,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3440,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 20,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 10000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pretrainLlama(**hyper_parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.configure_model()\n",
    "model.state_dict()\n",
    "state_dict = checkpoint['state_dict']\n",
    "model.load_state_dict(state_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizerFast, LlamaForCausalLM\n",
    "tokenizer = LlamaTokenizerFast('/data/rozen/home/e0833634/lama/protllama/batch_script/protein_10k.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[   1,  664,    9, 1564,  835, 2888, 3602, 1329, 2650,  208, 3926,  647,\n         3702,  368,  429,  144, 1865, 2583, 3879, 1787, 2407,  857,  139, 1063,\n         3998, 1628,  202,  727, 8715, 3004, 3646, 1153, 1444, 4489, 8600, 3056,\n           42, 3984, 1421,  327, 2232, 3687,  409, 1917,  945, 3956, 2042, 3646,\n         2690,   56, 2171, 1623,  916, 1241, 2968,  214,  860,  667,  559,  561,\n         5001, 3735, 7735, 1219, 4099, 3356, 2133, 3481,  191, 1302, 2276, 9569,\n         8432,  526, 2610, 5790, 1390,  281, 5220, 6575, 1317, 1808,  120, 2996,\n         1607,  860,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"ESQPDPKPDELHKSSKFTGLMENMKVLYDDNHVSAINVKSIDQFLYFDLIYSIKDTKLGNYDNVRVEFKNKDLADKYKDKYVDVFGANYYYQCYFSKKTNDINSHQTDKRKTCMYGGVTEHNGNQLDKYRSITVRVFEDGKNLLSFDVQTNKKKVTAQELDYLTRHYLVKNKKLYEFNNSPYETGYIKFIENENSFWYDMMPAPGDKFDQSKYLMMYNDNKMVDSKDVKIEVYLTTKKK</s>\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 87])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.generation_config.pad_token_id = model.model.generation_config.eos_token_id\n",
    "model.model.generation_config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "model.model.generation_config.max_length = 200\n",
    "# Parameters for manipulation of the model output logits\n",
    "model.model.generation_config.repetition_penalty = 1.2\n",
    "model.model.generation_config.temperature = 0.7 # randomness\n",
    "#model.model.generation_config.top_p = 0.9\n",
    "model.model.generation_config.top_k = 950\n",
    "model.model.generation_config.length_penalty = -0.1\n",
    "# model.model.generation_config.length_penalty = 1 # < 0 encourage shorter sequences\n",
    "# Parameters that control the generation strategy used\n",
    "model.model.generation_config.do_sample = True # use greedy decoding otherwise\n",
    "# Parameters that define the output variables of `generate`\n",
    "model.model.generation_config.num_return_sequences = 2\n",
    "model.model.generation_config.output_scores = True\n",
    "model.model.generation_config.return_dict_in_generate = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "GenerationConfig {\n  \"bos_token_id\": 1,\n  \"do_sample\": true,\n  \"eos_token_id\": 2,\n  \"length_penalty\": -0.1,\n  \"max_length\": 200,\n  \"num_return_sequences\": 2,\n  \"output_scores\": true,\n  \"pad_token_id\": 2,\n  \"repetition_penalty\": 1.2,\n  \"return_dict_in_generate\": true,\n  \"temperature\": 0.7,\n  \"top_k\": 950\n}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.generation_config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rozen/home/e0833634/.local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `-0.1` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": "SampleDecoderOnlyOutput(sequences=tensor([[   1,  664,    9, 1564,  835, 2888, 3602, 1329, 2650,  208, 3926,  647,\n         3702,  368,  429,  144, 1865, 2583, 3879, 1787, 2407,  857,  139, 1063,\n         3998, 1628,  202,  727, 8715, 3004, 3646, 1153, 1444, 4489, 8600, 3056,\n           42, 3984, 1421,  327, 2232, 3687,  409, 1917,  945, 3956, 2042, 3646,\n         2690,   56, 2171, 1623,  916, 1241, 2968,  214,  860,  667,  559,  561,\n         5001, 3735, 7735, 1219, 4099, 3356, 2133, 3481,  191, 1302, 2276, 9569,\n         8432,  526, 2610, 5790, 1390,  281, 5220, 6575, 1317, 1808,  120, 2996,\n         1607,  860,    2,  441,   72,  670, 1661,  597, 3873,  531, 9935,  703,\n          381, 1042, 1286,    7, 1715,  467, 1287, 5460,  317,   31, 2401, 2282,\n          639, 3420, 9857, 2454, 1273, 5587,   81,   89,  656, 6221, 6656, 2347,\n         1129,  458, 3223,  724, 1735, 2941, 2448, 6893,  491, 7528, 1552,  105,\n         1369, 9905, 7511, 1475,  686, 1273,   92,  983,  841, 3434,  332, 1702,\n         2745,  144,  365, 8136,   45, 4598, 2212, 7687, 1274, 2547,  795, 2447,\n           21,  569, 4391, 3561, 5804, 2908, 2182, 1014, 1519, 2384, 2931, 1417,\n          489, 9113, 2505,  229, 4148,  507,  806, 2416, 3004, 4012,   97,  948,\n         1375, 4240, 6315, 1380, 6021,  309, 1544, 1419, 3192, 1716,  193, 2543,\n         1455, 2819, 4403, 2969, 1630, 2666, 2653, 1880],\n        [   1,  664,    9, 1564,  835, 2888, 3602, 1329, 2650,  208, 3926,  647,\n         3702,  368,  429,  144, 1865, 2583, 3879, 1787, 2407,  857,  139, 1063,\n         3998, 1628,  202,  727, 8715, 3004, 3646, 1153, 1444, 4489, 8600, 3056,\n           42, 3984, 1421,  327, 2232, 3687,  409, 1917,  945, 3956, 2042, 3646,\n         2690,   56, 2171, 1623,  916, 1241, 2968,  214,  860,  667,  559,  561,\n         5001, 3735, 7735, 1219, 4099, 3356, 2133, 3481,  191, 1302, 2276, 9569,\n         8432,  526, 2610, 5790, 1390,  281, 5220, 6575, 1317, 1808,  120, 2996,\n         1607,  860,    2,   27,  670,   35, 2291, 3989, 1511, 3881, 2498, 3823,\n         2150, 1697,  807,   33, 6136, 2080, 3301, 4610, 2712, 3023,  481, 4739,\n         1489, 3362, 2200, 5013,   84, 1377, 2683,  348, 3392,   31, 2127,  147,\n         2718, 4148, 3054, 6446, 4351, 4987,  680,  710,  533,  906, 1633,  998,\n         2281, 5512, 2687, 4439,  200,  384, 3717, 8462,  291,  504, 2682, 4914,\n          607, 1799, 3969, 2826,  831,  253, 9481, 1919, 1664,   92, 2125, 2843,\n         1589, 1956, 1053,  173,  449, 5094,  657, 4271,  677, 1133, 2472,  723,\n         2565, 6112, 2201,  571, 1503,  183, 1251,  397,  829, 2527,   59, 2623,\n         1083,  394,   26, 7445, 1016, 2584, 1005, 3360,  427,  509, 2413, 1573,\n          950,  776, 8203, 3047, 3295, 1529,  399,  416]], device='cuda:0'), scores=(tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ...,    -inf, 16.9196,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]],\n       device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ...,    -inf, 10.7812,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,  1.9350]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ..., 11.2109,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[   -inf,    -inf,  1.5067,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf, 13.7928,  ...,    -inf,    -inf,    -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf, 6.9754,  ...,   -inf,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ..., 7.4833,   -inf,   -inf]],\n       device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ..., 5.0167,   -inf,   -inf]],\n       device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ..., -2.2768,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]],\n       device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf, 4.0597],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n        [  -inf,   -inf, 6.8731,  ...,   -inf,   -inf,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[   -inf,    -inf, 10.4074,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[   -inf,    -inf, -3.1490,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf, 12.6674,    -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf, 3.3389,  ...,   -inf,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf, 5.5134]],\n       device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf, 16.7188]],\n       device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf, 7.1150,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ...,    -inf, -0.7750,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n        [  -inf,   -inf, 9.2820,  ...,   -inf,   -inf,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ..., 9.1518,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf]],\n       device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ..., 4.2634,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf]],\n       device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf, 12.8571,    -inf]],\n       device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf, 15.3795, 15.9263]],\n       device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ..., 15.0670,    -inf,    -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf, 4.7433,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf, 14.2857,    -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf, 4.7740]],\n       device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf, 2.9632,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ..., 14.3750,    -inf,    -inf]],\n       device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ..., 9.5759,   -inf,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf, 5.4967],\n        [  -inf,   -inf,   -inf,  ..., 7.7344,   -inf,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf, 14.4196]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ..., 9.5703,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf, 7.0201]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf, 8.4263]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf,   -inf, 9.1853],\n        [  -inf,   -inf, 8.5798,  ...,   -inf,   -inf,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf, 18.3036]],\n       device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ..., 6.3114, 5.6110,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf]],\n       device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ..., 11.6071,    -inf,    -inf]],\n       device='cuda:0'), tensor([[   -inf,    -inf,  4.4248,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ..., 12.7902,    -inf,    -inf]],\n       device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ...,   -inf, 9.3917,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n        [   -inf,    -inf,    -inf,  ...,    -inf,    -inf, 13.0915]],\n       device='cuda:0'), tensor([[  -inf,   -inf,   -inf,  ..., 5.5664,   -inf,   -inf],\n        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf]],\n       device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0')), attentions=None, hidden_states=None)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the custom prefix_allowed_tokens function to enforce conditioning on A\n",
    "model = model.cuda()\n",
    "with torch.cuda.amp.autocast():\n",
    "    # Generate text B conditioned on A\n",
    "    generated_output =model.model.generate(inputs.input_ids,\n",
    "                                           #prefix_allowed_tokens_fn=custom_prefix_allowed_tokens,\n",
    "                                           )\n",
    "generated_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "113"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_generated = len(generated_output.scores)\n",
    "num_tokens_generated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 200])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_output.sequences.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "#works only when you generate one output\n",
    "def calculate_perplexity(output_scores, transition_scores):\n",
    "    #intializing stuff\n",
    "    num_sequences_generated = len(output_scores[0])\n",
    "\n",
    "    perplexities = []\n",
    "\n",
    "    for sequence_idx in range(num_sequences_generated):\n",
    "        logits_for_sequence = [tensor[sequence_idx].cpu().numpy() for tensor in output_scores]  # Collect scores for the same position in the sequence\n",
    "        specific_logits = transition_scores[sequence_idx].tolist()\n",
    "        # initialize a list to hold the indices\n",
    "        indices = []\n",
    "        # iterate through each sub-list and the specific logit at the same time\n",
    "        for i, (logits_for_tensor, specific_logit) in enumerate(zip(logits_for_sequence, specific_logits)):\n",
    "            # find index of specific logit in the sub-list & use small epsilon for float comparison - round errors\n",
    "            logits_for_tensor_np = logits_for_tensor\n",
    "            epsilon = 1e-5\n",
    "            index = next((idx for idx, val in enumerate(logits_for_tensor_np) if abs(val - specific_logit) < epsilon), None)\n",
    "            indices.append(index)\n",
    "            #'indices' now contains the indices of each specific logit in the corresponding sub-list\n",
    "\n",
    "        log_prob = 0.0\n",
    "\n",
    "        for i in range(num_tokens_generated):\n",
    "            #get rid of -inf for numerical stabulity\n",
    "            logits = [float(k) for k in output_scores[i][sequence_idx] if k !=float('-inf')]\n",
    "            #logits = [float(tensor[i]) for tensor in logits_for_sequence if tensor[i] != float('-inf')]#btw, you might want to know that len(logits) = top_k\n",
    "            # Apply the softmax function to convert logits to probabilities\n",
    "\n",
    "            total_sum = np.exp(logits).sum()\n",
    "            probability = np.exp(logits_for_sequence[sequence_idx][indices[i]]) / total_sum\n",
    "            #ppl = e^(-(average of log probability)) #https://huggingface.co/docs/transformers/perplexity\n",
    "            log_prob+=np.log(probability.item())\n",
    "        ppl = np.exp(-log_prob/len(indices))\n",
    "        perplexities.append(ppl)\n",
    "    return perplexities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.868042.hn-10-03/ipykernel_141428/2594907472.py:33: RuntimeWarning: divide by zero encountered in log\n",
      "  log_prob+=np.log(probability.item())\n"
     ]
    }
   ],
   "source": [
    "print(len(generated_output[0][0]))\n",
    "top_k_text = tokenizer.batch_decode(generated_output['sequences'], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "#calculate ppl\n",
    "transition_scores = model.model.compute_transition_scores(\n",
    "    generated_output.sequences, generated_output.scores, normalize_logits=False #normalize_logits applies softmax then log\n",
    "    )\n",
    "transition_scores_np = np.array(transition_scores.cpu())\n",
    "\n",
    "ppl = calculate_perplexity(generated_output.scores,transition_scores_np)\n",
    "print(f\"Top-k Sampling: {top_k_text}\")\n",
    "print('PPL', ppl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.model.can_generate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 113])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores = model.model.compute_transition_scores(\n",
    "    generated_output.sequences, generated_output.scores, normalize_logits=False #normalize_logits applies softmax then log\n",
    "    )\n",
    "transition_scores.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Tensor"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(transition_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 10000])"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_output.scores[0].size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "113"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_output.scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [65]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m generated_output_scores_np \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[43mgenerated_output\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscores\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m())\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "generated_output_scores_np = np.array(generated_output.scores.cpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}