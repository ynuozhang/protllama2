{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "from torch.cuda.amp import autocast\n",
    "from typing import List, Literal, Optional, Tuple, TypedDict\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from model import pretrainLlama\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "\n",
    "class Message(TypedDict):\n",
    "    role: Role\n",
    "    content: str\n",
    "\n",
    "\n",
    "class CompletionPrediction(TypedDict, total=False):\n",
    "    generation: str\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n",
    "\n",
    "\n",
    "class ChatPrediction(TypedDict, total=False):\n",
    "    generation: Message\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n",
    "\n",
    "\n",
    "Dialog = List[Message]\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "SPECIAL_TAGS = [B_INST, E_INST, \"<<SYS>>\", \"<</SYS>>\"]\n",
    "UNSAFE_ERROR = \"Error: special tags are not allowed as part of the prompt.\"\n",
    "\n",
    "\n",
    "class Llama:\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "        model_parallel_size: Optional[int] = None,\n",
    "    ) -> \"Llama\":\n",
    "        checkpoint = torch.load(\n",
    "            '/data/rozen/home/e0833634/lama/protllama/pl_model_cache/epoch=23-train_perplexity=1.161-val_perplexity=255.593-ppi_10_26_10k_2048.ckpt')\n",
    "        hyper_parameters = checkpoint[\"hyper_parameters\"]\n",
    "        original_hparam = hyper_parameters['hparam']\n",
    "\n",
    "        new_hparam = Namespace(\n",
    "            accumulate_grad_batches=original_hparam.accumulate_grad_batches,\n",
    "            attempts=original_hparam.attempts,\n",
    "            batch_size=original_hparam.batch_size,\n",
    "            date=original_hparam.date,\n",
    "            devices=original_hparam.devices,\n",
    "            epoch=original_hparam.epoch,\n",
    "            flash_attention=original_hparam.flash_attention,\n",
    "            hidden_size=original_hparam.hidden_size,\n",
    "            input_dataset_path=original_hparam.input_dataset_path,\n",
    "            intermediate_size=original_hparam.intermediate_size,\n",
    "            learning_rate=original_hparam.learning_rate,\n",
    "            max_position_embeddings=original_hparam.max_position_embeddings,\n",
    "            num_attention_heads=original_hparam.num_attention_heads,\n",
    "            num_hidden_layers=original_hparam.num_hidden_layers,\n",
    "            num_key_value_heads=original_hparam.num_key_value_heads,\n",
    "            num_workers=original_hparam.num_workers,\n",
    "            output_dataset_path=original_hparam.output_dataset_path,\n",
    "            save_top_k=original_hparam.save_top_k,\n",
    "            scheduler=original_hparam.scheduler,\n",
    "            strategy=original_hparam.strategy,\n",
    "            target=original_hparam.target,\n",
    "            tokenizer_path='/data/rozen/home/e0833634/lama/protllama/batch_script/',  # Update the tokenizer_path here\n",
    "            train_dataloader_length=original_hparam.train_dataloader_length,\n",
    "            vocab_size=original_hparam.vocab_size,\n",
    "\n",
    "            max_batch_size=max_batch_size,\n",
    "            max_seq_len=max_seq_len\n",
    "            )\n",
    "\n",
    "        # Update the hyper_parameters with the new Namespace\n",
    "        hyper_parameters['hparam'] = new_hparam\n",
    "        model = pretrainLlama(**hyper_parameters)\n",
    "        model.configure_model()\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        model.load_state_dict(state_dict)\n",
    "        model = model.cuda()\n",
    "        tokenizer = model.tokenizer\n",
    "\n",
    "        return Llama(model, tokenizer)\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tokens: List[List[int]],\n",
    "        max_gen_len: int,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n",
    "        params = self.model.hparam\n",
    "        bsz = len(prompt_tokens)\n",
    "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "        min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "        max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "        assert max_prompt_len <= params.max_seq_len\n",
    "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        pad_id = self.tokenizer.unk_id() #original pad is -1, make it equals to unk to make the id to 0\n",
    "        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "        if logprobs:\n",
    "            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "\n",
    "        prev_pos = 0\n",
    "        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "        input_text_mask = tokens != pad_id\n",
    "        if min_prompt_len == total_len:\n",
    "            #logits = self.model.forward(tokens, prev_pos)\n",
    "            logits = self.model.forward(tokens)\n",
    "            token_logprobs = -F.cross_entropy(\n",
    "                input=logits.transpose(1, 2),\n",
    "                target=tokens,\n",
    "                reduction=\"none\",\n",
    "                ignore_index=pad_id,\n",
    "            )\n",
    "\n",
    "        for cur_pos in range(min_prompt_len, total_len):\n",
    "            with autocast():\n",
    "                logits = self.model.forward(input_ids=tokens[:, prev_pos:cur_pos])[0]\n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if prompt has already been generated\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            if logprobs:\n",
    "                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "                    input=logits.transpose(1, 2),\n",
    "                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "                    reduction=\"none\",\n",
    "                    ignore_index=pad_id,\n",
    "                )\n",
    "            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "                next_token == self.tokenizer.eos_id()\n",
    "            )\n",
    "            prev_pos = cur_pos\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        if logprobs:\n",
    "            token_logprobs = token_logprobs.tolist()\n",
    "        out_tokens, out_logprobs = [], []\n",
    "        for i, toks in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            start = 0 if echo else len(prompt_tokens[i])\n",
    "            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            probs = None\n",
    "            if logprobs:\n",
    "                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to eos tok if any\n",
    "            #if self.tokenizer.eos_id() in toks:\n",
    "                #eos_idx = toks.index(self.tokenizer.eos_id())\n",
    "                #toks = toks[:eos_idx]\n",
    "                #probs = probs[:eos_idx] if logprobs else None\n",
    "            out_tokens.append(toks)\n",
    "            out_logprobs.append(probs)\n",
    "        return (out_tokens, out_logprobs if logprobs else None)\n",
    "\n",
    "    def text_completion(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> List[CompletionPrediction]:\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.model.hparam.max_seq_len - 1\n",
    "        #prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=True) for x in prompts]\n",
    "        prompt_tokens = []\n",
    "        for x in prompts:\n",
    "            t = [self.tokenizer.bos_id()] + self.tokenizer.encode(x) + [self.tokenizer.eos_id()]\n",
    "            prompt_tokens.append(t)\n",
    "        generation_tokens, generation_logprobs = self.generate(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            logprobs=logprobs,\n",
    "            echo=echo,\n",
    "        )\n",
    "        if logprobs:\n",
    "            return [\n",
    "                {\n",
    "                    \"generation\": self.tokenizer.decode(t),\n",
    "                    \"tokens\": [self.tokenizer.decode(x) for x in t],\n",
    "                    \"logprobs\": logprobs_i,\n",
    "                }\n",
    "                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "            ]\n",
    "        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]\n",
    "\n",
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator = Llama.build(\n",
    "    max_seq_len=2048,\n",
    "    max_batch_size=2,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompts: List[str] = [\n",
    "        # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "    \"YAPSALVLTVGKGVSATTAAPERAVTLTCAPGPSGTHPAAGSACADLAAVGGDLNALTRGEDVMCPMVYDPVLLTVDGVWQGKRVSYERVFSNECEMNAHGSSVFAF\",\n",
    "    \"DFVLDNEGNPLENGGTYYILSDITAFGGIRAAPTGNERCPLTVVQSRNELDKGIGTIISSPYRIRFIAEGHPLSLKFDSFAVIMLCVGIPTEWSVVEDLPEGPAVKIGENKDAMDGWFRLERVSDDEFNNYKLVFCPQKCGDIGISIDHDDGTRRLVVSKNKPLVVQFQKLD\"\n",
    "        # Few shot prompt (providing a few examples before asking model to complete more);\n",
    "        #\"\"\"Translate English to French:\n",
    "\n",
    "        #sea otter => loutre de mer\n",
    "        #peppermint => menthe poivrÃ©e\n",
    "        #plush girafe => girafe peluche\n",
    "        #cheese =>\"\"\",\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = generator.text_completion(\n",
    "        prompts,\n",
    "        max_gen_len=64,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        echo=True\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for prompt, result in zip(prompts, results):\n",
    "    print(prompt)\n",
    "    print(f\"> {result['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}