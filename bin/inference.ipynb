{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-03 15:13:12,319] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 15:13:16.649141: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "from torch.cuda.amp import autocast\n",
    "from typing import List, Literal, Optional, Tuple, TypedDict\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from model import pretrainLlama\n",
    "from argparse import Namespace\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "\n",
    "class Message(TypedDict):\n",
    "    role: Role\n",
    "    content: str\n",
    "\n",
    "\n",
    "class CompletionPrediction(TypedDict, total=False):\n",
    "    generation: str\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n",
    "\n",
    "\n",
    "class ChatPrediction(TypedDict, total=False):\n",
    "    generation: Message\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n",
    "\n",
    "\n",
    "Dialog = List[Message]\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "SPECIAL_TAGS = [B_INST, E_INST, \"<<SYS>>\", \"<</SYS>>\"]\n",
    "UNSAFE_ERROR = \"Error: special tags are not allowed as part of the prompt.\"\n",
    "\n",
    "\n",
    "class Llama:\n",
    "    \"\"\"Modified from llama2 github example_text_completion.py\"\"\"\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "        model_parallel_size: Optional[int] = None,\n",
    "    ) -> \"Llama\":\n",
    "        checkpoint = torch.load(\n",
    "            '/data/rozen/home/e0833634/lama/protllama/pl_model_cache/epoch=23-train_perplexity=1.161-val_perplexity=255.593-ppi_10_26_10k_2048.ckpt')\n",
    "        hyper_parameters = checkpoint[\"hyper_parameters\"]\n",
    "        original_hparam = hyper_parameters['hparam']\n",
    "\n",
    "        new_hparam = Namespace(\n",
    "            accumulate_grad_batches=original_hparam.accumulate_grad_batches,\n",
    "            attempts=original_hparam.attempts,\n",
    "            batch_size=original_hparam.batch_size,\n",
    "            date=original_hparam.date,\n",
    "            devices=original_hparam.devices,\n",
    "            epoch=original_hparam.epoch,\n",
    "            flash_attention=original_hparam.flash_attention,\n",
    "            hidden_size=original_hparam.hidden_size,\n",
    "            input_dataset_path=original_hparam.input_dataset_path,\n",
    "            intermediate_size=original_hparam.intermediate_size,\n",
    "            learning_rate=original_hparam.learning_rate,\n",
    "            max_position_embeddings=original_hparam.max_position_embeddings,\n",
    "            num_attention_heads=original_hparam.num_attention_heads,\n",
    "            num_hidden_layers=original_hparam.num_hidden_layers,\n",
    "            num_key_value_heads=original_hparam.num_key_value_heads,\n",
    "            num_workers=original_hparam.num_workers,\n",
    "            output_dataset_path=original_hparam.output_dataset_path,\n",
    "            save_top_k=original_hparam.save_top_k,\n",
    "            scheduler=original_hparam.scheduler,\n",
    "            strategy=original_hparam.strategy,\n",
    "            target=original_hparam.target,\n",
    "            tokenizer_path='/data/rozen/home/e0833634/lama/protllama/batch_script/',  # Update the tokenizer_path here\n",
    "            train_dataloader_length=original_hparam.train_dataloader_length,\n",
    "            vocab_size=original_hparam.vocab_size,\n",
    "\n",
    "            max_batch_size=max_batch_size,\n",
    "            max_seq_len=max_seq_len\n",
    "            )\n",
    "\n",
    "        # Update the hyper_parameters with the new Namespace\n",
    "        hyper_parameters['hparam'] = new_hparam\n",
    "        model = pretrainLlama(**hyper_parameters)\n",
    "        model.configure_model()\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        model.load_state_dict(state_dict)\n",
    "        model = model.cuda()\n",
    "        tokenizer = model.tokenizer\n",
    "\n",
    "        return Llama(model, tokenizer)\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tokens: List[List[int]],\n",
    "        max_gen_len: int,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        repetition_penalty: float = 1.0,\n",
    "        num_targets: int = 1,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> Tuple[List[List[List[int]]], Optional[List[List[List[float]]]]]:\n",
    "        params = self.model.hparam\n",
    "        bsz = len(prompt_tokens)\n",
    "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "        min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "        max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "        assert max_prompt_len <= params.max_seq_len\n",
    "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        pad_id = self.tokenizer.unk_id() #original pad is -1, make it equals to unk to make the id to 0\n",
    "        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "        #tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long)\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "            #tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long)\n",
    "        if logprobs:\n",
    "            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "\n",
    "        prev_pos = 0\n",
    "        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "        #eos_reached = torch.tensor([False] * bsz)\n",
    "        input_text_mask = tokens != pad_id\n",
    "        #if min_prompt_len == total_len:\n",
    "            #logits = self.model.forward(tokens, prev_pos)\n",
    "            #logits = self.model.forward(tokens)\n",
    "            #token_logprobs = -F.cross_entropy(\n",
    "                #input=logits.transpose(1, 2),\n",
    "                #target=tokens,\n",
    "                #reduction=\"none\",\n",
    "                #ignore_index=pad_id,\n",
    "            #)\n",
    "\n",
    "        perplexity = 0.0\n",
    "\n",
    "        output_tokens = []\n",
    "        output_logprobs = []\n",
    "\n",
    "        for _ in range(num_targets):\n",
    "            curr_tokens = tokens.clone()\n",
    "            for cur_pos in range(min_prompt_len, total_len):\n",
    "                with autocast():\n",
    "                    #print(tokens[:, prev_pos:cur_pos])\n",
    "                    logits = self.model.forward(input_ids=curr_tokens[:, prev_pos:cur_pos])[0]\n",
    "                if temperature > 0:\n",
    "                    probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "\n",
    "                    if repetition_penalty != 1.0:\n",
    "                        for i in range(bsz):\n",
    "                            for token_id in curr_tokens[i, prev_pos:cur_pos]:\n",
    "                                if token_id in curr_tokens[i, prev_pos:cur_pos]:\n",
    "                                    probs[i, token_id] /= repetition_penalty\n",
    "                    next_token = sample_top_p(probs, top_p)\n",
    "                else:\n",
    "                    next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "                next_token = next_token.reshape(-1)\n",
    "                # only replace token if prompt has already been generated\n",
    "                next_token = torch.where(\n",
    "                    input_text_mask[:, cur_pos], curr_tokens[:, cur_pos], next_token\n",
    "                )\n",
    "                curr_tokens[:, cur_pos] = next_token\n",
    "                if logprobs:\n",
    "                    token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "                        input=logits.transpose(1, 2),\n",
    "                        target=curr_tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "                        reduction=\"none\",\n",
    "                        ignore_index=pad_id,\n",
    "                    )\n",
    "                    perplexity += torch.exp(token_logprobs[:, cur_pos])\n",
    "                eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "                    next_token == self.tokenizer.eos_id()\n",
    "                )\n",
    "                prev_pos = cur_pos\n",
    "                if all(eos_reached):\n",
    "                    break\n",
    "\n",
    "            output_tokens.append(curr_tokens.tolist())\n",
    "            if logprobs:\n",
    "                output_logprobs.append(token_logprobs.tolist())\n",
    "        perplexity = float(perplexity.mean())\n",
    "\n",
    "        #if logprobs:\n",
    "            #token_logprobs = token_logprobs.tolist()\n",
    "        #out_tokens, out_logprobs = [], []\n",
    "        #for i, toks in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            #start = 0 if echo else len(prompt_tokens[i])\n",
    "            #toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            #probs = None\n",
    "            #if logprobs:\n",
    "                #probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to eos tok if any\n",
    "            #if self.tokenizer.eos_id() in toks:\n",
    "                #eos_idx = toks.index(self.tokenizer.eos_id())\n",
    "                #toks = toks[:eos_idx]\n",
    "                #probs = probs[:eos_idx] if logprobs else None\n",
    "            #out_tokens.append(toks)\n",
    "            #out_logprobs.append(probs)\n",
    "        return (output_tokens, output_logprobs if logprobs else None, perplexity)\n",
    "\n",
    "    def text_completion(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        repetition_penalty: float = 1.0,\n",
    "        num_targets: int = 1,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> List[CompletionPrediction]:\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.model.hparam.max_seq_len - 1\n",
    "        #prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=True) for x in prompts]\n",
    "        prompt_tokens = []\n",
    "        for x in prompts:\n",
    "            t = [self.tokenizer.bos_id()] + self.tokenizer.encode(x) + [self.tokenizer.eos_id()]\n",
    "            prompt_tokens.append(t)\n",
    "        generation_tokens, generation_logprobs, perplexities = self.generate(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            num_targets=num_targets,\n",
    "            logprobs=logprobs,\n",
    "            echo=echo,\n",
    "        )\n",
    "        if not isinstance(perplexities, list):\n",
    "            perplexities = [perplexities] * len(generation_tokens)\n",
    "        completions = []\n",
    "        for t, logprobs_i, perplexity in zip (generation_tokens, generation_logprobs, perplexities):\n",
    "            for target_tokens, logprobs_t in zip(t, logprobs_i):\n",
    "                completion = {\n",
    "                    \"generation\": self.tokenizer.decode(target_tokens),\n",
    "                    \"tokens\": [self.tokenizer.decode(x) for x in target_tokens],\n",
    "                    #\"logprobs\": logprobs_t,\n",
    "                    \"perplexity\": perplexity\n",
    "                }\n",
    "            completions.append(completion)\n",
    "        return completions\n",
    "\n",
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1280,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3440,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 20,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 10000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generator = Llama.build(\n",
    "    max_seq_len=2048,\n",
    "    max_batch_size=6,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "prompts: List[str] = [\n",
    "        # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "    \"YAPSALVLTVGKGVSATTAAPERAVTLTCAPGPSGTHPAAGSACADLAAVGGDLNALTRGEDVMCPMVYDPVLLTVDGVWQGKRVSYERVFSNECEMNAHGSSVFAF\",\n",
    "    \"ASSAVVFKQMVLQQALPMTLKGLDKASELATLTPEGLAREHSRLASGDGALRSLSTALAGIRAGSQVEESRIQAGRLLERSIGGIALQQWGTTGGAASQLVLDASPELRREITDQLHQVMSEVALLRQAVESEVS\",\n",
    "    \"QMNYEEVIKKYRGEENFDHAAYDWRLHSGVTPVKDQKNCGSCWAFSSIGSVESQYAIRKNKLITLSEQELVDCSFKNYGCNGGLINNAFEDMIELGGICPDGDYPYVSDAPNLCNIDRCTEKYGIKNYLSVPDNKLKEALRFLGPISISVAVSDDFAFYKEGIFDGECGDQLNHAVMLVGFGMKEIVNPLTKKGEKHYYYIIKNSWGQQWGERGFINIETDESGLMRKCGLGTDAFIPLIE\",\n",
    "    \"VQLQESGGGLVQPGGSLRLSCAASGSISSITTMGWYRQDGRELVALINSVGDTTYAGSVKGRFTISRDNAKNTVYLEMSSLKPEDTAVYYCNAFMSTNSGRTGSFWGQGTQVTVSS\",\n",
    "    \"AYSFKVVLLGEGCVGKTSLVLRYCTTLQASFLTKKLNIGGKRVNLAIWDTAGQERFHALGPIYYRDSNGAILVYDITDEDSFQKVKNWVKELRKMLGNEICLCIVGNKIDLEKERHVSIQEAESYAESVGAKHYHTSAKQNKGIEELFLDLCKRMIE\",\n",
    "    \"ARRKEFIMAELIQTEKAYVRDLRECMDTYLWEMTSGVEEIPPGIVNKELIIFGNMQEIYEFHNNIFLKELEKYEQLPEDVGHCFVTWADKFQMYVTYCKNKPDSTQLILEHAGSYFDEIQQRHGLANSISSYLIKPVQRITKYQLLLKELLTCCEEGKGEIKDGLEVMLSVPKRANDAMHLSMLEGFSQGELILQESFQHLFLFEMSLVFSKEVKDSSGRSKYLYKSKLFTSELGVTEHVEGDPCKFATSDNKIVLKASSIENKQDWIKHIREVIQERT\"\n",
    "        # Few shot prompt (providing a few examples before asking model to complete more);\n",
    "        #\"\"\"Translate English to French:\n",
    "\n",
    "        #sea otter => loutre de mer\n",
    "        #peppermint => menthe poivrÃ©e\n",
    "        #plush girafe => girafe peluche\n",
    "        #cheese =>\"\"\",\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "results = generator.text_completion(\n",
    "        prompts,\n",
    "        max_gen_len=64,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        #repetition_penalty=2,\n",
    "        echo=True,\n",
    "        logprobs=True\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['YAPSALVLTVGKGVSATTAAPERAVTLTCAPGPSGTHPAAGSACADLAAVGGDLNALTRGEDVMCPMVYDPVLLTVDGVWQGKRVSYERVFSNECEMNAHGSSVFAF',\n 'ASSAVVFKQMVLQQALPMTLKGLDKASELATLTPEGLAREHSRLASGDGALRSLSTALAGIRAGSQVEESRIQAGRLLERSIGGIALQQWGTTGGAASQLVLDASPELRREITDQLHQVMSEVALLRQAVESEVS',\n 'QMNYEEVIKKYRGEENFDHAAYDWRLHSGVTPVKDQKNCGSCWAFSSIGSVESQYAIRKNKLITLSEQELVDCSFKNYGCNGGLINNAFEDMIELGGICPDGDYPYVSDAPNLCNIDRCTEKYGIKNYLSVPDNKLKEALRFLGPISISVAVSDDFAFYKEGIFDGECGDQLNHAVMLVGFGMKEIVNPLTKKGEKHYYYIIKNSWGQQWGERGFINIETDESGLMRKCGLGTDAFIPLIE',\n 'VQLQESGGGLVQPGGSLRLSCAASGSISSITTMGWYRQDGRELVALINSVGDTTYAGSVKGRFTISRDNAKNTVYLEMSSLKPEDTAVYYCNAFMSTNSGRTGSFWGQGTQVTVSS',\n 'AYSFKVVLLGEGCVGKTSLVLRYCTTLQASFLTKKLNIGGKRVNLAIWDTAGQERFHALGPIYYRDSNGAILVYDITDEDSFQKVKNWVKELRKMLGNEICLCIVGNKIDLEKERHVSIQEAESYAESVGAKHYHTSAKQNKGIEELFLDLCKRMIE',\n 'ARRKEFIMAELIQTEKAYVRDLRECMDTYLWEMTSGVEEIPPGIVNKELIIFGNMQEIYEFHNNIFLKELEKYEQLPEDVGHCFVTWADKFQMYVTYCKNKPDSTQLILEHAGSYFDEIQQRHGLANSISSYLIKPVQRITKYQLLLKELLTCCEEGKGEIKDGLEVMLSVPKRANDAMHLSMLEGFSQGELILQESFQHLFLFEMSLVFSKEVKDSSGRSKYLYKSKLFTSELGVTEHVEGDPCKFATSDNKIVLKASSIENKQDWIKHIREVIQERT']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAPSALVLTVGKGVSATTAAPERAVTLTCAPGPSGTHPAAGSACADLAAVGGDLNALTRGEDVMCPMVYDPVLLTVDGVWQGKRVSYERVFSNECEMNAHGSSVFAF\n",
      "> {'generation': 'ARRKEFIMAELIQTEKAYVRDLRECMDTYLWEMTSGVEEIPPGIVNKELIIFGNMQEIYEFHNNIFLKELEKYEQLPEDVGHCFVTWADKFQMYVTYCKNKPDSTQLILEHAGSYFDEIQQRHGLANSISSYLIKPVQRITKYQLLLKELLTCCEEGKGEIKDGLEVMLSVPKRANDAMHLSMLEGFSQGELILQESFQHLFLFEMSLVFSKEVKDSSGRSKYLYKSKLFTSELGVTEHVEGDPCKFATSDNKIVLKASSIENKQDWIKHIREVIQERTMVDKIEEYRKFRTMDLSKEHEDLHLFSDGPQGKVLD MSHYVKVLDVPRFSDCFGGPQGKVLDDNGEHKGPQGKVLDGPQGKVLDCCRKVLDGPQGKVLDKLLKKFRVCGKHYMSQKVLDLMTSEAEFSDHSDMTGNFQLMTYNKLSKEHNAGPQGKVLDERVRKVWWRNFSDKVLDGPQGKVLDWREGPQGKVLDGPQGKVLDGPQGKVLDFSDGGGHRLRE', 'tokens': ['', '', 'ARR', 'KEF', 'IMA', 'ELIQ', 'TEK', 'AYV', 'RDL', 'REC', 'MDT', 'YLW', 'EMT', 'SGV', 'EEI', 'PPG', 'IVN', 'KEL', 'II', 'FGN', 'MQE', 'IYE', 'FHN', 'NIF', 'LKEL', 'EKY', 'EQL', 'PEDV', 'GHC', 'FVT', 'WA', 'DKF', 'QM', 'YVT', 'YC', 'KNK', 'PDS', 'TQL', 'IL', 'EHA', 'GSY', 'FDE', 'IQQ', 'RHG', 'LAN', 'SIS', 'SYL', 'IKP', 'VQR', 'ITK', 'YQL', 'LLKE', 'LLT', 'CC', 'EEG', 'KGE', 'IKD', 'GLEV', 'MLS', 'VPK', 'RAN', 'DAM', 'HLS', 'MLE', 'GFS', 'QGE', 'LI', 'LQE', 'SFQ', 'HLF', 'LFE', 'MSL', 'VFS', 'KEVK', 'DSS', 'GRS', 'KYL', 'YKS', 'KLF', 'TSE', 'LGV', 'TEH', 'VEG', 'DPC', 'KFA', 'TSD', 'NKI', 'VLK', 'ASS', 'IEN', 'KQ', 'DWI', 'KHI', 'REV', 'IQE', 'RT', '', 'MVD', 'KIE', 'EYR', 'KFR', 'TMD', 'LSKE', 'HED', 'LHL', 'FSD', 'GPQG', 'KVLD', 'MS', 'HYV', 'KVLD', 'VPR', 'FSD', 'CFG', 'GPQG', 'KVLD', 'DNG', 'EHK', 'GPQG', 'KVLD', 'GPQG', 'KVLD', 'CCR', 'KVLD', 'GPQG', 'KVLD', 'KLLK', 'KFR', 'VCG', 'KHY', 'MSQ', 'KVLD', 'LMT', 'SEAE', 'FSD', 'HSD', 'MTG', 'NFQ', 'LMT', 'YNK', 'LSKE', 'HNA', 'GPQG', 'KVLD', 'ERVR', 'KVW', 'WRN', 'FSD', 'KVLD', 'GPQG', 'KVLD', 'WRE', 'GPQG', 'KVLD', 'GPQG', 'KVLD', 'GPQG', 'KVLD', 'FSD', 'GGGH', 'RLRE'], 'perplexity': 6.2092132568359375}\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt, result in zip(prompts, results):\n",
    "    print(prompt)\n",
    "    print(f\"> {result}\")\n",
    "    print(\"\\n==================================\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# running alphafold for the generated sequence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "import subprocess"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Alphafold script execution failed\n",
      "Stdout: \n",
      "Stderr: /data/rozen/home/e0833634/lama/protllama/batch_script/run_alphafold_example.sh: line 44: singularity: command not found\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for any errors\n",
    "if process.returncode != 0:\n",
    "    print(\"Error: Alphafold script execution failed\")\n",
    "    print(\"Stdout:\", stdout)\n",
    "    print(\"Stderr:\", stderr)\n",
    "else:\n",
    "    # Alphafold script executed successfully\n",
    "    print(\"Alphafold script executed successfully\")\n",
    "\n",
    "    # Parse the output to find the best PLDDT or iPTM value\n",
    "    # You may need to parse the output based on the specific format of your script's output\n",
    "    # Look for lines containing PLDDT or iPTM values and extract the relevant information\n",
    "    # Example: Search for lines with \"PLDDT:\" or \"iPTM:\" and extract the values\n",
    "\n",
    "    best_plddt = None\n",
    "    best_iptm = None\n",
    "\n",
    "    for line in stdout.split('\\n'):\n",
    "        if \"PLDDT:\" in line:\n",
    "            # Extract the PLDDT value (you may need to parse it further)\n",
    "            plddt_value = line.split(\"PLDDT:\")[1].strip()\n",
    "            best_plddt = float(plddt_value)\n",
    "        elif \"iPTM:\" in line:\n",
    "            # Extract the iPTM value (you may need to parse it further)\n",
    "            iptm_value = line.split(\"iPTM:\")[1].strip()\n",
    "            best_iptm = float(iptm_value)\n",
    "\n",
    "    if best_plddt is not None:\n",
    "        print(f\"Best PLDDT: {best_plddt}\")\n",
    "\n",
    "    if best_iptm is not None:\n",
    "        print(f\"Best iPTM: {best_iptm}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "alphafold_script_path = \"/data/rozen/home/e0833634/lama/protllama/batch_script/run_alphafold_example.sh\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "# Execute the alphafold.sh script and capture the output\n",
    "process = subprocess.Popen([\"bash\", alphafold_script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "stdout, stderr = process.communicate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}