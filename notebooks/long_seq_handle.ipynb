{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = ['GLTNAFIASAPAREVRYDGVITPANANYRFMGGDKGGSLTVGSHLTGSNMVTIGPMGVVVFTNNNDYTGNTFIMGGGTLQLGSNTAWGSLPN\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFPLSPAQLGIWYAQHLDPQVPITIAQYVDLHGALDVEVLERASIDASHELGSGFLRIVERDGEPLQYV\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFP\\n',\n",
    "        'MDRLDFGGGE\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sequences'],\n    num_rows: 4\n})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "train_dataset = Dataset.from_dict({'sequences': text})\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3] at entry 0 and [5] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 53>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     50\u001B[0m loader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, collate_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m data: collate_fn(data, desired_max_sequence_length))\n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# Iterate over batches\u001B[39;00m\n\u001B[0;32m---> 53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m loader:\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;66;03m# Your training loop here\u001B[39;00m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28mprint\u001B[39m(batch)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m     47\u001B[0m dataset \u001B[38;5;241m=\u001B[39m FastaBatchedDataset(train_dataset, tokenizer)\n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m# Wrap the dataset in DataLoader\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m loader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, collate_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m data: \u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdesired_max_sequence_length\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# Iterate over batches\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m loader:\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;66;03m# Your training loop here\u001B[39;00m\n",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36mcollate_fn\u001B[0;34m(data, max_sequence_length, extra_toks_per_seq)\u001B[0m\n\u001B[1;32m     37\u001B[0m     current_buf_len \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m sz\n\u001B[1;32m     39\u001B[0m _flush_current_buf()\n\u001B[0;32m---> 40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatches\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [3] at entry 0 and [5] at entry 1"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FastaBatchedDataset(Dataset):\n",
    "    def __init__(self, sequence_strs, tokenizer):\n",
    "        self.sequence_strs = list(sequence_strs['sequences'])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized_sequences = [self.tokenizer.encode(s) for s in self.sequence_strs]\n",
    "\n",
    "    # Implement the __len__ method\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_sequences)\n",
    "\n",
    "    # Implement the __getitem__ method\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_sequences[idx]\n",
    "def collate_fn(data, max_sequence_length, extra_toks_per_seq=1):\n",
    "    sizes = [(len(tokens), tokens) for tokens in data]\n",
    "    sizes.sort(key=lambda x: x[0])\n",
    "\n",
    "    batches = []\n",
    "    buf = []\n",
    "    current_buf_len = 0\n",
    "\n",
    "    def _flush_current_buf():\n",
    "        nonlocal current_buf_len, buf\n",
    "        if len(buf) == 0:\n",
    "            return\n",
    "        batches.extend(buf)\n",
    "        buf = []\n",
    "        current_buf_len = 0\n",
    "\n",
    "    for sz, tokens in sizes:\n",
    "        sz += extra_toks_per_seq\n",
    "        if current_buf_len + sz > max_sequence_length:\n",
    "            _flush_current_buf()\n",
    "        buf.append(torch.tensor(tokens))\n",
    "        current_buf_len += sz\n",
    "\n",
    "    _flush_current_buf()\n",
    "    return torch.stack(batches)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "desired_max_sequence_length = 60\n",
    "train_dataset = {\"sequences\": [\"A\" * 10, \"A\" * 20, \"A\" * 60, \"A\" * 100]}\n",
    "tokenizer_path = '/data/rozen/home/e0833634/lama/protllama/batch_script/'\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path+\"protein_8k.model\")\n",
    "dataset = FastaBatchedDataset(train_dataset, tokenizer)\n",
    "\n",
    "# Wrap the dataset in DataLoader\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda data: collate_fn(data, desired_max_sequence_length))\n",
    "\n",
    "# Iterate over batches\n",
    "for batch in loader:\n",
    "    # Your training loop here\n",
    "    print(batch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GLTNAFIASAPAREVRYDGVITPANANYRFMGGDKGGSLTVGSHLTGSNMVTIGPMGVVVFTNNNDYTGNTFIMGGGTLQLGSNTAWGSLPN\\n', 'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFPLSPAQLGIWYAQHLDPQVPITIAQYVDLHGALDVEVLERASIDASHELGSGFLRIVERDGEPLQYV\\n', 'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFP\\n', 'MDRLDFGGGE\\n']\n",
      "[(11, 3), (34, 2), (93, 0), (100, 1)]\n",
      "[[3], [2], [0], [1]]\n",
      "['GLTNAFIASAPAREVRYDGVITPANANYRFMGGDKGGSLTVGSHLTGSNMVTIGPMGVVVFTNNNDYTGNTFIMGGGTLQLGSNTAWGSLPN\\n', 'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFPLSPAQLGIWYAQHLDPQVPITIAQYVDLHGALDVEVLERASIDASHELGSGFLRIVERDGEPLQYV\\n', 'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFP\\n', 'MDRLDFGGGE\\n']\n",
      "[(11, 3), (34, 2), (93, 0), (100, 1)]\n",
      "[[3], [2], [0], [1]]\n"
     ]
    }
   ],
   "source": [
    "class FastaBatchedDataset(object):\n",
    "    \"\"\"inspired by esm2, but instead of sorting the original sequences,\n",
    "    we should really sorting based on tokenized sequences\n",
    "    \"\"\"\n",
    "    def __init__(self,sequence_strs, tokenizer, max_sequence_length):\n",
    "        self.sequence_strs = sequence_strs['sequences']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.tokenized_sequence = [self.tokenizer.encode(s) for s in self.sequence_strs]\n",
    "        self.tokenized_batches = self.get_tokenized_batches()\n",
    "\n",
    "    def get_batch_indices(self, extra_toks_per_seq=1):\n",
    "        print(self.sequence_strs)\n",
    "        sizes = [(len(s), i) for i, s in enumerate(self.sequence_strs)]\n",
    "        sizes.sort()\n",
    "        print(sizes)\n",
    "        batches = []\n",
    "        buf = []\n",
    "        current_buf_len = 0\n",
    "\n",
    "        def _flush_current_buf():\n",
    "            nonlocal current_buf_len, buf\n",
    "            if len(buf) == 0:\n",
    "                return\n",
    "            batches.append(buf)\n",
    "            buf = []\n",
    "            current_buf_len = 0\n",
    "            #print('my batches is:')\n",
    "            #print(batches)\n",
    "\n",
    "        for sz, i in sizes:\n",
    "            # considering the extra bos\n",
    "            sz += extra_toks_per_seq\n",
    "            # check accumulative seq length in the buffer\n",
    "            if current_buf_len + sz > self.max_sequence_length:\n",
    "                _flush_current_buf()\n",
    "            buf.append(i)\n",
    "            current_buf_len += sz\n",
    "            #print('my buffer is:')\n",
    "            #print(buf)\n",
    "\n",
    "        _flush_current_buf()\n",
    "        return batches\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_windows(sequence, max_sequence_length, num_intervals):\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        \"\"\"based on Phil's implement\n",
    "        Returns: a list of sampled windows.\n",
    "        \"\"\"\n",
    "        sampled_windows = []\n",
    "        # the beginning window\n",
    "        sampled_windows.append(sequence[:max_sequence_length-1])\n",
    "\n",
    "        if num_intervals > 2:\n",
    "            max_start_index = len(sequence) - max_sequence_length\n",
    "            if max_start_index < 0:\n",
    "                raise ValueError(\"max_sequence_length greater than length of sequence\")\n",
    "            for _ in range(num_intervals):\n",
    "                # Randomly select start index of the window\n",
    "                start_index = random.randint(0, max_start_index)\n",
    "                # Extract window\n",
    "                # Considering the added special token bos\n",
    "                window = sequence[start_index:start_index + max_sequence_length-1]\n",
    "                # Append the window to the list of sampled windows\n",
    "                sampled_windows.append(window)\n",
    "\n",
    "        # the end window\n",
    "        sampled_windows.append(sequence[-(max_sequence_length-1):])\n",
    "        return sampled_windows\n",
    "\n",
    "    def get_tokenized_batches(self):\n",
    "        batch_indices = self.get_batch_indices(self.max_sequence_length)\n",
    "        print(batch_indices)\n",
    "        tokenized_batches = []\n",
    "\n",
    "        for batch_index in batch_indices:\n",
    "            batch_sequences = [self.tokenized_sequence[i] for i in batch_index]\n",
    "            tokenized_batch = []\n",
    "            for token_list in batch_sequences:\n",
    "                if len(token_list) >= self.max_sequence_length-1:\n",
    "                    # considering the added special token bos\n",
    "                    sampled_windows = self.sample_windows(token_list, self.max_sequence_length, len(token_list)//self.max_sequence_length)\n",
    "                    for sample in sampled_windows:\n",
    "                        sample.insert(0, self.tokenizer.bos_id())\n",
    "                        tokenized_batch.append(sample)\n",
    "                else:\n",
    "                    token_list.insert(0, self.tokenizer.bos_id())\n",
    "                    tokenized_batch.append(token_list)\n",
    "            tokenized_batches.append(tokenized_batch)\n",
    "        return tokenized_batches\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_batches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_batches[idx]\n",
    "\n",
    "import sentencepiece as spm\n",
    "tokenizer_path = '/data/rozen/home/e0833634/lama/protllama/batch_script/'\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path+\"protein_8k.model\")\n",
    "dataset = FastaBatchedDataset(train_dataset, tokenizer, 60).get_tokenized_batches()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[[[1, 1, 854, 1642, 653, 91]],\n [[1, 1, 854, 1642, 653, 1279, 639, 86, 1195, 1214, 6817, 49, 141, 2076]],\n [[1,\n   1,\n   820,\n   814,\n   3992,\n   37,\n   1313,\n   7791,\n   245,\n   7689,\n   701,\n   4163,\n   6182,\n   1302,\n   4844,\n   7660,\n   1104,\n   3455,\n   2232,\n   4290,\n   2886,\n   2351,\n   2844,\n   3768,\n   4240,\n   2298,\n   3576,\n   1302,\n   828,\n   114,\n   2232,\n   43,\n   6101,\n   1894]],\n [[1,\n   1,\n   854,\n   1642,\n   653,\n   1279,\n   639,\n   86,\n   1195,\n   1214,\n   6817,\n   1950,\n   3101,\n   7696,\n   1783,\n   6728,\n   3777,\n   2132,\n   2949,\n   2596,\n   1868,\n   2403,\n   3399,\n   760,\n   1197,\n   614,\n   2013,\n   2199,\n   2572,\n   688,\n   2203,\n   1363,\n   890,\n   1369,\n   5521]]]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[tensor([1]), tensor([1]), tensor([854]), tensor([1642]), tensor([653]), tensor([91])]]\n",
      "1 [[tensor([1]), tensor([1]), tensor([854]), tensor([1642]), tensor([653]), tensor([1279]), tensor([639]), tensor([86]), tensor([1195]), tensor([1214]), tensor([6817]), tensor([49]), tensor([141]), tensor([2076])]]\n",
      "2 [[tensor([1]), tensor([1]), tensor([820]), tensor([814]), tensor([3992]), tensor([37]), tensor([1313]), tensor([7791]), tensor([245]), tensor([7689]), tensor([701]), tensor([4163]), tensor([6182]), tensor([1302]), tensor([4844]), tensor([7660]), tensor([1104]), tensor([3455]), tensor([2232]), tensor([4290]), tensor([2886]), tensor([2351]), tensor([2844]), tensor([3768]), tensor([4240]), tensor([2298]), tensor([3576]), tensor([1302]), tensor([828]), tensor([114]), tensor([2232]), tensor([43]), tensor([6101]), tensor([1894])]]\n",
      "3 [[tensor([1]), tensor([1]), tensor([854]), tensor([1642]), tensor([653]), tensor([1279]), tensor([639]), tensor([86]), tensor([1195]), tensor([1214]), tensor([6817]), tensor([1950]), tensor([3101]), tensor([7696]), tensor([1783]), tensor([6728]), tensor([3777]), tensor([2132]), tensor([2949]), tensor([2596]), tensor([1868]), tensor([2403]), tensor([3399]), tensor([760]), tensor([1197]), tensor([614]), tensor([2013]), tensor([2199]), tensor([2572]), tensor([688]), tensor([2203]), tensor([1363]), tensor([890]), tensor([1369]), tensor([5521])]]\n"
     ]
    }
   ],
   "source": [
    "for batch_id, batch in enumerate(dataloader):\n",
    "    print(batch_id, batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "import sentencepiece as spm\n",
    "tokenizer_path = '/data/rozen/home/e0833634/lama/protllama/batch_script/'\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path+\"protein_8k.model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0 corresponds to token: <unk>\n",
      "ID 1 corresponds to token: <s>\n",
      "ID 2 corresponds to token: </s>\n"
     ]
    }
   ],
   "source": [
    "# Check the tokens corresponding to specific ids\n",
    "ids_to_check = [0, 1, 2]\n",
    "\n",
    "for id in ids_to_check:\n",
    "    token = tokenizer.IdToPiece(id)\n",
    "    print(f\"ID {id} corresponds to token: {token}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "def sample_windows(sequence, max_sequence_length, num_intervals):\n",
    "    \"\"\"based on Phil's implement\n",
    "    Returns: a list of sampled windows.\n",
    "    \"\"\"\n",
    "    sampled_windows = []\n",
    "    # the beginning window\n",
    "    sampled_windows.append(sequence[:max_sequence_length-1])\n",
    "\n",
    "    if num_intervals > 2:\n",
    "        max_start_index = len(sequence) - max_sequence_length\n",
    "        if max_start_index < 0:\n",
    "            raise ValueError(\"max_sequence_length greater than length of sequence\")\n",
    "        for _ in range(num_intervals):\n",
    "            # Randomly select start index of the window\n",
    "            start_index = random.randint(0, max_start_index)\n",
    "            # Extract window\n",
    "            # Considering the added special token bos\n",
    "            window = sequence[start_index:start_index + max_sequence_length-1]\n",
    "            # Append the window to the list of sampled windows\n",
    "            sampled_windows.append(window)\n",
    "\n",
    "    # the end window\n",
    "    sampled_windows.append(sequence[-(max_sequence_length-1):])\n",
    "    return sampled_windows"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_id()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "import torch\n",
    "def tokenize_batch(batch, tokenizer, max_sequence_length):\n",
    "    proteins = [sample['sequences'] for sample in batch]\n",
    "    data_tokens = tokenizer.encode(proteins)\n",
    "    bos = tokenizer.bos_id()\n",
    "    data_tokens_sampled = []\n",
    "    for token_list in data_tokens:\n",
    "        length = len(token_list)\n",
    "        print(token_list)\n",
    "        if length >= max_sequence_length-1:\n",
    "            # considering the added special token bos\n",
    "            sampled_windows = sample_windows(token_list, max_sequence_length, length//max_sequence_length)\n",
    "            for sample in sampled_windows:\n",
    "                sample.insert(0, bos)\n",
    "                data_tokens_sampled.append(sample)\n",
    "        else:\n",
    "            token_list.insert(0, bos)\n",
    "            data_tokens_sampled.append(token_list)\n",
    "            #data_tokens_sampled.append(token_list)\n",
    "\n",
    "    return data_tokens_sampled\n",
    "\n",
    "#tokenized_dataset = train_dataset.map(tokenize_batch, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "dataloader = DataLoader(train_dataset,\n",
    "                     batch_size=1,\n",
    "                     shuffle=False,\n",
    "                     drop_last=False,\n",
    "                     collate_fn=partial(tokenize_batch, tokenizer=tokenizer, max_sequence_length = 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[820, 814, 3992, 37, 1313, 7791, 245, 7689, 701, 4163, 6182, 1302, 4844, 7660, 1104, 3455, 2232, 4290, 2886, 2351, 2844, 3768, 4240, 2298, 3576, 1302, 828, 114, 2232, 43, 6101, 1894]\n",
      "0\n",
      "[1, 820, 814, 3992, 37, 1313, 7791, 245, 7689, 701]\n",
      "[1, 2844, 3768, 4240, 2298, 3576, 1302, 828, 114, 2232]\n",
      "[1, 37, 1313, 7791, 245, 7689, 701, 4163, 6182, 1302]\n",
      "[1, 820, 814, 3992, 37, 1313, 7791, 245, 7689, 701]\n",
      "[1, 2298, 3576, 1302, 828, 114, 2232, 43, 6101, 1894]\n",
      "[854, 1642, 653, 1279, 639, 86, 1195, 1214, 6817, 1950, 3101, 7696, 1783, 6728, 3777, 2132, 2949, 2596, 1868, 2403, 3399, 760, 1197, 614, 2013, 2199, 2572, 688, 2203, 1363, 890, 1369, 5521]\n",
      "1\n",
      "[1, 854, 1642, 653, 1279, 639, 86, 1195, 1214, 6817]\n",
      "[1, 614, 2013, 2199, 2572, 688, 2203, 1363, 890, 1369]\n",
      "[1, 6817, 1950, 3101, 7696, 1783, 6728, 3777, 2132, 2949]\n",
      "[1, 1214, 6817, 1950, 3101, 7696, 1783, 6728, 3777, 2132]\n",
      "[1, 2013, 2199, 2572, 688, 2203, 1363, 890, 1369, 5521]\n",
      "[854, 1642, 653, 1279, 639, 86, 1195, 1214, 6817, 49, 141, 2076]\n",
      "2\n",
      "[1, 854, 1642, 653, 1279, 639, 86, 1195, 1214, 6817]\n",
      "[1, 1279, 639, 86, 1195, 1214, 6817, 49, 141, 2076]\n",
      "[854, 1642, 653, 91]\n",
      "3\n",
      "[1, 854, 1642, 653, 91]\n"
     ]
    }
   ],
   "source": [
    "for batch_id, batch in enumerate(dataloader):\n",
    "    print(batch_id)\n",
    "    for v in batch:\n",
    "        print(v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sequences'],\n    num_rows: 4\n})"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['GLTNAFIASAPAREVRYDGVITPANANYRFMGGDKGGSLTVGSHLTGSNMVTIGPMGVVVFTNNNDYTGNTFIMGGGTLQLGSNTAWGSLPN\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFPLSPAQLGIWYAQHLDPQVPITIAQYVDLHGALDVEVLERASIDASHELGSGFLRIVERDGEPLQYV\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFP\\n',\n",
    "        'MDRLDFGGGE\\n']\n",
    "from datasets import Dataset\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "\n",
    "train_dataset = Dataset.from_dict({'sequences': text})\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "class FastaBatchedDataset(object):\n",
    "    \"\"\"inspired by esm2, but instead of sorting the original sequences,\n",
    "    we should really sorting based on tokenized sequences\n",
    "    \"\"\"\n",
    "    def __init__(self,sequence_strs, tokenizer, max_sequence_length):\n",
    "        self.sequence_strs = sequence_strs['sequences']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        raw_tokenized_sequences = [self.tokenizer.encode(s) for s in self.sequence_strs]\n",
    "        self.tokenized_sequences = []\n",
    "        for tokens in raw_tokenized_sequences:\n",
    "            if len(tokens) >= self.max_sequence_length-1:\n",
    "                # considering the added special token bos\n",
    "                sampled_windows = self.sample_windows(tokens, self.max_sequence_length)\n",
    "                for sample in sampled_windows:\n",
    "                    sample.insert(0, self.tokenizer.bos_id())\n",
    "                    self.tokenized_sequences.append(sample)\n",
    "            else:\n",
    "                tokens.insert(0, self.tokenizer.bos_id())\n",
    "                self.tokenized_sequences.append(tokens)\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_windows(sequence, max_sequence_length, extra_toks_per_seq=1):\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        \"\"\"based on Phil's implement\n",
    "        Returns: a list of sampled windows.\n",
    "        \"\"\"\n",
    "        sampled_windows = []\n",
    "        # the beginning window\n",
    "        sampled_windows.append(sequence[:max_sequence_length-extra_toks_per_seq])\n",
    "        # calculate the num of random slices needed, remove head and tail\n",
    "        num_slices_required = (len(sequence) // max_sequence_length) - 2\n",
    "        max_start_index = len(sequence) - max_sequence_length\n",
    "\n",
    "        if max_start_index < 0:\n",
    "            raise ValueError(\"max_sequence_length greater than length of sequence\")\n",
    "\n",
    "        if num_slices_required > 0:\n",
    "            for _ in range(num_slices_required):\n",
    "                # Randomly select start index of the window\n",
    "                start_index = random.randint(0, max_start_index)\n",
    "                # Extract window\n",
    "                # Considering the added special token bos\n",
    "                window = sequence[start_index:start_index + max_sequence_length-extra_toks_per_seq]\n",
    "                # Append the window to the list of sampled windows\n",
    "                sampled_windows.append(window)\n",
    "\n",
    "        # the end window\n",
    "        sampled_windows.append(sequence[-(max_sequence_length-extra_toks_per_seq):])\n",
    "        return sampled_windows\n",
    "\n",
    "    def get_batch_indices(self, extra_toks_per_seq=1):\n",
    "        print(self.sequence_strs)\n",
    "        sizes = [(len(tokens), i) for i, tokens in enumerate(self.tokenized_sequences)]\n",
    "        sizes.sort()\n",
    "        print(sizes)\n",
    "        batches = []\n",
    "        buf = []\n",
    "        current_buf_len = 0\n",
    "\n",
    "        def _flush_current_buf():\n",
    "            nonlocal current_buf_len, buf\n",
    "            if len(buf) == 0:\n",
    "                return\n",
    "            batches.append(buf)\n",
    "            buf = []\n",
    "            current_buf_len = 0\n",
    "            #print('my batches is:')\n",
    "            #print(batches)\n",
    "\n",
    "        for sz, i in sizes:\n",
    "            # considering the extra bos\n",
    "            sz += extra_toks_per_seq\n",
    "            # check accumulative seq length in the buffer\n",
    "            if current_buf_len + sz > self.max_sequence_length:\n",
    "                _flush_current_buf()\n",
    "            buf.append(i)\n",
    "            current_buf_len += sz\n",
    "            #print('my buffer is:')\n",
    "            #print(buf)\n",
    "\n",
    "        _flush_current_buf()\n",
    "        return batches\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_sequences[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "class BatchConverter(object):\n",
    "    \"\"\"add padding, create labels for GPT-alike training, used as collate_fn, need processed batch indices\n",
    "    processed (labels + tensor) batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        self.pad_token_id = tokenizer.pad_id()\n",
    "\n",
    "    def __call__(self, batches):\n",
    "        data_tokens = [torch.tensor(token_list) for token_list in batches]\n",
    "        data_tokens_padded = pad_sequence(data_tokens, batch_first=True, padding_value=self.pad_token_id)\n",
    "\n",
    "        # Create attention masks\n",
    "        attention_masks = (data_tokens_padded != self.pad_token_id).long()\n",
    "\n",
    "        # skip label==-100 during training so that these tokens won't be used in loss calculation\n",
    "        labels = data_tokens_padded.clone()\n",
    "        labels[data_tokens_padded == self.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': data_tokens_padded,\n",
    "            'attention_mask': attention_masks,\n",
    "            'labels': labels\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "tokenizer_path = '/data/rozen/home/e0833634/lama/protllama/batch_script/'\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path+\"protein_8k.model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GLTNAFIASAPAREVRYDGVITPANANYRFMGGDKGGSLTVGSHLTGSNMVTIGPMGVVVFTNNNDYTGNTFIMGGGTLQLGSNTAWGSLPN\\n', 'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFPLSPAQLGIWYAQHLDPQVPITIAQYVDLHGALDVEVLERASIDASHELGSGFLRIVERDGEPLQYV\\n', 'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFP\\n', 'MDRLDFGGGE\\n']\n",
      "[(5, 8), (10, 0), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 6), (10, 7)]\n"
     ]
    }
   ],
   "source": [
    "dataset = FastaBatchedDataset(train_dataset, tokenizer, 10)\n",
    "batches = dataset.get_batch_indices()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, collate_fn=BatchConverter(tokenizer),\n",
    "                                          batch_sampler=batches, pin_memory=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "[[8], [0], [1], [2], [3], [4], [5], [6], [7]]"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'input_ids': tensor([[   1,  854, 1642,  653,   91]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'labels': tensor([[   1,  854, 1642,  653,   91]])}\n",
      "1 {'input_ids': tensor([[   1,  820,  814, 3992,   37, 1313, 7791,  245, 7689,  701]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1,  820,  814, 3992,   37, 1313, 7791,  245, 7689,  701]])}\n",
      "2 {'input_ids': tensor([[   1, 2844, 3768, 4240, 2298, 3576, 1302,  828,  114, 2232]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1, 2844, 3768, 4240, 2298, 3576, 1302,  828,  114, 2232]])}\n",
      "3 {'input_ids': tensor([[   1, 2298, 3576, 1302,  828,  114, 2232,   43, 6101, 1894]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1, 2298, 3576, 1302,  828,  114, 2232,   43, 6101, 1894]])}\n",
      "4 {'input_ids': tensor([[   1,  854, 1642,  653, 1279,  639,   86, 1195, 1214, 6817]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1,  854, 1642,  653, 1279,  639,   86, 1195, 1214, 6817]])}\n",
      "5 {'input_ids': tensor([[   1, 3399,  760, 1197,  614, 2013, 2199, 2572,  688, 2203]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1, 3399,  760, 1197,  614, 2013, 2199, 2572,  688, 2203]])}\n",
      "6 {'input_ids': tensor([[   1, 2013, 2199, 2572,  688, 2203, 1363,  890, 1369, 5521]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1, 2013, 2199, 2572,  688, 2203, 1363,  890, 1369, 5521]])}\n",
      "7 {'input_ids': tensor([[   1,  854, 1642,  653, 1279,  639,   86, 1195, 1214, 6817]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1,  854, 1642,  653, 1279,  639,   86, 1195, 1214, 6817]])}\n",
      "8 {'input_ids': tensor([[   1, 1279,  639,   86, 1195, 1214, 6817,   49,  141, 2076]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1, 1279,  639,   86, 1195, 1214, 6817,   49,  141, 2076]])}\n"
     ]
    }
   ],
   "source": [
    "for batch_id, batch in enumerate(dataloader):\n",
    "    print(batch_id, batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[820, 814, 3992, 37, 1313, 7791, 245, 7689, 701, 4163, 6182, 1302, 4844, 7660, 1104, 3455, 2232, 4290, 2886, 2351, 2844, 3768, 4240, 2298, 3576, 1302, 828, 114, 2232, 43, 6101, 1894] 32\n",
      "[854, 1642, 653, 1279, 639, 86, 1195, 1214, 6817, 1950, 3101, 7696, 1783, 6728, 3777, 2132, 2949, 2596, 1868, 2403, 3399, 760, 1197, 614, 2013, 2199, 2572, 688, 2203, 1363, 890, 1369, 5521] 33\n",
      "0\n",
      "[1, 820, 814, 3992, 37, 1313, 7791, 245, 7689, 701]\n",
      "[1, 2844, 3768, 4240, 2298, 3576, 1302, 828, 114, 2232]\n",
      "[1, 2298, 3576, 1302, 828, 114, 2232, 43, 6101, 1894]\n",
      "[1, 854, 1642, 653, 1279, 639, 86, 1195, 1214, 6817]\n",
      "[1, 3399, 760, 1197, 614, 2013, 2199, 2572, 688, 2203]\n",
      "[1, 2013, 2199, 2572, 688, 2203, 1363, 890, 1369, 5521]\n",
      "[854, 1642, 653, 1279, 639, 86, 1195, 1214, 6817, 49, 141, 2076] 12\n",
      "[854, 1642, 653, 91] 4\n",
      "1\n",
      "[1, 854, 1642, 653, 1279, 639, 86, 1195, 1214, 6817]\n",
      "[1, 1279, 639, 86, 1195, 1214, 6817, 49, 141, 2076]\n",
      "[1, 854, 1642, 653, 91]\n"
     ]
    }
   ],
   "source": [
    "# verify via simple functions\n",
    "def sample_windows(sequence, max_sequence_length):\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    \"\"\"based on Phil's implement\n",
    "    Returns: a list of sampled windows.\n",
    "    \"\"\"\n",
    "    sampled_windows = []\n",
    "    # the beginning window\n",
    "    sampled_windows.append(sequence[:max_sequence_length - 1])\n",
    "     # Number of random slices needed\n",
    "    num_slices_required = (len(sequence) // max_sequence_length) - 2\n",
    "    max_start_index = len(sequence) - max_sequence_length\n",
    "\n",
    "    if max_start_index < 0:\n",
    "        raise ValueError(\"max_sequence_length greater than length of sequence\")\n",
    "\n",
    "    if num_slices_required > 0:\n",
    "        for _ in range(num_slices_required):\n",
    "            # Randomly select start index of the window\n",
    "            start_index = random.randint(0, max_start_index)\n",
    "            # Extract window\n",
    "            # Considering the added special token bos\n",
    "            window = sequence[start_index:start_index + max_sequence_length - 1]\n",
    "            # Append the window to the list of sampled windows\n",
    "            sampled_windows.append(window)\n",
    "\n",
    "    # the end window\n",
    "    sampled_windows.append(sequence[-(max_sequence_length - 1):])\n",
    "    return sampled_windows\n",
    "\n",
    "import torch\n",
    "\n",
    "def tokenize_batch(batch, tokenizer, max_sequence_length):\n",
    "    proteins = [sample['sequences'] for sample in batch]\n",
    "    data_tokens = tokenizer.encode(proteins)\n",
    "    bos = tokenizer.bos_id()\n",
    "    data_tokens_sampled = []\n",
    "    for token_list in data_tokens:\n",
    "        length = len(token_list)\n",
    "        print(token_list, len(token_list))\n",
    "        if length >= max_sequence_length - 1:\n",
    "            # considering the added special token bos\n",
    "            sampled_windows = sample_windows(token_list, max_sequence_length)\n",
    "            for sample in sampled_windows:\n",
    "                sample.insert(0, bos)\n",
    "                data_tokens_sampled.append(sample)\n",
    "        else:\n",
    "            token_list.insert(0, bos)\n",
    "            data_tokens_sampled.append(token_list)\n",
    "            #data_tokens_sampled.append(token_list)\n",
    "\n",
    "    return data_tokens_sampled\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "dataloader = DataLoader(train_dataset,\n",
    "                        batch_size=2,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False,\n",
    "                        collate_fn=partial(tokenize_batch, tokenizer=tokenizer, max_sequence_length=10))\n",
    "for batch_id, batch in enumerate(dataloader):\n",
    "    print(batch_id)\n",
    "    for v in batch:\n",
    "        print(v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open('/data/rozen/home/e0833634/lama/protllama/original_lama.pkl', 'rb') as f:\n",
    "    loaded_data_lama = pickle.load(f)\n",
    "#name = 'togethercomputer/RedPajama-Data-1T-Sample'\n",
    "#test_sample = loaded_data_lama['train'][:2]\n",
    "#test_sample_dict = DatasetDict({\"train\": test_sample})['train']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': [\"\\\\section{Introduction}\\n\\\\label{sec:intro}\\n\\n\\\\emph{Gender diversity}, or more often its lack thereof, among participants to\\nsoftware development activities has been thoroughly studied in recent years. In\\nparticular, the presence of, effects of, and countermeasures for \\\\emph{gender\\n  bias} in Free/Open Source Software (FOSS) have received a lot of attention\\nover the past decade~\\\\cite{david2008fossdevs, qiu2010kdewomen,\\n  nafus2012patches, kuechler2012genderfoss, vasilescu2014gender,\\n  oneil2016debiansurvey, robles2016womeninfoss, terrell2017gender,\\n  zacchiroli2021gender}.  \\\\emph{Geographic diversity} is on the other hand the\\nkind of diversity that stems from participants in some global activity coming\\nfrom different world regions and cultures.\\n\\nGeographic diversity in FOSS has received relatively little attention in scholarly\\nworks. In particular, while seminal survey-based and\\npoint-in-time medium-scale studies of the geographic origins of FOSS\\ncontributors exist~\\\\cite{ghosh2005understanding, david2008fossdevs,\\n  barahona2008geodiversity, takhteyev2010ossgeography, robles2014surveydataset,\\n  wachs2021ossgeography}, large-scale longitudinal studies of the geographic\\norigin of FOSS contributors are still lacking. Such a quantitative\\ncharacterization would be useful to inform decisions related to global\\ndevelopment teams~\\\\cite{herbsleb2007globalsweng} and hiring strategies in the\\ninformation technology (IT) market, as well as contribute factual information\\nto the debates on the economic impact and sociology of FOSS around the world.\\n\\n\\n\\\\paragraph{Contributions}\\n\\nWith this work we contribute to close this gap by conducting \\\\textbf{the first\\n  longitudinal study of the geographic origin of contributors to public code\\n  over 50 years.} Specifically, we provide a preliminary answer to the\\nfollowing research question:\\n\\\\begin{researchquestion}\\n  From which world regions do authors of publicly available commits come from\\n  and how has it changed over the past 50 years?\\n  \\\\label{rq:geodiversity}\\n\\\\end{researchquestion}\\nWe use as dataset the \\\\SWH/ archive~\\\\cite{swhipres2017} and analyze from it\\n2.2 billion\\\\xspace commits archived from 160 million\\\\xspace projects and authored by\\n43 million\\\\xspace authors during the 1971--2021 time period. \\nWe geolocate developers to\\n\\\\DATAWorldRegions/ world regions, using as signals email country code top-level domains (ccTLDs) and \\nauthor (first/last) names compared with name distributions around the world, and UTC offsets \\nmined from commit metadata.\\n\\nWe find evidence of the early dominance of North America in open source\\nsoftware, later joined by Europe. After that period, the geographic diversity \\nin public code has been constantly increasing.\\nWe also identify relevant historical shifts\\nrelated to the end of the UNIX wars and the increase of coding literacy in\\nCentral and South Asia, as well as of broader phenomena like colonialism and\\npeople movement across countries (immigration/emigration).\\n\\n\\n\\n\\n\\\\paragraph{Data availability.}\\n\\nA replication package for this paper is available from Zenodo at\\n\\\\url{https://doi.org/10.5281/zenodo.6390355}~\\\\cite{replication-package}.\\n\\n\\n \\\\section{Related Work}\\n\\\\label{sec:related}\\n\\nBoth early and recent works~\\\\cite{ghosh2005understanding, david2008fossdevs,\\n  robles2014surveydataset, oneil2016debiansurvey} have characterized the\\ngeography of Free/Open Source Software (FOSS) using \\\\emph{developer surveys},\\nwhich provide high-quality answers but are limited in size (2-5\\\\,K developers)\\nand can be biased by participant sampling.\\n\\nIn 2008 Barahona et al.~\\\\cite{barahona2008geodiversity} conducted a seminal\\nlarge-scale (for the time) study on FOSS \\\\emph{geography using mining software\\n  repositories (MSR) techniques}. They analyzed the origin of 1\\\\,M contributors\\nusing the SourceForge user database and mailing list archives over the\\n1999--2005 period, using as signals information similar to ours: email domains\\nand UTC offsets. \\nThe studied period (7 years) in~\\\\cite{barahona2008geodiversity} is shorter than \\nwhat is studied in the present paper (50 years) and the data sources are \\nlargely different; with that in mind, our results show a slightly larger quote of \\nEuropean v.~North American contributions.\\n\\nAnother empirical work from 2010 by Takhteyev and\\nHilts~\\\\cite{takhteyev2010ossgeography} harvested self-declared geographic\\nlocations of GitHub accounts recursively following their connections,\\ncollecting information for $\\\\approx$\\\\,70\\\\,K GitHub users.  A very recent\\nwork~\\\\cite{wachs2021ossgeography} by Wachs et al.~has geolocated half a million\\nGitHub users, having contributed at least 100 commits each, and who\\nself-declare locations on their GitHub profiles. While the study is\\npoint-in-time as of 2021, the authors compare their findings\\nagainst~\\\\cite{barahona2008geodiversity, takhteyev2010ossgeography} to\\ncharacterize the evolution of FOSS geography over the time snapshots taken by\\nthe three studies.\\n\\nCompared with previous empirical works, our study is much larger scale---having\\nanalyzed 43 million\\\\xspace authors of 2.2 billion\\\\xspace commits from 160 million\\\\xspace\\nprojects---longitudinal over 50 years of public code contributions rather than\\npoint in time, and also more fine-grained (with year-by-year granularity over\\nthe observed period). Methodologically, our study relies on Version Control\\nSystem (VCS) commit data rather than platform-declared location information.\\n\\n\\nOther works---in particular the work by Daniel~\\\\cite{daniel2013ossdiversity}\\nand, more recently, Rastogi et al.~\\\\cite{rastogi2016geobias,\\n  rastogi2018geobias, prana2021geogenderdiversity}---have studied geographic\\n\\\\emph{diversity and bias}, i.e., the extent to which the origin of FOSS\\ndevelopers affect their collaborative coding activities.\\nIn this work we characterized geographic diversity in public code for the first\\ntime at this scale, both in terms of contributors and observation period. We do\\nnot tackle the bias angle, but provide empirical data and findings that can be\\nleveraged to that end as future work.\\n\\n\\\\emph{Global software engineering}~\\\\cite{herbsleb2007globalsweng} is the\\nsub-field of software engineering that has analyzed the challenges of scaling\\ndeveloper collaboration globally, including the specific concern of how to deal\\nwith geographic diversity~\\\\cite{holmstrom2006globaldev, fraser2014eastwest}.\\nDecades later the present study provides evidence that can be used, in the\\nspecific case of public code and at a very large scale, to verify which\\npromises of global software engineering have borne fruit.\\n\\n\\n\\n\\n\\n\\n \\\\section{Methodology}\\n\\\\label{sec:method}\\n\\n\\n\\\\newif\\\\ifgrowthfig  \\\\growthfigtrue\\n\\\\ifgrowthfig\\n\\\\begin{figure}\\n  \\\\includegraphics[width=\\\\columnwidth]{yearly-commits}\\n  \\\\caption{Yearly public commits over time (log scale).\\n}\\n  \\\\label{fig:growth}\\n\\\\end{figure}\\n\\\\fi\\n\\n\\\\paragraph{Dataset}\\n\\nWe retrieved from \\\\SWH/~\\\\cite{swh-msr2019-dataset} all commits archived until \\\\DATALastCommitDate/.\\nThey amount to \\\\DATACommitsRaw/ commits, unique by SHA1 identifier, harvested from \\\\DATATotalCommitsInSH/ public projects coming from major development forges (GitHub, GitLab, etc.) and package repositories (Debian, PyPI, NPM, etc.).\\nCommits in the dataset are by \\\\DATAAuthorsRaw/ authors, unique by $\\\\langle$name, email$\\\\rangle$ pairs.\\nThe dataset came as two relational tables, one for commits and one for authors, with the former referencing the latter via a foreign key.\\n\\\\iflong\\nEach row in the commit table contains the following fields: commit SHA1 identifier, author and committer timestamps, author and committer identifiers (referencing the author table).\\nThe distinction between commit authors and committers come from Git, which allows to commit a change authored by someone else.\\nFor this study we focused on authors and ignored committers, as the difference between the two is not relevant for our research questions and the amount of commits with a committer other than its author is negligible.\\n\\\\fi\\nFor each entry in the author table we have author full name and email as two separate strings of raw bytes.\\n\\nWe removed implausible or unusable names that: are not decodable as UTF-8 (\\\\DATAAuthorsRmNondecodable/ author names removed), are email addresses instead of names (\\\\DATAAuthorsRmEmail/ ``names''), consist of only blank characters (\\\\DATAAuthorsRmBlank/), contain more than 10\\\\% non-letters (\\\\DATAAuthorsRmNonletter/), are longer than 100 characters (\\\\DATAAuthorsRmToolong/).\\nAfter filtering, about \\\\DATAAuthorsPlausibleApprox/ authors (\\\\DATAAuthorsPlausiblePct/ of the initial dataset) remained for further analysis.\\n\\nNote that the amount of public code commits (and authors) contained in the\\ninitial dataset grows exponentially over\\ntime~\\\\cite{swh-provenance-emse}\\\\ifgrowthfig, as shown for commits in\\n\\\\Cref{fig:growth}\\\\else: from $10^4$ commits in 1971, to $10^6$ in 1998, to\\nalmost $10^9$ in 2020\\\\fi. As a consequence the observed trends tend to be more\\nstable in recent decades than in 40+ year-old ones, due to statistics taken on\\nexponentially larger populations.\\n\\n\\n\\\\paragraph{Geolocation}\\n\\n\\\\begin{figure}\\n  \\\\centering\\n  \\\\includegraphics[clip,trim=6cm 6cm 0 0,width=\\\\linewidth]{subregions-ours}\\n  \\\\caption{The \\\\DATAWorldRegions/ world regions used as geolocation targets.}\\n  \\\\label{fig:worldmap}\\n\\\\end{figure}\\n\\nAs geolocation targets we use macro world regions derived from the United Nations geoscheme~\\\\cite{un1999geoscheme}.\\nTo avoid domination by large countries (e.g., China or Russia) within macro regions, we merged and split some regions based on geographic proximity and the sharing of preeminent cultural identification features, such as spoken language.\\n\\\\Cref{fig:worldmap} shows the final list of \\\\DATAWorldRegions/ world regions used as geolocation targets in this study.\\n\\nGeolocation of commit authors to world regions uses the two complementary techniques introduced in~\\\\cite{icse-seis-2022-gender}, briefly recalled below.\\nThe first one relies on the country code top-level domain (ccTLD) of email addresses extracted from commit metadata, e.g., \\\\texttt{.fr}, \\\\texttt{.ru}, \\\\texttt{.cn}, etc.\\nWe started from the IANA list of Latin character ccTLDs~\\\\cite{wikipedia-cctld} and manually mapped each corresponding territory to a target world region.\\n\\nThe second geolocation technique uses the UTC offset of commit timestamps (e.g., UTC-05:00) and author names to determine the most likely world region of the commit author.\\nFor each UTC offset we determine a list of compatible places (country, state, or dependent territory) in the world that, at the time of that commit, had that UTC offset; commit time is key here, as country UTC offsets vary over time due to timezone changes.\\nTo make this determination we use the IANA time zone database~\\\\cite{tzdata}.\\n\\nThen we assign to each place a score that captures the likelihood that a given author name is characteristic of it.\\nTo this end we use the Forebears dataset of the frequencies of the most common first and family names which, quoting from~\\\\cite{forebear-names}: {\\\\itshape ``provides the approximate incidence of forenames and surnames produced from a database of \\\\num{4 044 546 938} people (55.5\\\\% of living people in 2014). As of September 2019 it covers \\\\num{27 662 801} forenames and \\\\num{27 206 821} surnames in 236 jurisdictions.''}\\nAs in our dataset authors are full name strings (rather than split by first/family name), we first tokenize names (by blanks and case changes) and then lookup individual tokens in both first and family names frequency lists.\\nFor each element found in name lists we multiply the place population\\\\footnotemark{} by the name frequency to obtain a measure that is proportional to the number of persons bearing that name (token) in the specific place.\\n\\\\footnotetext{To obtain population totals---as the notion of ``place'' is heterogeneous: full countries v.~slices of large countries spanning multiple timezones---we use a mixture of primary sources (e.g., government websites), and non-primary ones (e.g., Wikipedia articles).}\\nWe sum this figure for all elements to obtain a place score, ending up with a list of $\\\\langle$place, score$\\\\rangle$ pairs.\\nWe then partition this list by the world region that a place belongs to and sum the score for all the places in each region to obtain an overall score, corresponding to the likelihood that the commit belongs to a given world region.\\nWe assign the starting commit as coming from the world region with the highest score.\\n\\nThe email-based technique suffers from the limited and unbalanced use of ccTLDs: most developers use generic TLDs such as \\\\texttt{.com}, \\\\texttt{.org}, or \\\\texttt{.net}.\\nMoreover this does not happen uniformly across zones: US-based developers, for example, use the \\\\texttt{.us} ccTLD much more seldomly than their European counterparts.\\nOn the other hand the offset/name-based technique relies on the UTC offset of the commit timestamps.\\nDue to tool configurations on developer setups, a large number of commits in the dataset has an UTC offset equal to zero.\\nThis affects less recent commits (\\\\DATACommitsTZZTwoThousandTwenty/ of 2020s commits have a zero offset) than older ones (\\\\DATACommitsTZZTwoThousand/ in 2000).\\nAs a result the offset/name-based technique could end up detecting a large share of older commits as authored by African developers, and to a lesser extent Europeans.\\n\\nTo counter these issues we combine the two geolocation techniques together by applying the offset/name-based techniques to all commits with a non-zero UTC offset, and the email-based on to all other commits.\\n\\n\\n \\\\section{Results and Discussion}\\n\\\\label{sec:results}\\n\\n\\\\begin{figure*}\\n  \\\\centering\\n  \\\\includegraphics[width=\\\\linewidth]{stacked.pdf}\\n  \\\\caption{Ratio of commits (above) and active authors (below) by world zone over the 1971--2020 period.}\\n  \\\\Description[Chart]{Stacked bar chart showing the world zone ratios for commits and authors over the 1971--2020 period.}\\n  \\\\label{fig:results}\\n\\\\end{figure*}\\n\\n\\n \\nTo answer \\\\cref{rq:geodiversity} we gathered the number of commits and distinct authors per year and per world zone.\\nWe present the obtained results in \\\\Cref{fig:results} as two stacked bar charts, showing yearly breakdowns for commits and authors respectively.\\nEvery bar represents a year and is partitioned in slices showing the commit/author ratio for each of the world regions of \\\\Cref{fig:worldmap} in that year.\\nTo avoid outliers due to sporadic contributors, in the author chart we only consider authors having contributed at least 5 commits in a given year.\\n\\nWhile observing trends in the charts remember that the total numbers of commits and authors grow exponentially over time.\\nHence for the first years in the charts, the number of data points in some world regions can be extremely small, with negative consequences on the stability of trends.\\n\\n\\n\\n\\n\\\\paragraph{Geographic diversity over time}\\n\\nOverall, the general trend appears to be that the \\\\textbf{geographic diversity in public code is increasing}: North America and Europe alternated their ``dominance'' until the middle of the 90s; from that moment on most other world regions show a slow but steady increment.\\nThis trend of increased participation into public code development includes Central and South Asia (comprising India), Russia, Africa, Central and South America,\\nNotice that also zones that do not seem to follow this trend, such as Australia and New Zealand, are also increasing their participation, but at a lower speed with respect to other zones.\\nFor example, Australia and New Zealand incremented the absolute number of their commits by about 3 orders of magnitude from 2000 to present days.\\n\\nAnother interesting phenomenon that can be appreciated in both charts is the sudden contraction of contributions from North America in 1995; since the charts depict ratios, this corresponds to other zones, and Europe in particular, increasing their share.\\nAn analysis of the main contributions in the years right before the contraction shows that nine out of ten have \\\\texttt{ucbvax.Berkeley.EDU} as author email domain, and the tenth is Keith Bostic, one of the leading Unix BSD developers, appearing with email \\\\texttt{bostic}.\\nNo developer with the same email domain appears anymore within the first hundred contributors in 1996.\\nThis shows the relevance that BSD Unix and the Computer Systems Research Group at the University of California at Berkeley had in the history of open source software.\\nThe group was disbanded in 1995, partially as a consequence of the so-called UNIX wars~\\\\cite{kernighan2019unixhistory}, and this contributes significantly---also because of the relatively low amount of public code circulating at the time---to the sudden drop of contributions from North America in subsequent years.\\nDescendant UNIX operating systems based on BSD, such as OpenBSD, FreeBSD, and NetBSD had smaller relevance to world trends due to (i) the increasing amount of open source code coming from elsewhere and (ii) their more geographically diverse developer community.\\n\\nAnother time frame in which the ratios for Europe and North America are subject to large, sudden changes is 1975--79.\\nA preliminary analysis shows that these ratios are erratic due to the very limited number of commits in those time period, but we were unable to detect a specific root cause.\\nTrends for those years should be subject to further studies, in collaboration with software historians.\\n\\n\\n\\\\paragraph{Colonialism}\\n\\nAnother trend that stands out from the charts is that Africa appears to be well represented.\\nTo assess if this results from a methodological bias, we double-checked the commits detected as originating from Africa for timezones included in the $[0, 3]$ range using both the email- the offset/name-based methods.\\nThe results show that the offset/name-based approach assigns 22.7\\\\% of the commits to Africa whereas the email-based one only assigns 2.7\\\\% of them.\\nWhile a deeper investigation is in order, it is our opinion that the phenomenon we are witnessing here is a consequence of colonialism, specifically the adoption of Europeans names in African countries.\\nFor example the name Eric, derived from Old Norse, is more popular in Ghana than it is in France or in the UK.\\nThis challenges the ability of the offset/name-based method to correctly differentiate between candidate places.\\nTogether with the fact that several African countries are largely populated, the offset/name-based method could detect European names as originating from Africa.\\nWhile this cuts both way, the likelihood of a random person contributing to public code is very different between European countries, all having a well-developed software industry, and African countries that do not all share this trait.\\n\\n\\n\\\\paragraph{Immigration/emigration}\\n\\nAnother area where a similar phenomenon could be at play is the evolution of Central and South America.\\nContribution from this macro region appears to be growing steadily.\\nTo assess if this is the result of a bias introduced by the name-based detection we analyzed the evolution of offset/name-based assignment over time for authors whose email domain is among the top-ten US-based entities in terms of overall contributions (estimated in turn by analyzing the most frequent email domains and manually selecting those belonging to US-based entities).\\nIn 1971 no author with an email from top US-based entities is detected as belonging to Central and South America, whereas in 2019 the ratio is 12\\\\%.\\nNowadays more than one tenth of the people email-associated to top US-based entities have popular Central and South American names, which we posit as a likely consequence of immigration into US (emigration from Central and South America).\\nSince immigration has a much longer history than what we are studying here, what we are witnessing probably includes long-term consequences of it, such as second and third generation immigrants employed in white-collar jobs, such as software development.\\n\\n\\n\\n\\n \\\\section{Limitations and Future Work}\\n\\\\label{sec:conclusion}\\n\\nWe have performed an exploratory, yet very large scale, empirical study of the geographic diversity in public code commits over time.\\nWe have analyzed 2.2 billion\\\\xspace public commits covering the \\\\DATAYearRange/ time period.\\nWe have geolocated developers to \\\\DATAWorldRegions/ world regions using as signals email domains, timezone offsets, and author names.\\nOur findings show that the geographic diversity in public code is increasing over time, and markedly so over the past 20--25 years.\\nObserved trends also co-occur with historical events and macro phenomena like the end of the UNIX wars, increase of coding literacy around the world, colonialism, and immigration.\\n\\n\\n\\\\medskip\\n\\\\emph{Limitations.}\\nThis study relies on a combination of two geolocation methods: one based on email domains, another based on commit UTC offsets and author names.\\nWe discussed some of the limitations of either method in \\\\Cref{sec:method}, motivating our decision of restricting the use of the email-based method to commits with a zero UTC offset.\\nAs a consequence, for most commits in the dataset the offset/name-based method is used.\\nWith such method, the frequencies of forenames and surnames are used to rank candidate zones that have a compatible UTC offset at commit time.\\n\\nA practical consequence of this is that for commits with, say, offset UTC+09:00 the candidate places can be Russia, Japan and Australia, depending on the specific date due to daylight saving time.\\nPopular forenames and surnames in these regions tend to be quite different so the likelihood of the method to provide a reliable detection is high.\\nFor other offsets the set of popular forenames and surnames from candidate zones can exhibit more substantial overlaps, negatively impacting detection accuracy.\\nWe have discussed some of these cases in \\\\Cref{sec:results}, but other might be lingering in the results impacting observed trends.\\n\\nThe choice of using the email-based method for commits with zero UTC offset, and the offset/name-based method elsewhere, has allowed us to study all developers not having a country-specific email domain (ccTLD), but comes with the risk of under-representing the world zones that have (in part and in some times of the year) an actual UTC offset of zero.\\n\\nA potential bias in this study could be introduced by the fact that the name database used for offset/name-based geolocation only contains names formed using Latin alphabet characters.\\nWe looked for names containing Chinese, Japanese, and Korean characters in the original dataset, finding only a negligible amount of authors who use non-Latin characters in their VCS names, which leads us to believe that the impact of this issue is minimal.\\n\\nWe did not apply identity merging (e.g., using state-of-the-art tools like SortingHat~\\\\cite{moreno2019sortinghat}), but we do not expect this to be a significant issue because: (a) to introduce bias in author trends the distribution of identity merges around the world should be uneven, which seems unlikely; and (b) the observed commit trends (which would be unaffected by identity merging) are very similar to observed author trends.\\n\\nWe did not systematically remove known bot accounts~\\\\cite{lebeuf2018swbots} from the author dataset, but we did check for the presence of software bots among the top committers of each year. We only found limited traces of continuous integration (CI) bots, used primarily to automate merge commits. After removing CI bots from the dataset the observed global trends were unchanged, therefore this paper presents unfiltered data.\\n\\n\\n\\\\medskip\\n\\\\emph{Future work.}\\nTo some extent the above limitations are the price to pay to study such a large dataset: there exists a trade-off between large-scale analysis and accuracy.\\nWe plan nonetheless to further investigate and mitigate them in future work.\\nMulti-method approaches, merging data mining with social science methods, could be applied to address some of the questions raised in this exploratory study.\\nWhile they do not scale to the whole dataset, multi-methods can be adopted to dig deeper into specific aspects, specifically those related to social phenomena.\\nSoftware is a social artifact, it is no wonder that aspects related to sociocultural evolution emerge when analyzing its evolution at this scale.\\n\\n\\n\\n\\n \\n\\\\clearpage\\n\\n\\n\",\n  '\\\\section{Introduction}\\n\\nOne of the fundamental ingredients in the theory of non-commutative or\\nquantum geometry is the notion of a differential calculus.\\nIn the framework of quantum groups the natural notion\\nis that of a\\nbicovariant differential calculus as introduced by Woronowicz\\n\\\\cite{Wor_calculi}. Due to the allowance of non-commutativity\\nthe uniqueness of a canonical calculus is lost.\\nIt is therefore desirable to classify the possible choices.\\nThe most important piece is the space of one-forms or ``first\\norder differential calculus\\'\\' to which we will restrict our attention\\nin the following. (From this point on we will use the term\\n``differential calculus\\'\\' to denote a\\nbicovariant first order differential calculus).\\n\\nMuch attention has been devoted to the investigation of differential\\ncalculi on quantum groups $C_q(G)$ of function algebra type for\\n$G$ a simple Lie group.\\nNatural differential calculi on matrix quantum groups were obtained by\\nJurco \\\\cite{Jur} and\\nCarow-Watamura et al.\\\\\\n\\\\cite{CaScWaWe}. A partial classification of calculi of the same\\ndimension as the natural ones\\nwas obtained by\\nSchm\\\\\"udgen and Sch\\\\\"uler \\\\cite{ScSc2}.\\nMore recently, a classification theorem for factorisable\\ncosemisimple quantum groups was obtained by Majid \\\\cite{Majid_calculi},\\ncovering the general $C_q(G)$ case. A similar result was\\nobtained later by Baumann and Schmitt \\\\cite{BaSc}.\\nAlso, Heckenberger and Schm\\\\\"udgen \\\\cite{HeSc} gave a\\ncomplete classification on $C_q(SL(N))$ and $C_q(Sp(N))$. \\n\\n\\nIn contrast, for $G$ not simple or semisimple the differential calculi\\non $C_q(G)$\\nare largely unknown. A particularly basic case is the Lie group $B_+$\\nassociated with the Lie algebra $\\\\lalg{b_+}$ generated by two elements\\n$X,H$ with the relation $[H,X]=X$. The quantum enveloping algebra\\n\\\\ensuremath{U_q(\\\\lalg{b_+})}{}\\nis self-dual, i.e.\\\\ is non-degenerately paired with itself \\\\cite{Drinfeld}.\\nThis has an interesting consequence: \\\\ensuremath{U_q(\\\\lalg{b_+})}{} may be identified with (a\\ncertain algebraic model of) \\\\ensuremath{C_q(B_+)}. The differential calculi on this\\nquantum group and on its ``classical limits\\'\\' \\\\ensuremath{C(B_+)}{} and \\\\ensuremath{U(\\\\lalg{b_+})}{}\\nwill be the main concern of this paper. We pay hereby equal attention\\nto the dual notion of ``quantum tangent space\\'\\'.\\n\\nIn section \\\\ref{sec:q} we obtain the complete classification of differential\\ncalculi on \\\\ensuremath{C_q(B_+)}{}. It turns out that (finite\\ndimensional) differential\\ncalculi are characterised by finite subsets $I\\\\subset\\\\mathbb{N}$.\\nThese\\nsets determine the decomposition into coirreducible (i.e.\\\\ not\\nadmitting quotients) differential calculi\\ncharacterised by single integers. For the coirreducible calculi the\\nexplicit formulas for the commutation relations and braided\\nderivations are given.\\n\\nIn section \\\\ref{sec:class} we give the complete classification for the\\nclassical function algebra \\\\ensuremath{C(B_+)}{}. It is essentially the same as in the\\n$q$-deformed setting and we stress this by giving an almost\\none-to-one correspondence of differential calculi to those obtained in\\nthe previous section. In contrast, however, the decomposition and\\ncoirreducibility properties do not hold at all. (One may even say that\\nthey are maximally violated). We give the explicit formulas for those\\ncalculi corresponding to coirreducible ones.\\n\\nMore interesting perhaps is the ``dual\\'\\' classical limit. I.e.\\\\ we\\nview \\\\ensuremath{U(\\\\lalg{b_+})}{} as a quantum function algebra with quantum enveloping\\nalgebra \\\\ensuremath{C(B_+)}{}. This is investigated in section \\\\ref{sec:dual}. It\\nturns out that in this setting we have considerably more freedom in\\nchoosing a\\ndifferential calculus since the bicovariance condition becomes much\\nweaker. This shows that this dual classical limit is in a sense\\n``unnatural\\'\\' as compared to the ordinary classical limit of section\\n\\\\ref{sec:class}. \\nHowever, we can still establish a correspondence of certain\\ndifferential calculi to those of section \\\\ref{sec:q}. The\\ndecomposition properties are conserved while the coirreducibility\\nproperties are not.\\nWe give the\\nformulas for the calculi corresponding to coirreducible ones.\\n\\nAnother interesting aspect of viewing \\\\ensuremath{U(\\\\lalg{b_+})}{} as a quantum function\\nalgebra is the connection to quantum deformed models of space-time and\\nits symmetries. In particular, the $\\\\kappa$-deformed Minkowski space\\ncoming from the $\\\\kappa$-deformed Poincar\\\\\\'e algebra\\n\\\\cite{LuNoRu}\\\\cite{MaRu} is just a simple generalisation of \\\\ensuremath{U(\\\\lalg{b_+})}.\\nWe use this in section \\\\ref{sec:kappa} to give\\na natural $4$-dimensional differential calculus. Then we show (in a\\nformal context) that integration is given by\\nthe usual Lesbegue integral on $\\\\mathbb{R}^n$ after normal ordering.\\nThis is obtained in an intrinsic context different from the standard\\n$\\\\kappa$-Poincar\\\\\\'e approach.\\n\\nA further important motivation for the investigation of differential\\ncalculi on\\n\\\\ensuremath{U(\\\\lalg{b_+})}{} and \\\\ensuremath{C(B_+)}{} is the relation of those objects to the Planck-scale\\nHopf algebra \\\\cite{Majid_Planck}\\\\cite{Majid_book}. This shall be\\ndeveloped elsewhere.\\n\\nIn the remaining parts of this introduction we will specify our\\nconventions and provide preliminaries on the quantum group \\\\ensuremath{U_q(\\\\lalg{b_+})}, its\\ndeformations, and differential calculi.\\n\\n\\n\\\\subsection{Conventions}\\n\\nThroughout, $\\\\k$ denotes a field of characteristic 0 and\\n$\\\\k(q)$ denotes the field of rational\\nfunctions in one parameter $q$ over $\\\\k$.\\n$\\\\k(q)$ is our ground field in\\nthe $q$-deformed setting, while $\\\\k$ is the\\nground field in the ``classical\\'\\' settings.\\nWithin section \\\\ref{sec:q} one could equally well view $\\\\k$ as the ground\\nfield with $q\\\\in\\\\k^*$ not a root of unity. This point of view is\\nproblematic, however, when obtaining ``classical limits\\'\\' as\\nin sections \\\\ref{sec:class} and \\\\ref{sec:dual}.\\n\\nThe positive integers are denoted by $\\\\mathbb{N}$ while the non-negative\\nintegers are denoted by $\\\\mathbb{N}_0$.\\nWe define $q$-integers, $q$-factorials and\\n$q$-binomials as follows:\\n\\\\begin{gather*}\\n[n]_q=\\\\sum_{i=0}^{n-1} q^i\\\\qquad\\n[n]_q!=[1]_q [2]_q\\\\cdots [n]_q\\\\qquad\\n\\\\binomq{n}{m}=\\\\frac{[n]_q!}{[m]_q! [n-m]_q!}\\n\\\\end{gather*}\\nFor a function of several variables (among\\nthem $x$) over $\\\\k$ we define\\n\\\\begin{gather*}\\n(T_{a,x} f)(x) = f(x+a)\\\\\\\\\\n(\\\\fdiff_{a,x} f)(x) = \\\\frac{f(x+a)-f(x)}{a}\\n\\\\end{gather*}\\nwith $a\\\\in\\\\k$ and similarly over $\\\\k(q)$\\n\\\\begin{gather*}\\n(Q_{m,x} f)(x) = f(q^m x)\\\\\\\\\\n(\\\\partial_{q,x} f)(x) = \\\\frac{f(x)-f(qx)}{x(1-q)}\\\\\\\\\\n\\\\end{gather*}\\nwith  $m\\\\in\\\\mathbb{Z}$.\\n\\nWe frequently use the notion of a polynomial in an extended\\nsense. Namely, if we have an algebra with an element $g$ and its\\ninverse $g^{-1}$ (as\\nin \\\\ensuremath{U_q(\\\\lalg{b_+})}{}) we will mean by a polynomial in $g,g^{-1}$ a finite power\\nseries in $g$ with exponents in $\\\\mathbb{Z}$. The length of such a polynomial\\nis the difference between highest and lowest degree.\\n\\nIf $H$ is a Hopf algebra, then $H^{op}$ will denote the Hopf algebra\\nwith the opposite product.\\n\\n\\\\subsection{\\\\ensuremath{U_q(\\\\lalg{b_+})}{} and its Classical Limits}\\n\\\\label{sec:intro_limits}\\n\\nWe recall that,\\nin the framework of quantum groups, the duality between enveloping algebra\\n$U(\\\\lalg{g})$ of the Lie algebra and algebra of functions $C(G)$ on the Lie\\ngroup carries over to $q$-deformations.\\nIn the case of\\n$\\\\lalg{b_+}$, the\\n$q$-deformed enveloping algebra \\\\ensuremath{U_q(\\\\lalg{b_+})}{} defined over $\\\\k(q)$ as\\n\\\\begin{gather*}\\nU_q(\\\\lalg{b_+})=\\\\k(q)\\\\langle X,g,g^{-1}\\\\rangle \\\\qquad\\n\\\\text{with relations} \\\\\\\\\\ng g^{-1}=1 \\\\qquad Xg=qgX \\\\\\\\\\n\\\\cop X=X\\\\otimes 1 + g\\\\otimes X \\\\qquad\\n\\\\cop g=g\\\\otimes g \\\\\\\\\\n\\\\cou (X)=0 \\\\qquad \\\\cou (g)=1 \\\\qquad\\n\\\\antip X=-g^{-1}X \\\\qquad \\\\antip g=g^{-1}\\n\\\\end{gather*}\\nis self-dual. Consequently, it\\nmay alternatively be viewed as the quantum algebra \\\\ensuremath{C_q(B_+)}{} of\\nfunctions on the Lie group $B_+$ associated with $\\\\lalg{b_+}$.\\nIt has two classical limits, the enveloping algebra \\\\ensuremath{U(\\\\lalg{b_+})}{}\\nand the function algebra $C(B_+)$.\\nThe transition to the classical enveloping algebra is achieved by\\nreplacing $q$\\nby $e^{-t}$ and $g$ by $e^{tH}$ in a formal power series setting in\\n$t$, introducing a new generator $H$. Now, all expressions are written in\\nthe form $\\\\sum_j a_j t^j$ and only the lowest order in $t$ is kept.\\nThe transition to the classical function algebra on the other hand is\\nachieved by setting $q=1$.\\nThis may be depicted as follows:\\n\\\\[\\\\begin{array}{c @{} c @{} c @{} c}\\n& \\\\ensuremath{U_q(\\\\lalg{b_+})} \\\\cong \\\\ensuremath{C_q(B_+)} && \\\\\\\\\\n& \\\\diagup \\\\hspace{\\\\stretch{1}} \\\\diagdown && \\\\\\\\\\n \\\\begin{array}{l} q=e^{-t} \\\\\\\\ g=e^{tH} \\\\end{array} \\\\Big| _{t\\\\to 0} \\n && q=1 &\\\\\\\\\\n \\\\swarrow &&& \\\\searrow \\\\\\\\\\n \\\\ensuremath{U(\\\\lalg{b_+})} & <\\\\cdots\\\\textrm{dual}\\\\cdots> && \\\\ensuremath{C(B_+)}\\n\\\\end{array}\\\\]\\nThe self-duality of \\\\ensuremath{U_q(\\\\lalg{b_+})}{} is expressed as a pairing\\n$\\\\ensuremath{U_q(\\\\lalg{b_+})}\\\\times\\\\ensuremath{U_q(\\\\lalg{b_+})}\\\\to\\\\k$\\nwith\\nitself:\\n\\\\[\\\\langle X^n g^m, X^r g^s\\\\rangle =\\n \\\\delta_{n,r} [n]_q!\\\\, q^{-n(n-1)/2} q^{-ms}\\n \\\\qquad\\\\forall n,r\\\\in\\\\mathbb{N}_0\\\\: m,s\\\\in\\\\mathbb{Z}\\\\]\\nIn the classical limit this becomes the pairing $\\\\ensuremath{U(\\\\lalg{b_+})}\\\\times\\\\ensuremath{C(B_+)}\\\\to\\\\k$\\n\\\\begin{equation}\\n\\\\langle X^n H^m, X^r g^s\\\\rangle =\\n \\\\delta_{n,r} n!\\\\, s^m\\\\qquad \\\\forall n,m,r\\\\in\\\\mathbb{N}_0\\\\: s\\\\in\\\\mathbb{Z}\\n\\\\label{eq:pair_class}\\n\\\\end{equation} \\n\\n\\n\\n\\\\subsection{Differential Calculi and Quantum Tangent Spaces}\\n\\nIn this section we recall some facts about differential calculi\\nalong the lines of Majid\\'s treatment in \\\\cite{Majid_calculi}.\\n\\nFollowing Woronowicz \\\\cite{Wor_calculi}, first order bicovariant differential\\ncalculi on a quantum group $A$ (of\\nfunction algebra type) are in one-to-one correspondence to submodules\\n$M$ of $\\\\ker\\\\cou\\\\subset A$ in the category $^A_A\\\\cal{M}$ of (say) left\\ncrossed modules of $A$ via left multiplication and left adjoint\\ncoaction:\\n\\\\[\\na\\\\triangleright v = av \\\\qquad \\\\mathrm{Ad_L}(v)\\n =v_{(1)}\\\\antip v_{(3)}\\\\otimes v_{(2)}\\n\\\\qquad \\\\forall a\\\\in A, v\\\\in A\\n\\\\]\\nMore precisely, given a crossed submodule $M$, the corresponding\\ncalculus is given by $\\\\Gamma=\\\\ker\\\\cou/M\\\\otimes A$ with $\\\\diff a =\\n\\\\pi(\\\\cop a - 1\\\\otimes a)$ ($\\\\pi$ the canonical projection).\\nThe right action and coaction on $\\\\Gamma$ are given by\\nthe right multiplication and coproduct on $A$, the left action and\\ncoaction by the tensor product ones with $\\\\ker\\\\cou/M$ as a left\\ncrossed module. In all of what follows, ``differential calculus\\'\\' will\\nmean ``bicovariant first order differential calculus\\'\\'.\\n\\nAlternatively \\\\cite{Majid_calculi}, given in addition a quantum group $H$\\ndually paired with $A$\\n(which we might think of as being of enveloping algebra type), we can\\nexpress the coaction of $A$ on\\nitself as an action of $H^{op}$ using the pairing:\\n\\\\[\\nh\\\\triangleright v = \\\\langle h, v_{(1)} \\\\antip v_{(3)}\\\\rangle v_{(2)}\\n\\\\qquad \\\\forall h\\\\in H^{op}, v\\\\in A\\n\\\\]\\nThereby we change from the category of (left) crossed $A$-modules to\\nthe category of left modules of the quantum double $A\\\\!\\\\bowtie\\\\! H^{op}$.\\n\\nIn this picture the pairing between $A$ and $H$ descends to a pairing\\nbetween $A/\\\\k 1$ (which we may identify with $\\\\ker\\\\cou\\\\subset A$) and\\n$\\\\ker\\\\cou\\\\subset H$. Further quotienting $A/\\\\k 1$ by $M$ (viewed in\\n$A/\\\\k 1$) leads to a pairing with the subspace $L\\\\subset\\\\ker\\\\cou H$\\nthat annihilates $M$. $L$ is called a ``quantum tangent space\\'\\'\\nand is dual to the differential calculus $\\\\Gamma$ generated by $M$ in\\nthe sense that $\\\\Gamma\\\\cong \\\\Lin(L,A)$ via\\n\\\\begin{equation}\\nA/(\\\\k 1+M)\\\\otimes A \\\\to \\\\Lin(L,A)\\\\qquad\\nv\\\\otimes a \\\\mapsto \\\\langle \\\\cdot, v\\\\rangle a\\n\\\\label{eq:eval}\\n\\\\end{equation}\\nif the pairing between $A/(\\\\k 1+M)$ and $L$ is non-degenerate.\\n\\nThe quantum tangent spaces are obtained directly by dualising the\\n(left) action of the quantum double on $A$ to a (right) action on\\n$H$. Explicitly, this is the adjoint action and the coregular action\\n\\\\[\\nh \\\\triangleright x = h_{(1)} x \\\\antip h_{(2)} \\\\qquad\\na \\\\triangleright x = \\\\langle x_{(1)}, a \\\\rangle x_{(2)}\\\\qquad\\n \\\\forall h\\\\in H, a\\\\in A^{op},x\\\\in A\\n\\\\]\\nwhere we have converted the right action to a left action by going\\nfrom \\\\mbox{$A\\\\!\\\\bowtie\\\\! H^{op}$}-modules to \\\\mbox{$H\\\\!\\\\bowtie\\\\! A^{op}$}-modules.\\nQuantum tangent spaces are subspaces of $\\\\ker\\\\cou\\\\subset H$ invariant\\nunder the projection of this action to $\\\\ker\\\\cou$ via \\\\mbox{$x\\\\mapsto\\nx-\\\\cou(x) 1$}. Alternatively, the left action of $A^{op}$ can be\\nconverted to a left coaction of $H$ being the comultiplication (with\\nsubsequent projection onto $H\\\\otimes\\\\ker\\\\cou$).\\n\\nWe can use the evaluation map (\\\\ref{eq:eval})\\nto define a ``braided derivation\\'\\' on elements of the quantum tangent\\nspace via\\n\\\\[\\\\partial_x:A\\\\to A\\\\qquad \\\\partial_x(a)={\\\\diff a}(x)=\\\\langle\\nx,a_{(1)}\\\\rangle a_{(2)}\\\\qquad\\\\forall x\\\\in L, a\\\\in A\\\\]\\nThis obeys the braided derivation rule\\n\\\\[\\\\partial_x(a b)=(\\\\partial_x a) b\\n + a_{(2)} \\\\partial_{a_{(1)}\\\\triangleright x}b\\\\qquad\\\\forall x\\\\in L, a\\\\in A\\\\]\\n\\nGiven a right invariant basis $\\\\{\\\\eta_i\\\\}_{i\\\\in I}$ of $\\\\Gamma$ with a\\ndual basis $\\\\{\\\\phi_i\\\\}_{i\\\\in I}$ of $L$ we have\\n\\\\[{\\\\diff a}=\\\\sum_{i\\\\in I} \\\\eta_i\\\\cdot \\\\partial_i(a)\\\\qquad\\\\forall a\\\\in A\\\\]\\nwhere we denote $\\\\partial_i=\\\\partial_{\\\\phi_i}$. (This can be easily\\nseen to hold by evaluation against $\\\\phi_i\\\\ \\\\forall i$.)\\n\\n\\n\\\\section{Classification on \\\\ensuremath{C_q(B_+)}{} and \\\\ensuremath{U_q(\\\\lalg{b_+})}{}}\\n\\\\label{sec:q}\\n\\nIn this section we completely classify differential calculi on \\\\ensuremath{C_q(B_+)}{}\\nand, dually, quantum tangent spaces on \\\\ensuremath{U_q(\\\\lalg{b_+})}{}. We start by\\nclassifying the relevant crossed modules and then proceed to a\\ndetailed description of the calculi.\\n\\n\\\\begin{lem}\\n\\\\label{lem:cqbp_class}\\n(a) Left crossed \\\\ensuremath{C_q(B_+)}-submodules $M\\\\subseteq\\\\ensuremath{C_q(B_+)}$ by left\\nmultiplication and left\\nadjoint coaction are in one-to-one correspondence to\\npairs $(P,I)$\\nwhere $P\\\\in\\\\k(q)[g]$ is a polynomial with $P(0)=1$ and $I\\\\subset\\\\mathbb{N}$ is\\nfinite.\\n$\\\\codim M<\\\\infty$ iff $P=1$. In particular $\\\\codim M=\\\\sum_{n\\\\in I}n$\\nif $P=1$.\\n\\n(b) The finite codimensional maximal $M$\\ncorrespond to the pairs $(1,\\\\{n\\\\})$ with $n$ the\\ncodimension. The infinite codimensional maximal $M$ are characterised by\\n$(P,\\\\emptyset)$ with $P$ irreducible and $P(g)\\\\neq 1-q^{-k}g$ for any\\n$k\\\\in\\\\mathbb{N}_0$.\\n\\n(c) Crossed submodules $M$ of finite\\ncodimension are intersections of maximal ones.\\nIn particular $M=\\\\bigcap_{n\\\\in I} M^n$, with $M^n$ corresponding to\\n$(1,\\\\{n\\\\})$.\\n\\\\end{lem}\\n\\\\begin{proof}\\n(a) Let $M\\\\subseteq\\\\ensuremath{C_q(B_+)}$ be a crossed \\\\ensuremath{C_q(B_+)}-submodule by left\\nmultiplication and left adjoint coaction and let\\n$\\\\sum_n X^n P_n(g) \\\\in M$, where $P_n$ are polynomials in $g,g^{-1}$\\n(every element of \\\\ensuremath{C_q(B_+)}{} can be expressed in\\nthis form). From the formula for the coaction ((\\\\ref{eq:adl}), see appendix)\\nwe observe that for all $n$ and for all $t\\\\le n$ the element\\n\\\\[X^t P_n(g) \\\\prod_{s=1}^{n-t} (1-q^{s-n}g)\\\\]\\nlies in $M$.\\nIn particular\\nthis is true for $t=n$, meaning that elements of constant degree in $X$\\nlie separately in $M$. It is therefore enough to consider such\\nelements.\\n\\nLet now $X^n P(g) \\\\in M$.\\nBy left multiplication $X^n P(g)$ generates any element of the form\\n$X^k P(g) Q(g)$, where $k\\\\ge n$ and $Q$ is any polynomial in\\n$g,g^{-1}$. (Note that $Q(q^kg) X^k=X^k Q(g)$.)\\nWe see that $M$ contains the following elements:\\n\\\\[\\\\begin{array}{ll}\\n\\\\vdots & \\\\\\\\\\nX^{n+2} & P(g) \\\\\\\\\\nX^{n+1} & P(g) \\\\\\\\\\nX^n & P(g) \\\\\\\\\\nX^{n-1} & P(g) (1-q^{1-n}g) \\\\\\\\\\nX^{n-2} & P(g) (1-q^{1-n}g) (1-q^{2-n}g) \\\\\\\\\\n\\\\vdots & \\\\\\\\\\nX & P(g) (1-q^{1-n}g) (1-q^{2-n}g) \\\\ldots (1-q^{-1}g) \\\\\\\\\\n& P(g) (1-q^{1-n}g) (1-q^{2-n}g) \\\\ldots (1-q^{-1}g)(1-g) \\n\\\\end{array}\\n\\\\]\\nMoreover, if $M$ is generated by $X^n P(g)$ as a module\\nthen these elements generate a basis for $M$ as a vector\\nspace by left\\nmultiplication with polynomials in $g,g^{-1}$. (Observe that the\\napplication of the coaction to any of the elements shown does not\\ngenerate elements of new type.)\\n\\nNow, let $M$ be a given crossed submodule. We pick, among the\\nelements in $M$ of the form $X^n P(g)$ with $P$ of minimal\\nlength,\\none\\nwith lowest degree in $X$. Then certainly the elements listed above are\\nin $M$. Furthermore for any element of the form $X^k Q(g)$, $Q$ must\\ncontain $P$ as a factor and for $k<n$, $Q$ must contain $P(g) (1-q^{1-n}g)$\\nas a factor. We continue by picking the smallest $n_2$, so that\\n$X^{n_2} P(g) (1-q^{1-n}g) \\\\in M$. Certainly $n_2<n$. Again, for any\\nelement of $X^l Q(g)$ in $M$ with $l<n_2$, we have that\\n$P(g) (1-q^{1-n}g) (1-q^{1-n_2}g)$ divides Q(g). We proceed by\\ninduction, until we arrive at degree zero in $X$.\\n\\nWe obtain the following elements generating a basis for $M$ by left\\nmultiplication with polynomials in $g,g^{-1}$ (rename $n_1=n$):\\n\\n\\\\[ \\\\begin{array}{ll}\\n\\\\vdots & \\\\\\\\\\nX^{n_1+1} & P(g) \\\\\\\\\\nX^{n_1} & P(g) \\\\\\\\\\nX^{n_1-1} & P(g) (1-q^{1-{n_1}}g) \\\\\\\\\\n\\\\vdots & \\\\\\\\\\nX^{n_2} & P(g) (1-q^{1-{n_1}}g) \\\\\\\\\\nX^{n_2-1} & P(g) (1-q^{1-{n_1}}g) (1-q^{1-n_2})\\\\\\\\\\n\\\\vdots & \\\\\\\\\\nX^{n_3} & P(g) (1-q^{1-{n_1}}g) (1-q^{1-{n_2}}g) \\\\\\\\\\nX^{n_3-1} & P(g) (1-q^{1-{n_1}}g) (1-q^{1-{n_2}}g) (1-q^{1-n_3})\\\\\\\\\\n\\\\vdots & \\\\\\\\\\n& P(g) (1-q^{1-{n_1}}g) (1-q^{1-n_2}g) (1-q^{1-n_3}g) \\\\ldots (1-q^{1-n_m}g) \\n\\\\end{array}\\n\\\\]\\nWe see that the integers $n_1,\\\\ldots,n_m$ uniquely determine the shape\\nof this picture. The polynomial $P(g)$ on the other hand can be\\nshifted (by $g$ and $g^{-1}$) or renormalised. To determine $M$\\nuniquely we shift and normalise $P$ in such a way that it contains no\\nnegative powers\\nand has unit constant coefficient. $P$ can then be viewed as a\\npolynomial $\\\\in\\\\k(q)[g]$.\\n\\nWe see that the codimension of $M$ is the sum of the lengths of the\\npolynomials in $g$ over all degrees in $X$ in the above\\npicture. Finite codimension corresponds to $P=1$. In this\\ncase the codimension is the sum\\n$n_1+\\\\ldots +n_m$.\\n\\n(b) We observe that polynomials of the form $1-q^{j}g$\\nhave no common divisors for distinct $j$. Therefore,\\nfinite codimensional crossed\\nsubmodules are maximal if and only if\\nthere is just one integer ($m=1$). Thus, the maximal left\\ncrossed submodule of\\ncodimension $k$ is generated by $X^k$ and $1-q^{1-k}g$.\\nFor an infinite codimensional crossed submodule we certainly need\\n$m=0$. Then, the maximality corresponds to irreducibility of\\n$P$.\\n\\n(c) This is again due to the distinctness of factors $1-q^j g$.\\n\\\\end{proof}\\n\\n\\n\\\\begin{cor}\\n\\\\label{cor:cqbp_eclass}\\n(a) Left crossed \\\\ensuremath{C_q(B_+)}-submodules $M\\\\subseteq\\\\ker\\\\cou\\\\subset\\\\ensuremath{C_q(B_+)}$\\nare in one-to-one correspondence to pairs\\n$(P,I)$ as in lemma \\\\ref{lem:cqbp_class}\\nwith the additional constraint $(1-g)$ divides $P(g)$ or $1\\\\in I$.\\n$\\\\codim M<\\\\infty$ iff $P=1$. In particular $\\\\codim M=(\\\\sum_{n\\\\in I}n)-1$\\nif $P=1$.\\n\\n(b) The finite codimensional maximal $M$\\ncorrespond to the pairs\\n$(1,\\\\{1,n\\\\})$ with $n\\\\ge 2$ the\\ncodimension. The infinite codimensional maximal $M$ correspond to pairs\\n$(P,\\\\{1\\\\})$ with $P$ irreducible and $P(g)\\\\neq 1-q^{-k}g$ for any\\n$k\\\\in\\\\mathbb{N}_0$.\\n\\n(c) Crossed submodules $M$ of finite\\ncodimension are intersections of maximal ones.\\nIn particular $M=\\\\bigcap_{n\\\\in I} M^n$, with $M^n$ corresponding to\\n$(1,\\\\{1,n\\\\})$.\\n\\\\end{cor}\\n\\\\begin{proof}\\nFirst observe that $\\\\sum_n X^n P_n(g)\\\\in \\\\ker\\\\cou$ if and only if\\n$(1-g)$ divides $P_0(g)$. This is to say that that $\\\\ker\\\\cou$\\nis the crossed submodule corresponding to the pair $(1,\\\\{1\\\\})$ in\\nlemma \\\\ref{lem:cqbp_class}. We obtain the classification\\nfrom the one of lemmas \\\\ref{lem:cqbp_class} by intersecting\\neverything with this crossed submodule. In particular, this reduces\\nthe codimension by one in the finite codimensional case.\\n\\\\end{proof}\\n\\n\\n\\n\\\\begin{lem}\\n\\\\label{lem:uqbp_class}\\n(a) Left crossed \\\\ensuremath{U_q(\\\\lalg{b_+})}-submodules $L\\\\subseteq\\\\ensuremath{U_q(\\\\lalg{b_+})}$ via the left adjoint\\naction and left\\nregular coaction are in one-to-one correspondence to the set\\n$3^{\\\\mathbb{N}_0}\\\\times2^{\\\\mathbb{N}}$.\\nFinite dimensional $L$ are in one-to-one correspondence to\\nfinite sets $I\\\\subset\\\\mathbb{N}$ and $\\\\dim L=\\\\sum_{n\\\\in I}n$.\\n\\n(b) Finite dimensional irreducible $L$ correspond to $\\\\{n\\\\}$\\nwith $n$ the dimension.\\n\\n(c) Finite dimensional $L$ are direct sums of irreducible ones. In\\nparticular $L=\\\\oplus_{n\\\\in I} L^n$ with $L^n$ corresponding to $\\\\{n\\\\}$.\\n\\\\end{lem}\\n\\\\begin{proof}\\n(a) The action takes the explicit form\\n\\\\[g\\\\triangleright X^n g^k = q^{-n} X^n g^k\\\\qquad\\nX\\\\triangleright X^n g^k = X^{n+1}g^k(1-q^{-(n+k)})\\\\]\\nwhile the coproduct is\\n\\\\[\\\\cop(X^n g^k)=\\\\sum_{r=0}^{n} \\\\binomq{n}{r}\\n q^{-r(n-r)} X^{n-r} g^{k+r}\\\\otimes X^r g^k\\\\]\\nwhich we view as a left coaction here.\\nLet now $L\\\\subseteq\\\\ensuremath{U_q(\\\\lalg{b_+})}$ be a crossed \\\\ensuremath{U_q(\\\\lalg{b_+})}-submodule via this action\\nand coaction. For $\\\\sum_n X^n P_n(g)\\\\in L$ invariance under\\nthe action by\\n$g$ clearly means that \\\\mbox{$X^n P_n(g)\\\\in L\\\\ \\\\forall n$}. Then from\\ninvariance under the coaction we can conclude that\\nif $X^n \\\\sum_j a_j g^j\\\\in L$ we must have\\n$X^n g^j\\\\in L\\\\ \\\\forall j$.\\nI.e.\\\\ elements of the form $X^n g^j$ lie separately in $L$ and it is\\nsufficient to consider such elements. From the coaction we learn that\\nif $X^n g^j\\\\in L$ we have $X^m g^j\\\\in L\\\\ \\\\forall m\\\\le n$.\\nThe action\\nby $X$ leads to $X^n g^j\\\\in L \\\\Rightarrow X^{n+1} g^j\\\\in\\nL$ except if\\n$n+j=0$. The classification is given by the possible choices we have\\nfor each power in $g$. For every positive integer $j$ we can\\nchoose wether or not to include the span of\\n$\\\\{ X^n g^j|\\\\forall n\\\\}$ in $L$ and for\\nevery non-positive\\ninteger we can choose to include either the span of $\\\\{ X^n\\ng^j|\\\\forall n\\\\}$\\nor just\\n$\\\\{ X^n g^j|\\\\forall n\\\\le -j\\\\}$ or neither. I.e.\\\\ for positive\\nintegers ($\\\\mathbb{N}$) we have two choices while for non-positive (identified\\nwith $\\\\mathbb{N}_0$) ones we have three choices.\\n\\nClearly, the finite dimensional $L$ are those where we choose only to\\ninclude finitely many powers of $g$ and also only finitely many powers\\nof $X$. The latter is only possible for the non-positive powers\\nof $g$.\\nBy identifying positive integers $n$ with powers $1-n$ of $g$, we\\nobtain a classification by finite subsets of $\\\\mathbb{N}$.\\n\\n(b) Irreducibility clearly corresponds to just including one power of $g$\\nin the finite dimensional case.\\n\\n(c) The decomposition property is obvious from the discussion.\\n\\\\end{proof}\\n\\n\\n\\\\begin{cor}\\n\\\\label{cor:uqbp_eclass}\\n(a) Left crossed \\\\ensuremath{U_q(\\\\lalg{b_+})}-submodules $L\\\\subseteq\\\\ker\\\\cou\\\\subset\\\\ensuremath{U_q(\\\\lalg{b_+})}$ via\\nthe left adjoint\\naction and left regular coaction (with subsequent projection to\\n$\\\\ker\\\\cou$ via $x\\\\mapsto x-\\\\cou(x)1$) are in one-to-one correspondence to\\nthe set $3^{\\\\mathbb{N}}\\\\times2^{\\\\mathbb{N}_0}$.\\nFinite dimensional $L$ are in one-to-one correspondence to\\nfinite sets\\n$I\\\\subset\\\\mathbb{N}\\\\setminus\\\\{1\\\\}$ and $\\\\dim L=\\\\sum_{n\\\\in I}n$.\\n\\n(b) Finite dimensional irreducible $L$ correspond to $\\\\{n\\\\}$\\nwith $n\\\\ge 2$ the dimension.\\n\\n(c) Finite dimensional $L$ are direct sums of irreducible ones. In\\nparticular $L=\\\\oplus_{n\\\\in I} L^n$ with $L^n$ corresponding to $\\\\{n\\\\}$.\\n\\\\end{cor}\\n\\\\begin{proof}\\nOnly a small modification of lemma \\\\ref{lem:uqbp_class} is\\nnecessary. Elements of\\nthe form $P(g)$ are replaced by elements of the form\\n$P(g)-P(1)$. Monomials with non-vanishing degree in $X$ are unchanged.\\nThe choices for elements of degree $0$ in $g$ are reduced to either\\nincluding the span of\\n$\\\\{ X^k |\\\\forall k>0 \\\\}$ in the crossed submodule or not. In\\nparticular, the crossed submodule characterised by \\\\{1\\\\} in lemma\\n\\\\ref{lem:uqbp_class} is projected out.\\n\\\\end{proof}\\n\\nDifferential calculi in the original sense of Woronowicz are\\nclassified by corollary \\\\ref{cor:cqbp_eclass} while from the quantum\\ntangent space\\npoint of view the\\nclassification is given by corollary \\\\ref{cor:uqbp_eclass}.\\nIn the finite dimensional case the duality is strict in the sense of a\\none-to-one correspondence.\\nThe infinite dimensional case on the other hand depends strongly on\\nthe algebraic models we use for the function or enveloping\\nalgebras. It is therefore not surprising that in the present purely\\nalgebraic context the classifications are quite different in this\\ncase. We will restrict ourselves to the finite dimensional\\ncase in the following description of the differential calculi.\\n\\n\\n\\\\begin{thm}\\n\\\\label{thm:q_calc}\\n(a) Finite dimensional differential calculi $\\\\Gamma$ on \\\\ensuremath{C_q(B_+)}{} and\\ncorresponding quantum tangent spaces $L$ on \\\\ensuremath{U_q(\\\\lalg{b_+})}{} are\\nin one-to-one correspondence to\\nfinite sets $I\\\\subset\\\\mathbb{N}\\\\setminus\\\\{1\\\\}$. In particular\\n$\\\\dim\\\\Gamma=\\\\dim L=\\\\sum_{n\\\\in I}n$.\\n\\n(b) Coirreducible $\\\\Gamma$ and irreducible $L$ correspond to\\n$\\\\{n\\\\}$ with $n\\\\ge 2$ the dimension.\\nSuch a $\\\\Gamma$ has a\\nright invariant basis $\\\\eta_0,\\\\dots,\\\\eta_{n-1}$ so that the relations\\n\\\\begin{gather*}\\n\\\\diff X=\\\\eta_1+(q^{n-1}-1)\\\\eta_0 X \\\\qquad\\n \\\\diff g=(q^{n-1}-1)\\\\eta_0 g\\\\\\\\\\n[a,\\\\eta_0]=\\\\diff a\\\\quad \\\\forall a\\\\in\\\\ensuremath{C_q(B_+)}\\\\\\\\\\n[g,\\\\eta_i]_{q^{n-1-i}}=0\\\\quad \\\\forall i\\\\qquad\\n[X,\\\\eta_i]_{q^{n-1-i}}=\\\\begin{cases}\\n \\\\eta_{i+1} & \\\\text{if}\\\\ i<n-1 \\\\\\\\\\n 0 & \\\\text{if}\\\\ i=n-1\\n \\\\end{cases}\\n\\\\end{gather*}\\nhold, where $[a,b]_p := a b - p b a$. By choosing the dual basis on\\nthe corresponding irreducible $L$ we obtain\\nthe braided derivations\\n\\\\begin{gather*}\\n\\\\partial_i\\\\no{f}=\\n \\\\no{Q_{n-1-i,g} Q_{n-1-i,X} \\\\frac{1}{[i]_q!} (\\\\partial_{q,X})^i f}\\n \\\\qquad\\\\forall i\\\\ge 1\\\\\\\\\\n\\\\partial_0\\\\no{f}=\\n \\\\no{Q_{n-1,g} Q_{n-1,X} f - f}\\n\\\\end{gather*}\\nfor $f\\\\in \\\\k(q)[X,g,g^{-1}]$ with normal ordering\\n$\\\\k(q)[X,g,g^{-1}]\\\\to \\\\ensuremath{C_q(B_+)}$ given by \\\\mbox{$g^n X^m\\\\mapsto g^n X^m$}.\\n\\n(c) Finite dimensional $\\\\Gamma$ and $L$ decompose into direct sums of\\ncoirreducible respectively irreducible ones.\\nIn particular $\\\\Gamma=\\\\oplus_{n\\\\in I}\\\\Gamma^n$ and\\n$L=\\\\oplus_{n\\\\in I}L^n$ with $\\\\Gamma^n$ and $L^n$ corresponding to $\\\\{n\\\\}$.\\n\\\\end{thm}\\n\\\\begin{proof}\\n(a) We observe that the classifications of lemma\\n\\\\ref{lem:cqbp_class} and lemma \\\\ref{lem:uqbp_class} or\\ncorollary \\\\ref{cor:cqbp_eclass} and corollary \\\\ref{cor:uqbp_eclass}\\nare dual to each other in the finite (co){}dimensional case. More\\nprecisely, for $I\\\\subset\\\\mathbb{N}$ finite the crossed submodule $M$\\ncorresponding to $(1,I)$ in lemma \\\\ref{lem:cqbp_class} is the\\nannihilator of the crossed\\nsubmodule $L$ corresponding to $I$ in lemma \\\\ref{lem:uqbp_class} \\nand vice versa.\\n$\\\\ensuremath{C_q(B_+)}/M$ and $L$ are dual spaces with the induced pairing.\\nFor $I\\\\subset\\\\mathbb{N}\\\\setminus\\\\{1\\\\}$ finite this descends to \\n$M$ corresponding to $(1,I\\\\cup\\\\{1\\\\})$ in corollary\\n\\\\ref{cor:cqbp_eclass} and $L$ corresponding to $I$ in corollary\\n\\\\ref{cor:uqbp_eclass}.\\nFor the dimension of $\\\\Gamma$ observe\\n$\\\\dim\\\\Gamma=\\\\dim{\\\\ker\\\\cou/M}=\\\\codim M$.\\n\\n(b) Coirreducibility (having no proper quotient) of $\\\\Gamma$\\nclearly corresponds to maximality of $M$. The statement then follows\\nfrom parts (b) of corollaries\\n\\\\ref{cor:cqbp_eclass} and \\\\ref{cor:uqbp_eclass}. The formulas are\\nobtained by choosing the basis $\\\\eta_0,\\\\dots,\\\\eta_{n-1}$ of\\n$\\\\ker\\\\cou/M$ as the equivalence classes of \\n\\\\[(g-1)/(q^{n-1}-1),X,\\\\dots,X^{n-1}\\\\]\\nThe dual basis of $L$ is then given by\\n\\\\[g^{1-n}-1, X g^{1-n},\\\\dots, q^{k(k-1)} \\\\frac{1}{[k]_q!} X^k g^{1-n},\\n\\\\dots,q^{(n-1)(n-2)} \\\\frac{1}{[n-1]_q!} X^{n-1} g^{1-n}\\\\]\\n\\n(c) The statement follows from corollaries \\\\ref{cor:cqbp_eclass} and \\n\\\\ref{cor:uqbp_eclass} parts (c) with the observation\\n\\\\[\\\\ker\\\\cou/M=\\\\ker\\\\cou/{\\\\bigcap_{n\\\\in I}}M^n\\n=\\\\oplus_{n\\\\in I}\\\\ker\\\\cou/M^n\\\\]\\n\\\\end{proof}\\n\\n\\\\begin{cor}\\nThere is precisely one differential calculus on \\\\ensuremath{C_q(B_+)}{} which is\\nnatural in the sense that it\\nhas dimension $2$.\\nIt is coirreducible and obeys the relations\\n\\\\begin{gather*}\\n[g,\\\\diff X]=0\\\\qquad [g,\\\\diff g]_q=0\\\\qquad\\n[X,\\\\diff X]_q=0\\\\qquad [X,\\\\diff g]_q=(q-1)({\\\\diff X}) g\\n\\\\end{gather*}\\nwith $[a,b]_q:=ab-qba$. In particular we have\\n\\\\begin{gather*}\\n\\\\diff\\\\no{f} = {\\\\diff g} \\\\no{\\\\partial_{q,g} f} + {\\\\diff X}\\n\\\\no{\\\\partial_{q,X} f}\\\\qquad\\\\forall f\\\\in \\\\k(q)[X,g,g^{-1}]\\n\\\\end{gather*}\\n\\\\end{cor}\\n\\\\begin{proof}\\nThis is a special case of theorem \\\\ref{thm:q_calc}.\\nThe formulas follow from (b) with $n=2$.\\n\\\\end{proof}\\n\\n\\n\\n\\\\section{Classification in the Classical Limit}\\n\\\\label{sec:class}\\n\\nIn this section we give the complete classification of differential\\ncalculi and quantum tangent spaces in the classical case of \\\\ensuremath{C(B_+)}{}\\nalong the lines of the previous section.\\nWe pay particular\\nattention to the relation to the $q$-deformed setting.\\n\\n\\nThe classical limit \\\\ensuremath{C(B_+)}{} of the quantum group \\\\ensuremath{C_q(B_+)}{} is\\nsimply obtained by substituting the parameter $q$ with $1$.\\nThe\\nclassification of left crossed submodules in part (a) of lemma\\n\\\\ref{lem:cqbp_class} remains\\nunchanged, as one may check by going through the proof.\\nIn particular, we get a correspondence of crossed modules in the\\n$q$-deformed setting with crossed modules in the\\nclassical setting\\nas a map of \\npairs $(P,I)\\\\mapsto (P,I)$\\nthat converts polynomials $\\\\k(q)[g]$ to polynomials $\\\\k[g]$ (if\\ndefined) and leaves\\nsets $I$ unchanged. This is one-to-one in the finite\\ndimensional case.\\nHowever, we did use the distinctness of powers of $q$ in part (b) and\\n(c) of lemma\\n$\\\\ref{lem:cqbp_class}$ and have to account for changing this. The\\nonly place where we used it, was in observing that\\nfactors $1-q^j g $ have no common divisors for distinct $j$. This was\\ncrucial to conclude the maximality (b) of certain finite codimensional\\ncrossed submodules and the intersection property (c).\\nNow, all those factors become $1-g$.\\n\\n\\\\begin{cor}\\n\\\\label{cor:cbp_class}\\n(a) Left crossed \\\\ensuremath{C(B_+)}-submodules $M\\\\subseteq\\\\ensuremath{C(B_+)}$ by left\\nmultiplication and left\\nadjoint coaction are in one-to-one correspondence to\\npairs $(P,I)$\\nwhere $P\\\\in\\\\k[g]$ is a polynomial with $P(0)=1$ and $I\\\\subset\\\\mathbb{N}$ is\\nfinite.\\n$\\\\codim M<\\\\infty$ iff $P=1$. In particular $\\\\codim M=\\\\sum_{n\\\\in I}n$\\nif $P=1$.\\n\\n(b) The infinite codimensional maximal $M$ are characterised by\\n$(P,\\\\emptyset)$ with $P$ irreducible and $P(g)\\\\neq 1-g$ for any\\n$k\\\\in\\\\mathbb{N}_0$.\\n\\\\end{cor}\\n\\nIn the restriction to $\\\\ker\\\\cou\\\\subset\\\\ensuremath{C(B_+)}$ corresponding to corollary\\n\\\\ref{cor:cqbp_eclass} we observe another difference to the\\n$q$-deformed setting.\\nSince the condition for a crossed submodule to lie in $\\\\ker\\\\cou$ is exactly\\nto have factors $1-g$ in the $X$-free monomials this condition may now\\nbe satisfied more easily. If the characterising polynomial does not\\ncontain this factor it is now sufficient to have just any non-empty\\ncharacterising integer set $I$ and it need not contain $1$. Consequently,\\nthe map $(P,I)\\\\mapsto (P,I)$ does not reach all crossed submodules now.\\n\\n\\\\begin{cor}\\n\\\\label{cor:cbp_eclass}\\n(a) Left crossed \\\\ensuremath{C(B_+)}-submodules $M\\\\subseteq\\\\ker\\\\cou\\\\subset\\\\ensuremath{C(B_+)}$\\nare in one-to-one correspondence to pairs\\n$(P,I)$ as in corollary \\\\ref{cor:cbp_class}\\nwith the additional constraint $(1-g)$ divides $P(g)$ or $I$ non-empty.\\n$\\\\codim M<\\\\infty$ iff $P=1$. In particular $\\\\codim M=(\\\\sum_{n\\\\in I}n)-1$\\nif $P=1$.\\n\\n(b) The infinite codimensional maximal $M$ correspond to pairs\\n$(P,\\\\{1\\\\})$ with $P$ irreducible and $P(g)\\\\neq 1-g$.\\n\\\\end{cor}\\n\\n\\nLet us now turn to quantum tangent spaces on \\\\ensuremath{U(\\\\lalg{b_+})}{}. Here, the process\\nto go from the $q$-deformed setting to the classical one is not quite\\nso straightforward.\\n\\n\\\\begin{lem}\\n\\\\label{lem:ubp_class}\\nProper left crossed \\\\ensuremath{U(\\\\lalg{b_+})}-submodules $L\\\\subset\\\\ensuremath{U(\\\\lalg{b_+})}$ via the left\\nadjoint action\\nand left regular coaction are\\nin one-to-one correspondence to pairs $(l,I)$ with $l\\\\in\\\\mathbb{N}_0$ and\\n$I\\\\subset\\\\mathbb{N}$ finite. $\\\\dim L<\\\\infty$ iff $l=0$. In particular $\\\\dim\\nL=\\\\sum_{n\\\\in I}n$ if $l=0$.\\n\\\\end{lem}\\n\\\\begin{proof}\\nThe left adjoint action takes the form\\n\\\\[\\nX\\\\triangleright X^n H^m = X^{n+1}(H^m-(H+1)^m) \\\\qquad\\nH\\\\triangleright X^n H^m = n X^n H^m\\n\\\\]\\nwhile the coaction is\\n\\\\[\\n\\\\cop(X^n H^m) = \\\\sum_{i=1}^n \\\\sum_{j=1}^m \\\\binom{n}{i} \\\\binom{m}{j}\\nX^i H^j\\\\otimes X^{n-1} H^{m-j}\\n\\\\]\\nLet $L$ be a crossed submodule invariant under the action and coaction.\\nThe (repeated) action of $H$ separates elements by degree in $X$. It is\\ntherefore sufficient to consider elements of the form $X^n P(H)$, where\\n$P$ is a polynomial.\\nBy acting with $X$ on an element $X^n P(H)$ we obtain\\n$X^{n+1}(P(H)-P(H+1))$. Subsequently applying the coaction and\\nprojecting on the left hand side of the tensor product onto $X$ (in\\nthe basis $X^i H^j$ of \\\\ensuremath{U(\\\\lalg{b_+})})\\nleads to the element $X^n (P(H)-P(H+1))$. Now the degree of\\n$P(H)-P(H+1)$ is exactly the degree of $P(H)$ minus $1$. Thus we have\\npolynomials $X^n P_i(H)$ of any degree $i=\\\\deg(P_i)\\\\le \\\\deg(P)$ in $L$\\nby induction. In particular, $X^n H^m\\\\in L$ for all\\n$m\\\\le\\\\deg(P)$. It is thus sufficient to consider elements of\\nthe form $X^n H^m$. Given such an element, the coaction generates all\\nelements of the form $X^i H^j$ with $i\\\\le n, j\\\\le m$.\\n\\nFor given $n$, the characterising datum is the maximal $m$ so\\nthat $X^n H^m\\\\in L$. Due to the coaction this cannot decrease\\nwith decreasing $n$ and due to the action of $X$ this can decrease at\\nmost by $1$ when increasing $n$ by $1$. This leads to the\\nclassification given. For $l\\\\in N_0$ and $I\\\\subset\\\\mathbb{N}$ finite, the\\ncorresponding crossed submodule\\nis generated by\\n\\\\begin{gather*}\\nX^{n_m-1} H^{l+m-1}, X^{n_m+n_{m-1}-1} H^{l+m-2},\\\\dots,\\nX^{(\\\\sum_i n_i)-1} H^{l}\\\\\\\\\\n\\\\text{and}\\\\qquad\\nX^{(\\\\sum_i n_i)+k} H^{l-1}\\\\quad \\\\forall k\\\\ge 0\\\\quad\\\\text{if}\\\\quad l>0\\n\\\\end{gather*}\\nas a crossed module.\\n\\\\end{proof}\\n\\nFor the transition from the $q$-deformed (lemma\\n\\\\ref{lem:uqbp_class}) to the classical case we\\nobserve that the space spanned by $g^{s_1},\\\\dots,g^{s_m}$ with $m$\\ndifferent integers $s_i\\\\in\\\\mathbb{Z}$ maps to the space spanned by\\n$1, H, \\\\dots, H^{m-1}$ in the\\nprescription of the classical limit (as described in section\\n\\\\ref{sec:intro_limits}). I.e.\\\\ the classical crossed submodule\\ncharacterised by an integer $l$ and a finite set $I\\\\subset\\\\mathbb{N}$ comes\\nfrom a crossed submodule characterised by this same $I$ and additionally $l$\\nother integers $j\\\\in\\\\mathbb{Z}$ for which $X^k g^{1-j}$ is included. In\\nparticular, we have a one-to-one correspondence in the finite\\ndimensional case.\\n\\nTo formulate the analogue of corollary \\\\ref{cor:uqbp_eclass} for the\\nclassical case is essentially straightforward now. However, as for\\n\\\\ensuremath{C(B_+)}{}, we obtain more crossed submodules than those from the $q$-deformed\\nsetting. This is due to the degeneracy introduced by forgetting the\\npowers of $g$ and just retaining the number of different powers. \\n\\n\\\\begin{cor}\\n\\\\label{cor:ubp_eclass}\\n(a) Proper left crossed \\\\ensuremath{U(\\\\lalg{b_+})}-submodules\\n$L\\\\subset\\\\ker\\\\cou\\\\subset\\\\ensuremath{U(\\\\lalg{b_+})}$ via the\\nleft adjoint\\naction and left regular coaction (with subsequent projection to\\n$\\\\ker\\\\cou$ via $x\\\\mapsto x-\\\\cou(x)1$) are in one-to-one correspondence to\\npairs $(l,I)$ with $l\\\\in\\\\mathbb{N}_0$ and $I\\\\subset\\\\mathbb{N}$ finite where $l\\\\neq 0$\\nor $I\\\\neq\\\\emptyset$.\\n$\\\\dim L<\\\\infty$ iff $l=0$. In particular $\\\\dim\\nL=(\\\\sum_{n\\\\in I}n)-1$ if $l=0$.\\n\\\\end{cor}\\n\\n\\nAs in the $q$-deformed setting, we give a description of the finite\\ndimensional differential calculi where we have a strict duality to\\nquantum tangent spaces.\\n\\n\\\\begin{prop}\\n(a) Finite dimensional differential calculi $\\\\Gamma$ on \\\\ensuremath{C(B_+)}{} and\\nfinite dimensional quantum tangent spaces $L$ on \\\\ensuremath{U(\\\\lalg{b_+})}{} are\\nin one-to-one correspondence to non-empty finite sets $I\\\\subset\\\\mathbb{N}$.\\nIn particular $\\\\dim\\\\Gamma=\\\\dim L=(\\\\sum_{n\\\\in I}) n)-1$.\\n\\nThe $\\\\Gamma$ with $1\\\\in\\\\mathbb{N}$ are in\\none-to-one correspondence to the finite dimensional\\ncalculi and quantum tangent spaces of the $q$-deformed setting\\n(theorem \\\\ref{thm:q_calc}(a)).\\n\\n(b) The differential calculus $\\\\Gamma$ of dimension $n\\\\ge 2$\\ncorresponding to the\\ncoirreducible one of \\\\ensuremath{C_q(B_+)}{} (theorem \\\\ref{thm:q_calc}(b)) has a right\\ninvariant\\nbasis $\\\\eta_0,\\\\dots,\\\\eta_{n-1}$ so that\\n\\\\begin{gather*}\\n\\\\diff X=\\\\eta_1+\\\\eta_0 X \\\\qquad\\n \\\\diff g=\\\\eta_0 g\\\\\\\\\\n[g, \\\\eta_i]=0\\\\ \\\\forall i \\\\qquad\\n[X, \\\\eta_i]=\\\\begin{cases}\\n 0 & \\\\text{if}\\\\ i=0\\\\ \\\\text{or}\\\\ i=n-1\\\\\\\\\\n \\\\eta_{i+1} & \\\\text{if}\\\\ 0<i<n-1\\n \\\\end{cases}\\n\\\\end{gather*}\\nhold. The braided derivations obtained from the dual basis of the\\ncorresponding $L$ are\\ngiven by\\n\\\\begin{gather*}\\n\\\\partial_i f=\\\\frac{1}{i!}\\n \\\\left(\\\\frac{\\\\partial}{\\\\partial X}\\\\right)^i f\\\\qquad\\n \\\\forall i\\\\ge 1\\\\\\\\\\n\\\\partial_0 f=\\\\left(X \\\\frac{\\\\partial}{X}+\\n g \\\\frac{\\\\partial}{g}\\\\right) f\\n\\\\end{gather*}\\nfor $f\\\\in\\\\ensuremath{C(B_+)}$.\\n\\n(c) The differential calculus of dimension $n-1$ \\ncorresponding to the\\none in (b) with $1$ removed from the characterising set is\\nthe same as the one above, except that we set $\\\\eta_0=0$ and\\n$\\\\partial_0=0$.\\n\\\\end{prop}\\n\\\\begin{proof}\\n(a) We observe that the classifications of corollary\\n\\\\ref{cor:cbp_class} and lemma \\\\ref{lem:ubp_class} or\\ncorollary \\\\ref{cor:cbp_eclass} and corollary \\\\ref{cor:ubp_eclass}\\nare dual to each other in the finite (co)dimensional case.\\nMore\\nprecisely, for $I\\\\subset\\\\mathbb{N}$ finite the crossed submodule $M$\\ncorresponding to $(1,I)$ in corollary \\\\ref{cor:cbp_class} is the\\nannihilator of the crossed\\nsubmodule $L$ corresponding to $(0,I)$ in lemma \\\\ref{lem:ubp_class} \\nand vice versa.\\n$\\\\ensuremath{C(B_+)}/M$ and $L$ are dual spaces with the induced pairing.\\nFor non-empty $I$ this descends to \\n$M$ corresponding to $(1,I)$ in corollary\\n\\\\ref{cor:cbp_eclass} and $L$ corresponding to $(0,I)$ in corollary\\n\\\\ref{cor:ubp_eclass}.\\nFor the dimension of $\\\\Gamma$ note\\n$\\\\dim\\\\Gamma=\\\\dim{\\\\ker\\\\cou/M}=\\\\codim M$.\\n\\n(b) For $I=\\\\{1,n\\\\}$ we choose in\\n$\\\\ker\\\\cou\\\\subset\\\\ensuremath{C(B_+)}$ the basis $\\\\eta_0,\\\\dots,\\\\eta_{n-1}$ as the\\nequivalence classes of\\n$g-1,X,\\\\dots,X^{n-1}$. The dual basis in $L$\\nis then $H,X,\\\\dots,\\\\frac{1}{k!}X^k,\\\\dots,\\\\frac{1}{(n-1)!}X^{n-1}$.\\nThis leads to the\\nformulas given.\\n\\n(c) For $I=\\\\{n\\\\}$ we get the same as in (b) except that $\\\\eta_0$ and\\n$\\\\partial_0$ disappear.\\n\\\\end{proof}\\n\\nThe classical commutative calculus is the special case of (b) with\\n$n=2$. It is the only calculus of dimension $2$ with\\n$\\\\diff g\\\\neq 0$. Note that it is not coirreducible.\\n\\n\\n\\n\\n\\\\section{The Dual Classical Limit}\\n\\\\label{sec:dual}\\n\\nWe proceed in this section to the more interesting point of view where\\nwe consider the classical algebras, but with their roles\\ninterchanged. I.e.\\\\ we view \\\\ensuremath{U(\\\\lalg{b_+})}{} as the ``function algebra\\'\\'\\nand \\\\ensuremath{C(B_+)}{} as the ``enveloping algebra\\'\\'. Due to the self-duality of\\n\\\\ensuremath{U_q(\\\\lalg{b_+})}{}, we can again view the differential calculi and quantum tangent\\nspaces as classical limits of the $q$-deformed setting investigated in\\nsection \\\\ref{sec:q}.\\n\\nIn this dual setting the bicovariance constraint for differential\\ncalculi becomes much\\nweaker. In particular, the adjoint action on a classical function\\nalgebra is trivial due to commutativity and the adjoint coaction on a\\nclassical enveloping algebra is trivial due to cocommutativity.\\nIn effect, the correspondence with the\\n$q$-deformed setting is much weaker than in the ordinary case of\\nsection \\\\ref{sec:class}.\\nThere are much more differential\\ncalculi and quantum tangent spaces than in the $q$-deformed setting.\\n\\nWe will not attempt to classify all of them in the following but\\nessentially \\ncontend ourselves with those objects coming from the $q$-deformed setting.\\n\\n\\\\begin{lem}\\n\\\\label{lem:cbp_dual}\\nLeft \\\\ensuremath{C(B_+)}-subcomodules $\\\\subseteq\\\\ensuremath{C(B_+)}$ via the left regular coaction are\\n$\\\\mathbb{Z}$-graded subspaces of \\\\ensuremath{C(B_+)}{} with $|X^n g^m|=n+m$,\\nstable under formal derivation in $X$.\\n\\nBy choosing any ordering in \\\\ensuremath{C_q(B_+)}{}, left crossed submodules via left\\nregular action and adjoint coaction are in one-to-one correspondence\\nto certain subcomodules of \\\\ensuremath{C(B_+)}{} by setting $q=1$. Direct sums\\ncorrespond to direct sums.\\n\\nThis descends to $\\\\ker\\\\cou\\\\subset\\\\ensuremath{C(B_+)}$ by the projection $x\\\\mapsto\\nx-\\\\cou(x) 1$.\\n\\\\end{lem}\\n\\\\begin{proof}\\nThe coproduct on \\\\ensuremath{C(B_+)}{} is\\n\\\\[\\\\cop(X^n g^k)=\\\\sum_{r=0}^{n} \\\\binom{n}{r}\\n X^{n-r} g^{k+r}\\\\otimes X^r g^k\\\\]\\nwhich we view as a left coaction.\\nProjecting on the left hand side of the tensor product onto $g^l$ in a\\nbasis $X^n g^k$, we\\nobserve that coacting on an element\\n$\\\\sum_{n,k} a_{n,k} X^n g^k$ we obtain elements\\n$\\\\sum_n a_{n,l-n} X^n g^{l-n}$ for all $l$.\\nI.e.\\\\ elements of the form\\n$\\\\sum_n b_n X^n g^{l-n}$ lie\\nseparately in a subcomodule and it is\\nsufficient to consider such elements. Writing the coaction\\non such an element as\\n\\\\[\\\\sum_t \\\\frac{1}{t!} X^t g^{l-t}\\\\otimes \\\\sum_n b_n\\n \\\\frac{n!}{(n-t)!} X^{n-t} g^{l-n}\\\\]\\nwe see that the coaction generates all formal derivatives in $X$\\nof this element. This gives us the classification: \\\\ensuremath{C(B_+)}-subcomodules\\n$\\\\subseteq\\\\ensuremath{C(B_+)}$ under the left regular coaction are $\\\\mathbb{Z}$-graded\\nsubspaces with $|X^n g^m|=n+m$, stable under formal derivation in\\n$X$ given by $X^n\\ng^m \\\\mapsto n X^{n-1} g^m$.\\n\\nThe correspondence with the \\\\ensuremath{C_q(B_+)} case follows from\\nthe trivial observation\\nthat the coproduct of \\\\ensuremath{C(B_+)}{} is the same as that of \\\\ensuremath{C_q(B_+)}{} with $q=1$.\\n\\nThe restriction to $\\\\ker\\\\cou$ is straightforward.\\n\\\\end{proof}\\n\\n\\n\\n\\\\begin{lem}\\n\\\\label{lem:ubp_dual}\\nThe process of obtaining the classical limit \\\\ensuremath{U(\\\\lalg{b_+})}{} from \\\\ensuremath{U_q(\\\\lalg{b_+})}{} is\\nwell defined for subspaces and sends crossed \\\\ensuremath{U_q(\\\\lalg{b_+})}-submodules\\n$\\\\subset\\\\ensuremath{U_q(\\\\lalg{b_+})}$ by\\nregular action and adjoint coaction to \\\\ensuremath{U(\\\\lalg{b_+})}-submodules $\\\\subset\\\\ensuremath{U(\\\\lalg{b_+})}$\\nby regular\\naction. This map is injective in the finite codimensional\\ncase. Intersections and codimensions are preserved in this case.\\n\\nThis descends to $\\\\ker\\\\cou$.\\n\\\\end{lem}\\n\\\\begin{proof}\\nTo obtain the classical limit of a left ideal it is enough to\\napply the limiting process (as described in section\\n\\\\ref{sec:intro_limits}) to the\\nmodule generators (We can forget the additional comodule\\nstructure). On the one hand,\\nany element generated by left multiplication with polynomials in\\n$g$ corresponds to some element generated by left multiplication with a\\npolynomial in $H$, that is, there will be no more generators in the\\nclassical setting. On the other hand, left multiplication by a\\npolynomial in $H$ comes\\nfrom left multiplication by the same polynomial in $g-1$, that is,\\nthere will be no fewer generators.\\n\\nThe maximal left crossed \\\\ensuremath{U_q(\\\\lalg{b_+})}-submodule $\\\\subseteq\\\\ensuremath{U_q(\\\\lalg{b_+})}$\\nby left multiplication and adjoint coaction of\\ncodimension $n$ ($n\\\\ge 1$) is generated as a left ideal by\\n$\\\\{1-q^{1-n}g,X^n\\\\}$ (see lemma\\n\\\\ref{lem:cqbp_class}). Applying the limiting process to this\\nleads to the\\nleft ideal of \\\\ensuremath{U(\\\\lalg{b_+})}{} (which is not maximal for $n\\\\neq 1$) generated by\\n$\\\\{H+n-1,X^n\\\\}$ having also codimension $n$.\\n\\nMore generally, the picture given for arbitrary finite codimensional left\\ncrossed modules of \\\\ensuremath{U_q(\\\\lalg{b_+})}{} in terms of generators with respect to\\npolynomials in $g,g^{-1}$ in lemma \\\\ref{lem:cqbp_class} carries over\\nby replacing factors\\n$1-q^{1-n}g$ with factors $H+n-1$ leading to generators with\\nrespect to polynomials in $H$. In particular,\\nintersections go to intersections since the distinctness of\\nthe factors for different $n$ is conserved.\\n\\nThe restriction to $\\\\ker\\\\cou$ is straightforward.\\n\\\\end{proof}\\n\\n\\nWe are now in a position to give a detailed description of the\\ndifferential calculi induced from the $q$-deformed setting by the\\nlimiting process.\\n\\n\\\\begin{prop}\\n(a) Certain finite dimensional\\ndifferential calculi $\\\\Gamma$ on \\\\ensuremath{U(\\\\lalg{b_+})}{} and quantum tangent spaces $L$\\non \\\\ensuremath{C(B_+)}{}\\nare in one-to-one correspondence to finite dimensional differential\\ncalculi on \\\\ensuremath{U_q(\\\\lalg{b_+})}{} and quantum\\ntangent spaces on \\\\ensuremath{C_q(B_+)}{}. Intersections correspond to intersections.\\n\\n(b) In particular,\\n$\\\\Gamma$ and $L$ corresponding to coirreducible differential calculi\\non \\\\ensuremath{U_q(\\\\lalg{b_+})}{} and\\nirreducible quantum tangent spaces on \\\\ensuremath{C_q(B_+)}{} via the limiting process\\nare given as follows:\\n$\\\\Gamma$ has a right invariant basis\\n$\\\\eta_0,\\\\dots,\\\\eta_{n-1}$ so that\\n\\\\begin{gather*}\\n\\\\diff X=\\\\eta_1 \\\\qquad \\\\diff H=(1-n)\\\\eta_0 \\\\\\\\\\n[H, \\\\eta_i]=(1-n+i)\\\\eta_i\\\\quad\\\\forall i\\\\qquad\\n[X, \\\\eta_i]=\\\\begin{cases}\\n \\\\eta_{i+1} & \\\\text{if}\\\\ \\\\ i<n-1\\\\\\\\\\n 0 & \\\\text{if}\\\\ \\\\ i=n-1\\n\\\\end{cases}\\n\\\\end{gather*}\\nholds. The braided derivations corresponding to the dual basis of\\n$L$ are given by\\n\\\\begin{gather*}\\n\\\\partial_i\\\\no{f}=\\\\no{T_{1-n+i,H}\\n \\\\frac{1}{i!}\\\\left(\\\\frac{\\\\partial}{\\\\partial X}\\\\right)^i f}\\n \\\\qquad\\\\forall i\\\\ge 1\\\\\\\\\\n\\\\partial_0\\\\no{f}=\\\\no{T_{1-n,H} f - f}\\n\\\\end{gather*}\\nfor $f\\\\in\\\\k[X,H]$\\nwith the normal ordering $\\\\k[X,H]\\\\to \\\\ensuremath{U(\\\\lalg{b_+})}$ via $H^n X^m\\\\mapsto H^n X^m$.\\n\\\\end{prop}\\n\\\\begin{proof}\\n(a) The strict duality between \\\\ensuremath{C(B_+)}-subcomodules $L\\\\subseteq\\\\ker\\\\cou$\\ngiven by lemma \\\\ref{lem:cbp_dual} and corollary \\\\ref{cor:uqbp_eclass}\\nand \\\\ensuremath{U(\\\\lalg{b_+})}-modules $\\\\ensuremath{U(\\\\lalg{b_+})}/(\\\\k 1+M)$ with $M$ given by lemma\\n\\\\ref{lem:ubp_dual} and\\ncorollary \\\\ref{cor:cqbp_eclass} can be checked explicitly.\\nIt is essentially due to mutual annihilation of factors $H+k$ in\\n\\\\ensuremath{U(\\\\lalg{b_+})}{} with elements $g^k$ in \\\\ensuremath{C(B_+)}{}.\\n\\n(b) $L$ is generated by\\n$\\\\{g^{1-n}-1,Xg^{1-n},\\\\dots,\\nX^{n-1}g^{1-n}\\\\}$ and\\n$M$ is generated by $\\\\{H(H+n-1),X(H+n-1),X^n \\\\}$.\\nThe formulas are obtained by denoting with\\n$\\\\eta_0,\\\\dots,\\\\eta_{n-1}$ the equivalence classes of\\n$H/(1-n),X,\\\\dots,X^{n-1}$ in $\\\\ensuremath{U(\\\\lalg{b_+})}/(\\\\k 1+M)$.\\nThe dual basis of $L$ is then\\n\\\\[g^{1-n}-1,X g^{1-n},\\n\\\\dots,\\\\frac{1}{(n-1)!}X^{n-1}\\ng^{1-n}\\\\]\\n\\\\end{proof}\\n\\n\\nIn contrast to the $q$-deformed setting and to the usual classical\\nsetting the many freedoms in choosing a calculus leave us with many\\n$2$-dimensional calculi. It is not obvious which one we should\\nconsider to be the ``natural\\'\\' one. Let us first look at the\\n$2$-dimensional calculus coming from the $q$-deformed\\nsetting as described in (b). The relations become\\n\\\\begin{gather*}\\n[\\\\diff H, a]=\\\\diff a\\\\qquad [\\\\diff X, a]=0\\\\qquad\\\\forall a\\\\in\\\\ensuremath{U(\\\\lalg{b_+})}\\\\\\\\\\n\\\\diff\\\\no{f} =\\\\diff H \\\\no{\\\\fdiff_{1,H} f} \\n + \\\\diff X \\\\no{\\\\frac{\\\\partial}{\\\\partial X} f}\\n\\\\end{gather*}\\nfor $f\\\\in\\\\k[X,H]$.\\n\\nWe might want to consider calculi which are closer to the classical\\ntheory in the sense that derivatives are not finite differences but\\nusual derivatives. Let us therefore demand\\n\\\\[\\\\diff P(H)=\\\\diff H \\\\frac{\\\\partial}{\\\\partial H} P(H)\\\\qquad\\n\\\\text{and}\\\\qquad\\n\\\\diff P(X)=\\\\diff X \\\\frac{\\\\partial}{\\\\partial X} P(X)\\\\]\\nfor polynomials $P$ and ${\\\\diff X}\\\\neq 0$ and ${\\\\diff H}\\\\neq 0$.\\n\\n\\\\begin{prop}\\n\\\\label{prop:nat_bp}\\nThere is precisely one differential calculus of dimension $2$ meeting\\nthese conditions. It obeys the relations\\n\\\\begin{gather*}\\n[a,\\\\diff H]=0\\\\qquad [X,\\\\diff X]=0\\\\qquad [H,\\\\diff X]=\\\\diff X\\\\\\\\\\n\\\\diff \\\\no{f} =\\\\diff H \\\\no{\\\\frac{\\\\partial}{\\\\partial H} f}\\n +\\\\diff X \\\\no{\\\\frac{\\\\partial}{\\\\partial X} f}\\n\\\\end{gather*}\\nwhere the normal ordering $\\\\k[X,H]\\\\to \\\\ensuremath{U(\\\\lalg{b_+})}$ is given by\\n$X^n H^m\\\\mapsto X^n H^m$.\\n\\\\end{prop}\\n\\\\begin{proof}\\nLet $M$ be the left ideal corresponding to the calculus. It is easy to\\nsee that for a primitive element $a$ the classical derivation condition\\ncorresponds to $a^2\\\\in M$ and $a\\\\notin M$. In our case $X^2,H^2\\\\in\\nM$. If we take the\\nideal generated from these two elements we obtain an ideal of\\n$\\\\ker\\\\cou$ of codimension $3$. Now, it is sufficient without loss of\\ngenerality to add a generator of the form $\\\\alpha H+\\\\beta X+\\\\gamma\\nXH$. $\\\\alpha$ and $\\\\beta$ must then be zero in order not\\nto generate $X$ or $H$ in $M$.\\nI.e.\\\\ $M$ is generated by $H^2,\\nXH, X^2$. The relations stated follow.\\n\\\\end{proof}\\n\\n\\n\\n\\\\section{Remarks on $\\\\kappa$-Minkowski Space and Integration}\\n\\\\label{sec:kappa}\\n\\nThere is a straightforward generalisation of \\\\ensuremath{U(\\\\lalg{b_+})}.\\nLet us define the Lie algebra $\\\\lalg b_{n+}$ as generated by\\n$x_0,\\\\dots, x_{n-1}$ with relations\\n\\\\[ [x_0,x_i]=x_i\\\\qquad [x_i,x_j]=0\\\\qquad\\\\forall i,j\\\\ge 1\\\\]\\nIts enveloping algebra \\\\ensuremath{U(\\\\lalg{b}_{n+})}{} is nothing but (rescaled) $\\\\kappa$-Minkowski\\nspace as introduced in \\\\cite{MaRu}. In this section we make some\\nremarks about its intrinsic geometry.\\n\\nWe have an injective Lie algebra\\nhomomorphism $b_{n+}\\\\to b_+$ given by\\n$x_0\\\\mapsto H$ and $x_i\\\\mapsto X$.\\nThis is an isomorphism for $n=2$. The injective Lie algebra\\nhomomorphism extends to an injective homomorphism of enveloping\\nalgebras $\\\\ensuremath{U(\\\\lalg{b_+})}\\\\to \\\\ensuremath{U(\\\\lalg{b}_{n+})}$ in the obvious way. This gives rise\\nto an injective map from the set of submodules of \\\\ensuremath{U(\\\\lalg{b_+})}{} to the set of\\nsubmodules of \\\\ensuremath{U(\\\\lalg{b}_{n+})}{} by taking the pre-image. In\\nparticular this induces an injective\\nmap from the set of differential calculi on \\\\ensuremath{U(\\\\lalg{b_+})}{} to the set of\\ndifferential calculi on \\\\ensuremath{U(\\\\lalg{b}_{n+})}{} which are invariant under permutations\\nof the $x_i\\\\ i\\\\ge 1$.\\n\\n\\\\begin{cor}\\n\\\\label{cor:nat_bnp}\\nThere is a natural $n$-dimensional differential calculus on \\\\ensuremath{U(\\\\lalg{b}_{n+})}{}\\ninduced from the one considered in proposition\\n\\\\ref{prop:nat_bp}.\\nIt obeys the relations\\n\\\\begin{gather*}\\n[a,\\\\diff x_0]=0\\\\quad\\\\forall a\\\\in \\\\ensuremath{U(\\\\lalg{b}_{n+})}\\\\qquad [x_i,\\\\diff x_j]=0\\n \\\\quad [x_0,\\\\diff x_i]=\\\\diff x_i\\\\qquad\\\\forall i,j\\\\ge 1\\\\\\\\\\n\\\\diff \\\\no{f} =\\\\sum_{\\\\mu=0}^{n-1}\\\\diff x_{\\\\mu}\\n \\\\no{\\\\frac{\\\\partial}{\\\\partial x_{\\\\mu}} f}\\n\\\\end{gather*}\\nwhere the normal ordering is given by\\n\\\\[\\\\k[x_0,\\\\dots,x_{n-1}]\\\\to \\\\ensuremath{U(\\\\lalg{b}_{n+})}\\\\quad\\\\text{via}\\\\quad\\nx_{n-1}^{m_{n-1}}\\\\cdots\\nx_0^{m_0}\\\\mapsto x_{n-1}^{m_{n-1}}\\\\cdots x_0^{m_0}\\\\]\\n\\\\end{cor}\\n\\\\begin{proof}\\nThe calculus is obtained from the ideal generated by\\n\\\\[x_0^2,x_i x_j, x_i x_0\\\\qquad\\\\forall i,j\\\\ge 1\\\\]\\nbeing the pre-image of\\n$X^2,XH,X^2$ in \\\\ensuremath{U(\\\\lalg{b_+})}{}.\\n\\\\end{proof}\\n\\nLet us try to push the analogy with the commutative case further and\\ntake a look at the notion of integration. The natural way to encode\\nthe condition of translation invariance from the classical context\\nin the quantum group context\\nis given by the condition\\n\\\\[(\\\\int\\\\otimes\\\\id)\\\\circ\\\\cop a=1 \\\\int a\\\\qquad\\\\forall a\\\\in A\\\\]\\nwhich defines a right integral on a quantum group $A$\\n\\\\cite{Sweedler}.\\n(Correspondingly, we have the notion of a left integral.)\\nLet us\\nformulate a slightly\\nweaker version of this equation\\nin the context of a Hopf algebra $H$ dually paired with\\n$A$. We write\\n\\\\[\\\\int (h-\\\\cou(h))\\\\triangleright a = 0\\\\qquad \\\\forall h\\\\in H, a\\\\in A\\\\]\\nwhere the action of $H$ on $A$ is the coregular action\\n$h\\\\triangleright a = a_{(1)}\\\\langle a_{(2)}, h\\\\rangle$\\ngiven by the pairing.\\n\\nIn the present context we set $A=\\\\ensuremath{U(\\\\lalg{b}_{n+})}$ and $H=\\\\ensuremath{C(B_{n+})}$. We define the\\nlatter as a generalisation of \\\\ensuremath{C(B_+)}{} with commuting\\ngenerators $g,p_1,\\\\dots,p_{n-1}$ and coproducts\\n\\\\[\\\\cop p_i=p_i\\\\otimes 1+g\\\\otimes p_i\\\\qquad \\\\cop g=g\\\\otimes g\\\\]\\nThis can be identified (upon rescaling) as the momentum sector of the\\nfull $\\\\kappa$-Poincar\\\\\\'e algebra (with $g=e^{p_0}$).\\nThe pairing is the natural extension of (\\\\ref{eq:pair_class}):\\n\\\\[\\\\langle x_{n-1}^{m_{n-1}}\\\\cdots x_1^{m_1} x_0^{k},\\n  p_{n-1}^{r_{n-1}}\\\\cdots p_1^{r_1} g^s\\\\rangle\\n  = \\\\delta_{m_{n-1},r_{n-1}}\\\\cdots\\\\delta_{m_1,r_1} m_{n-1}!\\\\cdots m_1!\\n  s^k\\\\]\\nThe resulting coregular\\naction is conveniently expressed as (see also \\\\cite{MaRu})\\n\\\\[p_i\\\\triangleright\\\\no{f}=\\\\no{\\\\frac{\\\\partial}{\\\\partial x_i} f}\\\\qquad\\n  g\\\\triangleright\\\\no{f}=\\\\no{T_{1,x_0} f}\\\\]\\n  with $f\\\\in\\\\k[x_0,\\\\dots,x_{n-1}]$.\\nDue to cocommutativity, the notions of left and right integral\\ncoincide. The invariance conditions for integration become\\n\\\\[\\\\int \\\\no{\\\\frac{\\\\partial}{\\\\partial x_i} f}=0\\\\quad\\n\\\\forall i\\\\in\\\\{1,\\\\dots,n-1\\\\} \\n\\\\qquad\\\\text{and}\\\\qquad \\\\int \\\\no{\\\\fdiff_{1,x_0} f}=0\\\\]\\nThe condition on the left is familiar and states the invariance under\\ninfinitesimal translations in the $x_i$. The condition on the right states the\\ninvariance under integer translations in $x_0$. However, we should\\nremember that we use a certain algebraic model of \\\\ensuremath{C(B_{n+})}{}. We might add,\\nfor example, a generator $p_0$\\nto \\\\ensuremath{C(B_{n+})}{}\\nthat is dual to $x_0$ and behaves\\nas the ``logarithm\\'\\' of $g$, i.e.\\\\ acts as an infinitesimal\\ntranslation in $x_0$. We then have the condition of infinitesimal\\ntranslation invariance\\n\\\\[\\\\int \\\\no{\\\\frac{\\\\partial}{\\\\partial x_{\\\\mu}} f}=0\\\\]\\nfor all $\\\\mu\\\\in\\\\{0,1,\\\\dots,{n-1}\\\\}$.\\n\\nIn the present purely algebraic context these conditions do not make\\nmuch sense. In fact they would force the integral to be zero on the\\nwhole algebra. This is not surprising, since we are dealing only with\\npolynomial functions which would not be integrable in the classical\\ncase either.\\nIn contrast, if we had for example the algebra of smooth functions\\nin two real variables, the conditions just characterise the usual\\nLesbegue integral (up to normalisation).\\nLet us assume $\\\\k=\\\\mathbb{R}$ and suppose that we have extended the normal\\nordering vector\\nspace isomorphism $\\\\mathbb{R}[x_0,\\\\dots,x_{n-1}]\\\\cong \\\\ensuremath{U(\\\\lalg{b}_{n+})}$ to a vector space\\nisomorphism of some sufficiently large class of functions on $\\\\mathbb{R}^n$ with a\\nsuitable completion $\\\\hat{U}(\\\\lalg{b_{n+}})$ in a functional\\nanalytic framework (embedding \\\\ensuremath{U(\\\\lalg{b}_{n+})}{} in some operator algebra on a\\nHilbert space). It is then natural to define the integration on\\n$\\\\hat{U}(\\\\lalg{b_{n+}})$ by\\n\\\\[\\\\int \\\\no{f}=\\\\int_{\\\\mathbb{R}^n} f\\\\ dx_0\\\\cdots dx_{n-1}\\\\]\\nwhere the right hand side is just the usual Lesbegue integral in $n$\\nreal variables $x_0,\\\\dots,x_{n-1}$. This\\nintegral is unique (up to normalisation) in\\nsatisfying the covariance condition since, as we have seen,\\nthese correspond\\njust to the usual translation invariance in the classical case via normal\\nordering, for which the Lesbegue integral is the unique solution.\\nIt is also the $q\\\\to 1$ limit of the translation invariant integral on\\n\\\\ensuremath{U_q(\\\\lalg{b_+})}{} obtained in \\\\cite{Majid_qreg}.\\n\\nWe see that the natural differential calculus in corollary\\n\\\\ref{cor:nat_bnp} is\\ncompatible with this integration in that the appearing braided\\nderivations are exactly the actions of the translation generators\\n$p_{\\\\mu}$. However, we should stress that this calculus is not\\ncovariant under the full $\\\\kappa$-Poincar\\\\\\'e algebra, since it was\\nshown in \\\\cite{GoKoMa} that in $n=4$ there is no such\\ncalculus of dimension $4$. Our results therefore indicate a new\\nintrinsic approach to $\\\\kappa$-Minkowski space that allows a\\nbicovariant\\ndifferential calculus of dimension $4$ and a unique translation\\ninvariant integral by normal ordering and Lesbegue integration.\\n\\n\\n\\n\\\\section*{Acknowledgements}\\nI would like to thank S.~Majid for proposing this project,\\nand for fruitful discussions during the preparation of this paper.\\n\\n\\n'],\n 'meta': [\"{'timestamp': '2022-03-30T02:27:00', 'yymm': '2203', 'arxiv_id': '2203.15369', 'language': 'en', 'url': 'https://arxiv.org/abs/2203.15369'}\",\n  \"{'timestamp': '1998-07-19T14:33:52', 'yymm': '9807', 'arxiv_id': 'math/9807097', 'language': 'en', 'url': 'https://arxiv.org/abs/math/9807097'}\"]}"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "test_sample = loaded_data_lama['train'][:2]\n",
    "test_sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "with open(\"/data/rozen/home/e0833634/lama/protllama/notebooks/text_sample.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_sample, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"below is the original Llama tokenization handling, load your text sample with pickle first\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 12:40:54.760319: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.llama.tokenization_llama import LlamaTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Internal: /sentencepiece/python/bundled/sentencepiece/src/sentencepiece_processor.cc(848) [model_proto->ParseFromArray(serialized.data(), serialized.size())] ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [76]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m t \u001B[38;5;241m=\u001B[39m \u001B[43mLlamaTokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer_path\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprotein_8k.vocab\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama.py:156\u001B[0m, in \u001B[0;36mLlamaTokenizer.__init__\u001B[0;34m(self, vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces, use_default_system_prompt, spaces_between_special_tokens, legacy, **kwargs)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_eos_token \u001B[38;5;241m=\u001B[39m add_eos_token\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_default_system_prompt \u001B[38;5;241m=\u001B[39m use_default_system_prompt\n\u001B[0;32m--> 156\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msp_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_spm_processor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama.py:166\u001B[0m, in \u001B[0;36mLlamaTokenizer.get_spm_processor\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    164\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m spm\u001B[38;5;241m.\u001B[39mSentencePieceProcessor(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msp_model_kwargs)\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlegacy:  \u001B[38;5;66;03m# no dependency on protobuf\u001B[39;00m\n\u001B[0;32m--> 166\u001B[0m     \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoad\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/sentencepiece/__init__.py:367\u001B[0m, in \u001B[0;36mSentencePieceProcessor.Load\u001B[0;34m(self, model_file, model_proto)\u001B[0m\n\u001B[1;32m    365\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_proto:\n\u001B[1;32m    366\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLoadFromSerializedProto(model_proto)\n\u001B[0;32m--> 367\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoadFromFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_file\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/sentencepiece/__init__.py:171\u001B[0m, in \u001B[0;36mSentencePieceProcessor.LoadFromFile\u001B[0;34m(self, arg)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mLoadFromFile\u001B[39m(\u001B[38;5;28mself\u001B[39m, arg):\n\u001B[0;32m--> 171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_sentencepiece\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSentencePieceProcessor_LoadFromFile\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Internal: /sentencepiece/python/bundled/sentencepiece/src/sentencepiece_processor.cc(848) [model_proto->ParseFromArray(serialized.data(), serialized.size())] "
     ]
    }
   ],
   "source": [
    "t = LlamaTokenizer(vocab_file=tokenizer_path+'protein_8k.vocab')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained('hf-internal-testing/llama-tokenizer')\n",
    "# follows fast chat: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py#L257\n",
    "tokenizer.pad_token = tokenizer.unk_token\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "train_set = loaded_data_lama['train']\n",
    "train_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def tokenize_batch(batch):\n",
    "    texts = [sample['text'] for sample in batch]\n",
    "    data = tokenizer(texts, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=1024)\n",
    "    data['labels'] = data['input_ids'].clone()\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dataloader = DataLoader(train_set, batch_size=2, shuffle=False,\n",
    "                        drop_last=True,\n",
    "                        collate_fn=partial(tokenize_batch))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "with open(\"/data/rozen/home/e0833634/lama/protllama/notebooks/llama_dataloader.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataloader, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "with open(\"/data/rozen/home/e0833634/lama/protllama/notebooks/llama_dataloader.pkl\", \"rb\") as f:\n",
    "    dataloader = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[    1,   320,  2042,  ...,   424, 23460, 29889],\n",
      "        [    1,   320,  2042,  ...,   393,   445, 14581]])\n",
      "torch.Size([2, 1024])\n",
      "attention_mask tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "torch.Size([2, 1024])\n",
      "labels tensor([[    1,   320,  2042,  ...,   424, 23460, 29889],\n",
      "        [    1,   320,  2042,  ...,   393,   445, 14581]])\n",
      "torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for step, batch in enumerate(dataloader):\n",
    "    if step == 1:\n",
    "        break\n",
    "    for k, v in batch.items():\n",
    "        print(k, v)\n",
    "        print(np.shape(v))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}