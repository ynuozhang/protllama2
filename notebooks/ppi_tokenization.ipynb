{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-10-15 17:51:48.266155: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import glob\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import pickle\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from functools import partial\n",
    "from transformers.models.llama.tokenization_llama import LlamaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from pytorch_lightning import seed_everything\n",
    "import random\n",
    "import gc\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "text1 = ['GLTNAFIASAPAREVRYDGVITPANANYRFMGGDKGGSLTVGSHLTGSNMVTIGPMGVVVFTNNNDYTGNTFIMGGGTLQLGSNTAWGSLPN\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFPLSPAQLGIWYAQHLDPQVPITIAQYVDLHGALDVEVLERASIDASHELGSGFLRIVERDGEPLQYV\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFP\\n',\n",
    "        'MDRLDFGGGE\\n']\n",
    "text2 = ['GLTNAFIASAPAREVRYDGVITPANANYRFMGGDKGGSLTVGSHLTGSNMVTIGPMGVVVFTNNNDYTGNTFIMGGGTLQLGSNTAWGSLPN\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFPLSPAQLGIWYAQHLDPQVPITIAQYVDLHGALDVEVLERASIDASHELGSGFLRIVERDGEPLQYV\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFP\\n',\n",
    "        'MDRLDFGGGE\\n']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 854, 1642, 653, 91, 2, 854, 1642, 653, 91], [1, 854, 1642, 653, 91, 2, 854, 1642, 653, 91]]\n"
     ]
    }
   ],
   "source": [
    "def standalone_tokenize_function(batch_text1, batch_text2, max_sequence_length):\n",
    "    global global_tokenizer\n",
    "\n",
    "    batch_output = []\n",
    "\n",
    "    # Assuming batch_text1 and batch_text2 are lists of sequences\n",
    "    for s1, s2 in zip(batch_text1, batch_text2):\n",
    "        protein_1 = global_tokenizer.encode(s1)\n",
    "        protein_2 = global_tokenizer.encode(s2)\n",
    "\n",
    "        # Here, we're not looping over the tokens but instead directly working with tokenized sequences.\n",
    "        if len(protein_1) + len(protein_2) + 2 <= max_sequence_length:\n",
    "            batch_output.append([global_tokenizer.bos_id()] + protein_1 + [global_tokenizer.eos_id()] + protein_2)\n",
    "            batch_output.append([global_tokenizer.bos_id()] + protein_2 + [global_tokenizer.eos_id()] + protein_1)\n",
    "\n",
    "    return batch_output\n",
    "a = standalone_tokenize_function(text1, text2, 10)\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 854, 1642, 653, 91, 2, 854, 1642, 653, 91], [1, 854, 1642, 653, 91, 2, 854, 1642, 653, 91]]\n"
     ]
    }
   ],
   "source": [
    "def standalone_tokenize_function(s1, s2, max_sequence_length):\n",
    "    global global_tokenizer\n",
    "    try:\n",
    "        protein_1 = global_tokenizer.encode(s1)\n",
    "        protein_2 = global_tokenizer.encode(s2)\n",
    "        batch_output = []\n",
    "        for protein1, protein2 in zip(protein_1, protein_2):\n",
    "            if len(protein1) + len(protein2) + 2 > max_sequence_length:\n",
    "                pass\n",
    "            else:\n",
    "                batch_output.append(\n",
    "                    [global_tokenizer.bos_id()] + protein1 + [global_tokenizer.eos_id()] + protein2)\n",
    "                batch_output.append(\n",
    "                    [global_tokenizer.bos_id()] + protein2 + [global_tokenizer.eos_id()] + protein1)\n",
    "        return batch_output\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error during tokenization of string fsl: fel\")\n",
    "\n",
    "a = standalone_tokenize_function(text1, text2, 10)\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "class SingleSequenceDataset(Dataset):\n",
    "    def __init__(self, sequence_strs_1, sequence_strs_2, tokenizer, max_sequence_length):\n",
    "        self.sequence_strs_1 = sequence_strs_1\n",
    "        self.sequence_strs_2 = sequence_strs_2\n",
    "        self.tokenizer = tokenizer\n",
    "        global global_tokenizer\n",
    "        global_tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.tokenized_sequences = standalone_tokenize_function(sequence_strs_1, sequence_strs_2, self.max_sequence_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        returns the number of items in the dataset (number of pairs)\n",
    "        \"\"\"\n",
    "        return 2* len(self.sequence_strs_1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(self.sequence_strs_1, idx)\n",
    "        s1 = self.sequence_strs_1[idx]\n",
    "        s2 = self.sequence_strs_2[idx]\n",
    "        tokenized_pairs = standalone_tokenize_function(s1, s2, self.max_sequence_length)\n",
    "        return tokenized_pairs\n",
    "\n",
    "def dynamic_batching_and_padding_collate_fn(batch, max_tokens_per_batch=512):\n",
    "    batches_grouped_by_token_limit = []\n",
    "    current_batch = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    for tokenized_sequence in sorted(batch, key=len, reverse=True):  # Sorting for efficient batching\n",
    "        if current_token_count + len(tokenized_sequence) > max_tokens_per_batch:\n",
    "            batches_grouped_by_token_limit.append(current_batch)\n",
    "            current_batch = []\n",
    "            current_token_count = 0\n",
    "\n",
    "        current_batch.append(tokenized_sequence)\n",
    "        current_token_count += len(tokenized_sequence)\n",
    "\n",
    "    if current_batch:\n",
    "        batches_grouped_by_token_limit.append(current_batch)\n",
    "\n",
    "    padded_batches = []\n",
    "    for sequences in batches_grouped_by_token_limit:\n",
    "        # Padding each batch to the length of its longest sequence.\n",
    "        max_length = max(len(seq) for seq in sequences)\n",
    "        padded_sequences = [seq + [global_tokenizer.pad_token_id()] * (max_length - len(seq)) for seq in sequences]\n",
    "        padded_batches.append(torch.tensor(padded_sequences))\n",
    "\n",
    "    return padded_batches  # List of tensors, where each tensor is a padded batch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "text1 = ['GLTNAFIASAPAREVRYDGVITPANANYRFMGGDKGGSLTVGSHLTGSNMVTIGPMGVVVFTNNNDYTGNTFIMGGGTLQLGSNTAWGSLPN\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFPLSPAQLGIWYAQHLDPQVPITIAQYVDLHGALDVEVLERASIDASHELGSGFLRIVERDGEPLQYV\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFP\\n',\n",
    "        'MDRLDFGGGE\\n',\n",
    "        'APVPVSGQPVSSE\\n']\n",
    "text2 = ['GLTNAFIASAPAREVRYDGVITPANANYRFMGGDKGGSLTVGSHLTGSNMVTIGPMGVVVFTNNNDYTGNTFIMGGGTLQLGSNTAWGSLPN\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFPLSPAQLGIWYAQHLDPQVPITIAQYVDLHGALDVEVLERASIDASHELGSGFLRIVERDGEPLQYV\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFP\\n',\n",
    "        'MDRLDFGGGE\\n',\n",
    "        'APVPVSGQPVSSE\\n']\n",
    "tokenizer_path = '/data/rozen/home/e0833634/lama/protllama/batch_script/'\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path+\"protein_8k.model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "batch_size = 2  # This is the number of sequences.\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=SingleSequenceDataset(text1, text2, tokenizer, max_sequence_length=10),\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=dynamic_batching_and_padding_collate_fn\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GLTNAFIASAPAREVRYDGVITPANANYRFMGGDKGGSLTVGSHLTGSNMVTIGPMGVVVFTNNNDYTGNTFIMGGGTLQLGSNTAWGSLPN\\n', 'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFPLSPAQLGIWYAQHLDPQVPITIAQYVDLHGALDVEVLERASIDASHELGSGFLRIVERDGEPLQYV\\n', 'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFP\\n', 'MDRLDFGGGE\\n', 'APVPVSGQPVSSE\\n'] [4, 3]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [112]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(batch)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:2807\u001B[0m, in \u001B[0;36mDataset.__getitems__\u001B[0;34m(self, keys)\u001B[0m\n\u001B[1;32m   2805\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitems__\u001B[39m(\u001B[38;5;28mself\u001B[39m, keys: List) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List:\n\u001B[1;32m   2806\u001B[0m     \u001B[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 2807\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getitem__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2808\u001B[0m     n_examples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch[\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(batch))])\n\u001B[1;32m   2809\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [{col: array[i] \u001B[38;5;28;01mfor\u001B[39;00m col, array \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mitems()} \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_examples)]\n",
      "Input \u001B[0;32mIn [109]\u001B[0m, in \u001B[0;36mSingleSequenceDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msequence_strs_1, idx)\n\u001B[0;32m---> 19\u001B[0m     s1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msequence_strs_1\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     20\u001B[0m     s2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msequence_strs_2[idx]\n\u001B[1;32m     21\u001B[0m     tokenized_pairs \u001B[38;5;241m=\u001B[39m standalone_tokenize_function(s1, s2, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_sequence_length)\n",
      "\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def dynamic_batching_and_padding_collate_fn(batch):\n",
    "    pad_token_id = global_tokenizer.pad_token_id()\n",
    "\n",
    "    # Find the length of the longest sequence in the batch\n",
    "    max_length = max(len(seq) for seq in batch)\n",
    "\n",
    "    # Pad each sequence in the batch to max_length\n",
    "    padded_sequences = [seq + [pad_token_id] * (max_length - len(seq)) for seq in batch]\n",
    "    data_tokens_padded = torch.tensor(padded_sequences)\n",
    "\n",
    "    # Create attention masks\n",
    "    attention_masks = (data_tokens_padded != pad_token_id).long()\n",
    "\n",
    "    # Create labels by cloning the input tokens and setting the padding tokens to -100\n",
    "    labels = data_tokens_padded.clone()\n",
    "    labels[data_tokens_padded == pad_token_id] = -100\n",
    "\n",
    "    return {\n",
    "        'input_ids': data_tokens_padded,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': labels\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}