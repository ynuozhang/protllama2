{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from tokenizers import Tokenizer, AddedToken\n",
    "import json\n",
    "from tokenizers.models import BPE\n",
    "import gc\n",
    "import torch\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'4.34.1'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "path = '../../protgpt2/'\n",
    "\n",
    "tokenizer = Tokenizer.from_file(path+'tokenizer.json')\n",
    "special_tokens = [\n",
    "    AddedToken(\"<BOS>\", single_word=True, normalized=True),\n",
    "    AddedToken(\"<EOS>\", single_word=True, normalized=True),\n",
    "    AddedToken(\"<PAD>\", single_word=True, normalized=True),\n",
    "]\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "def load_tokenizer_with_special_tokens(path):\n",
    "    # Load the tokenizer\n",
    "    tokenizer = Tokenizer.from_file(path + 'tokenizer.json')\n",
    "\n",
    "    # Set bos_token_id and eos_token_id to 0\n",
    "    tokenizer.bos_token_id = 0\n",
    "    tokenizer.eos_token_id = 0\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "path = '../../protgpt2/'  # replace with your directory\n",
    "tokenizer = load_tokenizer_with_special_tokens(path)\n",
    "\n",
    "# Confirming\n",
    "assert tokenizer.bos_token_id == 0\n",
    "assert tokenizer.eos_token_id == 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# initialize config for the model\n",
    "from transformers.models.llama.configuration_llama import LlamaConfig\n",
    "MODEL_CONFIGS = {\n",
    "    'protllama2': LlamaConfig(max_position_embeddings=1024, # maximum length\n",
    "                      hidden_size = 1280,\n",
    "                      bos_token_id = tokenizer.bos_token_id,\n",
    "                      eos_token_id = tokenizer.eos_token_id,\n",
    "                      transformers_version = transformers.__version__,\n",
    "                      intermediate_size = 3440,\n",
    "                      vocab_size = 50257),\n",
    "    '7b': LlamaConfig(max_position_embeddings=512,\n",
    "                      hidden_size = 640,\n",
    "                      intermediate_size = 1720,\n",
    "                      _flash_attn_2_enabled=True)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "{'protllama2': LlamaConfig {\n   \"attention_bias\": false,\n   \"bos_token_id\": 0,\n   \"eos_token_id\": 0,\n   \"hidden_act\": \"silu\",\n   \"hidden_size\": 1280,\n   \"initializer_range\": 0.02,\n   \"intermediate_size\": 3440,\n   \"max_position_embeddings\": 1024,\n   \"model_type\": \"llama\",\n   \"num_attention_heads\": 32,\n   \"num_hidden_layers\": 32,\n   \"num_key_value_heads\": 32,\n   \"pretraining_tp\": 1,\n   \"rms_norm_eps\": 1e-06,\n   \"rope_scaling\": null,\n   \"rope_theta\": 10000.0,\n   \"tie_word_embeddings\": false,\n   \"transformers_version\": \"4.34.1\",\n   \"use_cache\": true,\n   \"vocab_size\": 50257\n },\n '7b': LlamaConfig {\n   \"attention_bias\": false,\n   \"bos_token_id\": 1,\n   \"eos_token_id\": 2,\n   \"hidden_act\": \"silu\",\n   \"hidden_size\": 640,\n   \"initializer_range\": 0.02,\n   \"intermediate_size\": 1720,\n   \"max_position_embeddings\": 512,\n   \"model_type\": \"llama\",\n   \"num_attention_heads\": 32,\n   \"num_hidden_layers\": 32,\n   \"num_key_value_heads\": 32,\n   \"pretraining_tp\": 1,\n   \"rms_norm_eps\": 1e-06,\n   \"rope_scaling\": null,\n   \"rope_theta\": 10000.0,\n   \"tie_word_embeddings\": false,\n   \"transformers_version\": \"4.34.1\",\n   \"use_cache\": true,\n   \"vocab_size\": 32000\n }}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIGS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (397744783.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Input \u001B[0;32mIn [14]\u001B[0;36m\u001B[0m\n\u001B[0;31m    if not getattr(MODEL_CONFIGS['7b'], '_flash_attn_2_enabled', False)\u001B[0m\n\u001B[0m                                                                       ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if not getattr(MODEL_CONFIGS['7b'], '_flash_attn_2_enabled', False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from flash_attn import flash_attn_func, flash_attn_varlen_func\n",
    "from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong\n"
     ]
    }
   ],
   "source": [
    "if not getattr(MODEL_CONFIGS['7b'], \"_flash_attn_2_enabled\", False):\n",
    "    # This block is executed if _flash_attn_2_enabled attribute is either missing or set to a falsy value\n",
    "    print('yes')\n",
    "else:\n",
    "    # This block is executed if _flash_attn_2_enabled attribute is set to a truthy value\n",
    "    print('wrong')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 20:38:52.304255: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 640,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1720,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 640)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaFlashAttention2(\n          (q_proj): Linear(in_features=640, out_features=640, bias=False)\n          (k_proj): Linear(in_features=640, out_features=640, bias=False)\n          (v_proj): Linear(in_features=640, out_features=640, bias=False)\n          (o_proj): Linear(in_features=640, out_features=640, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=640, out_features=1720, bias=False)\n          (up_proj): Linear(in_features=640, out_features=1720, bias=False)\n          (down_proj): Linear(in_features=1720, out_features=640, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=640, out_features=32000, bias=False)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "import flash_attn\n",
    "model = LlamaForCausalLM(MODEL_CONFIGS['7b'])\n",
    "print(model.config)  # If this attribute exists and is True, flash_attn is likely being used.\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using flash_attn in one of the decoder layers.\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM(MODEL_CONFIGS['7b'])\n",
    "for layer in model.model.layers:\n",
    "    if 'LlamaFlashAttention2' in str(type(layer.self_attn)):\n",
    "        print(\"Using flash_attn in one of the decoder layers.\")\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if is_flash_attn_2_available():\n",
    "    from flash_attn import flash_attn_func, flash_attn_varlen_func\n",
    "    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LlamaFlashAttention2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m LlamaForCausalLM(MODEL_CONFIGS[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m7b\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(layer\u001B[38;5;241m.\u001B[39mself_attn, \u001B[43mLlamaFlashAttention2\u001B[49m):\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing flash_attn in one of the decoder layers.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'LlamaFlashAttention2' is not defined"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM(MODEL_CONFIGS['7b'])\n",
    "for layer in model.model.layers:\n",
    "    if isinstance(layer.self_attn, LlamaFlashAttention2):\n",
    "        print(\"Using flash_attn in one of the decoder layers.\")\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'meta'],\n        num_rows: 930514\n    })\n})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/data/rozen/home/e0833634/lama/protllama/original_lama.pkl', 'rb') as f:\n",
    "    loaded_data_lama = pickle.load(f)\n",
    "#name = 'togethercomputer/RedPajama-Data-1T-Sample'\n",
    "loaded_data_lama"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_ds = loaded_data_lama['train']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "valid_dict = {'text': [\"Since it was initiated by the Brazil workers' party~\\cite{wainwright2003making} in the 90s, Participatory budgeting (PB)~\\cite{cabannes2004participatory}\"]}\n",
    "valid_dataset = Dataset.from_dict(valid_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 14:55:21.409685: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from colossalai.nn.lr_scheduler import CosineAnnealingWarmupLR\n",
    "from colossalai.nn.optimizer import HybridAdam\n",
    "from torch.optim import Optimizer\n",
    "from typing import Optional, Tuple\n",
    "from transformers.models.llama.tokenization_llama import LlamaTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "from data_utils_modified import prepare_dataloader_modified\n",
    "tokenizer = LlamaTokenizer.from_pretrained('hf-internal-testing/llama-tokenizer')\n",
    "# follows fast chat: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py#L257\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "def tokenize_batch(batch, tokenizer: Optional[LlamaTokenizer] = None, max_length: int = 1024):\n",
    "    texts = [sample['text'] for sample in batch]\n",
    "    data = tokenizer(texts, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=max_length)\n",
    "    data['labels'] = data['input_ids'].clone()\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dataloader = prepare_dataloader_modified(train_ds, batch_size=2, shuffle=False,\n",
    "                                drop_last=True,\n",
    "                                collate_fn=partial(tokenize_batch, tokenizer=tokenizer, max_length=1024))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "valid_dataloader = prepare_dataloader_modified(valid_dataset, batch_size=2, shuffle=False,\n",
    "                                drop_last=False,\n",
    "                                collate_fn=partial(tokenize_batch, tokenizer=tokenizer, max_length=1024))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[   1, 4001,  372,  ...,    0,    0,    0]])\n",
      "attention_mask tensor([[1, 1, 1,  ..., 0, 0, 0]])\n",
      "labels tensor([[   1, 4001,  372,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(valid_dataloader):\n",
    "    for k, v in batch.items():\n",
    "        print(k,v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "start_step = 0\n",
    "sampler_start_idx = 0\n",
    "num_steps_per_epoch = len(dataloader)\n",
    "num_epoch = 4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "465257"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_steps_per_epoch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "torch.manual_seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "config = '7b'\n",
    "config = MODEL_CONFIGS[config]\n",
    "model = LlamaForCausalLM(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 640)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=640, out_features=640, bias=False)\n          (k_proj): Linear(in_features=640, out_features=640, bias=False)\n          (v_proj): Linear(in_features=640, out_features=640, bias=False)\n          (o_proj): Linear(in_features=640, out_features=640, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=640, out_features=1720, bias=False)\n          (up_proj): Linear(in_features=640, out_features=1720, bias=False)\n          (down_proj): Linear(in_features=1720, out_features=640, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=640, out_features=32000, bias=False)\n)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-0.0190, -0.0132,  0.0111,  ...,  0.0009,  0.0142, -0.0255],\n        [-0.0295,  0.0292,  0.0068,  ..., -0.0125,  0.0372, -0.0088],\n        [-0.0024, -0.0122, -0.0183,  ...,  0.0047,  0.0111, -0.0331],\n        ...,\n        [ 0.0007, -0.0253, -0.0171,  ..., -0.0376,  0.0025,  0.0206],\n        [-0.0065,  0.0047, -0.0297,  ...,  0.0141,  0.0164, -0.0147],\n        [-0.0012, -0.0015, -0.0108,  ...,  0.0174, -0.0097, -0.0093]],\n       requires_grad=True)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:2' # do this before importing pytorch\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=100,\n",
    "                                            num_training_steps=num_epoch * len(dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "1861028"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epoch * len(dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "device = torch.device(str(\"cuda:0\") if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number is  0\n",
      "training loss:\n",
      "tensor(10.4766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "training loss:\n",
      "tensor(10.5344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "training loss:\n",
      "tensor(10.4629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "validation loss is 11.01328182220459\n",
      "epoch number is  1\n",
      "training loss:\n",
      "tensor(10.2157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "training loss:\n",
      "tensor(10.1563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "training loss:\n",
      "tensor(10.0343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "validation loss is 10.708047866821289\n",
      "epoch number is  2\n",
      "training loss:\n",
      "tensor(9.7984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "training loss:\n",
      "tensor(9.7844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "training loss:\n",
      "tensor(9.7133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "validation loss is 10.701873779296875\n",
      "epoch number is  3\n",
      "training loss:\n",
      "tensor(9.4875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "training loss:\n",
      "tensor(9.5191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "training loss:\n",
      "tensor(9.4817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "validation loss is 10.892144203186035\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    print('epoch number is ', str(epoch))\n",
    "    model.train()\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        if step == 3:\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        for k, v in batch.items():\n",
    "            batch = {k: v.cuda() for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        print('training loss:')\n",
    "        print(loss)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch_valid in enumerate(valid_dataloader):\n",
    "            if step == 3:\n",
    "                break\n",
    "            for k_, v_ in batch_valid.items():\n",
    "                batch_valid = {k_: v_.cuda() for k_, v_ in batch_valid.items()}\n",
    "            outputs_valid = model(**batch_valid)\n",
    "            loss_valid = outputs_valid[0].item()\n",
    "            print('validation loss is', loss_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "10.730388641357422"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_valid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(9.7198, device='cuda:0', grad_fn=<NllLossBackward0>)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.7465, -1.2179, -0.6294,  ...,  0.5506,  0.2126, -0.6999],\n         [-0.3763, -1.2060, -0.7949,  ...,  0.4821,  0.1066, -0.6537],\n         [-0.4480, -1.2591, -0.6291,  ...,  0.4949,  0.2238, -0.6204],\n         ...,\n         [ 0.0123, -0.9344, -0.5872,  ...,  0.4297,  0.5233, -0.4541],\n         [-0.2127, -0.9061, -0.6173,  ...,  0.3971,  0.4578, -0.4449],\n         [-0.0504, -1.0030, -0.6453,  ...,  0.3347,  0.3408, -0.4290]]],\n       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = outputs.logits\n",
    "logit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1024, 32000])"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(logit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[29906, 29906, 29906,  ...,    13,    13,    13]], device='cuda:0')"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_ids = torch.argmax(logit, dim=-1)\n",
    "print(np.shape(predicted_token_ids))\n",
    "predicted_token_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([29906, 29906, 29906,  ...,    13,    13,    13], device='cuda:0')"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_ids[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222222222222222222222222222222222222222222222\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "22\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_text = tokenizer.decode(predicted_token_ids[0])  # Assuming batch size is 1\n",
    "print(predicted_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit = outputs_valid.logits\n",
    "predicted_token_ids = torch.argmax(logit, dim=-1)\n",
    "predicted_text = tokenizer.decode(predicted_token_ids[0])  # Assuming batch size is 1\n",
    "print(predicted_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "'<s>\\\\section{Introduction}\\n\\\\label{sec:intro}\\n\\n\\\\emph{Gender diversity}, or more often its lack thereof, among participants to\\nsoftware development activities has been thoroughly studied in recent years. In\\nparticular, the presence of, effects of, and countermeasures for \\\\emph{gender\\n  bias} in Free/Open Source Software (FOSS) have received a lot of attention\\nover the past decade~\\\\cite{david2008fossdevs, qiu2010kdewomen,\\n  nafus2012patches, kuechler2012genderfoss, vasilescu2014gender,\\n  oneil2016debiansurvey, robles2016womeninfoss, terrell2017gender,\\n  zacchiroli2021gender}.  \\\\emph{Geographic diversity} is on the other hand the\\nkind of diversity that stems from participants in some global activity coming\\nfrom different world regions and cultures.\\n\\nGeographic diversity in FOSS has received relatively little attention in scholarly\\nworks. In particular, while seminal survey-based and\\npoint-in-time medium-scale studies of the geographic origins of FOSS\\ncontributors exist~\\\\cite{ghosh2005understanding, david2008fossdevs,\\n  barahona2008geodiversity, takhteyev2010ossgeography, robles2014surveydataset,\\n  wachs2021ossgeography}, large-scale longitudinal studies of the geographic\\norigin of FOSS contributors are still lacking. Such a quantitative\\ncharacterization would be useful to inform decisions related to global\\ndevelopment teams~\\\\cite{herbsleb2007globalsweng} and hiring strategies in the\\ninformation technology (IT) market, as well as contribute factual information\\nto the debates on the economic impact and sociology of FOSS around the world.\\n\\n\\n\\\\paragraph{Contributions}\\n\\nWith this work we contribute to close this gap by conducting \\\\textbf{the first\\n  longitudinal study of the geographic origin of contributors to public code\\n  over 50 years.} Specifically, we provide a preliminary answer to the\\nfollowing research question:\\n\\\\begin{researchquestion}\\n  From which world regions do authors of publicly available commits come from\\n  and how has it changed over the past 50 years?\\n  \\\\label{rq:geodiversity}\\n\\\\end{researchquestion}\\nWe use as dataset the \\\\SWH/ archive~\\\\cite{swhipres2017} and analyze from it\\n2.2 billion\\\\xspace commits archived from 160 million\\\\xspace projects and authored by\\n43 million\\\\xspace authors during the 1971--2021 time period. \\nWe geolocate developers to\\n\\\\DATAWorldRegions/ world regions, using as signals email country code top-level domains (ccTLDs) and \\nauthor (first/last) names compared with name distributions around the world, and UTC offsets \\nmined from commit metadata.\\n\\nWe find evidence of the early dominance of North America in open source\\nsoftware, later joined by Europe. After that period, the geographic diversity \\nin public code has been constantly increasing.\\nWe also identify relevant historical shifts\\nrelated to the end of the UNIX wars and the increase of coding literacy in\\nCentral and South Asia, as well as of broader phenomena like colonialism and\\npeople movement across countries (immigration/emigration).\\n\\n\\n\\n\\n\\\\paragraph{Data availability.}\\n\\nA replication package for this paper is available from Zenodo at\\n\\\\url{https://doi.org/10.5281/zenodo.6390355}~\\\\cite{replication-package}.\\n\\n\\n \\\\section{Related Work}\\n\\\\label{sec:related}\\n\\nBoth early and recent works~\\\\cite{ghosh2005understanding, david2008fossdevs,\\n  robles2014surveydataset, oneil2016debiansurvey} have characterized the\\ngeography of Free/Open Source Software (FOSS) using \\\\emph{developer surveys},\\nwhich provide high-quality answers but are limited in size (2-5\\\\,K developers)\\nand can be biased by participant sampling.'"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch['input_ids'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[   1, 4001,  372,  ...,    0,    0,    0]], device='cuda:0')"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[    1,   320,  2042,  ...,   424, 23460, 29889]], device='cuda:0'),\n 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'),\n 'labels': tensor([[    1,   320,  2042,  ...,   424, 23460, 29889]], device='cuda:0')}"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/930514 [00:00<?, ?it/s]\n",
      "Epoch 0: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(start_epoch, num_epoch):\n",
    "    model.train()\n",
    "    with tqdm(enumerate(dataloader),\n",
    "              desc=f'Epoch {epoch}',\n",
    "              total=num_steps_per_epoch,\n",
    "              initial=start_step) as pbar:\n",
    "        for step, batch in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            for k, v in batch.items():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "                break\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    with tqdm(enumerate(valid_dataloader),\n",
    "              desc=f'Epoch {epoch}',\n",
    "              initial=start_step) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for step, batch_valid in enumerate(valid_dataloader):\n",
    "                for k_, v_ in batch_valid.items():\n",
    "                    batch_valid = {k_: v_.cuda() for k_, v_ in batch_valid.items()}\n",
    "                outputs_valid = model(**batch_valid)\n",
    "                loss_valid = outputs_valid[0].item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(10.5123, device='cuda:0', grad_fn=<NllLossBackward0>)"
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "data": {
      "text/plain": "11.107804298400879"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_valid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/465257 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "value cannot be converted to type at::Half without overflow",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [21]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m#batch = {k: v.cuda() for k, v in batch.items()}\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m     loss \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     18\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:820\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    817\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m    819\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m--> 820\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    821\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    822\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    823\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    824\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    826\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    828\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    829\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    830\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    832\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    833\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpretraining_tp \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:668\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    664\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    665\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones(\n\u001B[1;32m    666\u001B[0m         (batch_size, seq_length_with_past), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mbool, device\u001B[38;5;241m=\u001B[39minputs_embeds\u001B[38;5;241m.\u001B[39mdevice\n\u001B[1;32m    667\u001B[0m     )\n\u001B[0;32m--> 668\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_decoder_attention_mask\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_length\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    672\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m inputs_embeds\n\u001B[1;32m    674\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_checkpointing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:596\u001B[0m, in \u001B[0;36mLlamaModel._prepare_decoder_attention_mask\u001B[0;34m(self, attention_mask, input_shape, inputs_embeds, past_key_values_length)\u001B[0m\n\u001B[1;32m    594\u001B[0m combined_attention_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m input_shape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 596\u001B[0m     combined_attention_mask \u001B[38;5;241m=\u001B[39m \u001B[43m_make_causal_mask\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    597\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_shape\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    598\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    599\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    600\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    601\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    604\u001B[0m     \u001B[38;5;66;03m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001B[39;00m\n\u001B[1;32m    605\u001B[0m     expanded_attn_mask \u001B[38;5;241m=\u001B[39m _expand_mask(attention_mask, inputs_embeds\u001B[38;5;241m.\u001B[39mdtype, tgt_len\u001B[38;5;241m=\u001B[39minput_shape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\u001B[38;5;241m.\u001B[39mto(\n\u001B[1;32m    606\u001B[0m         inputs_embeds\u001B[38;5;241m.\u001B[39mdevice\n\u001B[1;32m    607\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:50\u001B[0m, in \u001B[0;36m_make_causal_mask\u001B[0;34m(input_ids_shape, dtype, device, past_key_values_length)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;124;03mMake causal mask used for bi-directional self-attention.\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     49\u001B[0m bsz, tgt_len \u001B[38;5;241m=\u001B[39m input_ids_shape\n\u001B[0;32m---> 50\u001B[0m mask \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfull\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgt_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_len\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m mask_cond \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39marange(mask\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     52\u001B[0m mask\u001B[38;5;241m.\u001B[39mmasked_fill_(mask_cond \u001B[38;5;241m<\u001B[39m (mask_cond \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mview(mask\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), \u001B[38;5;241m1\u001B[39m), \u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: value cannot be converted to type at::Half without overflow"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "for epoch in range(start_epoch, num_epoch):\n",
    "    model.train()\n",
    "    with tqdm(enumerate(dataloader),\n",
    "              desc=f'Epoch {epoch}',\n",
    "              total=num_steps_per_epoch,\n",
    "              initial=start_step) as pbar:\n",
    "        for step, batch in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(): # handle mixed precisions\n",
    "                for k, v in batch.items():\n",
    "                    batch = {k: v.cuda()}\n",
    "                    break\n",
    "                #batch = {k: v.cuda() for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs[0]\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # gradient clipping to prevent overflows\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [24]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43moutputs\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1024])"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "text1 = loaded_data_lama['train']['text'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\\\section{Introduction}\\n\\\\label{sec:intro}\\n\\n\\\\emph{Gender diversity}, or more often its lack thereof, among participants to\\nsoftware development activities has been thoroughly studied in recent years. In\\nparticular, the presence of, effects of, and countermeasures for \\\\emph{gender\\n  bias} in Free/Open Source Software (FOSS) have received a lot of attention\\nover the past decade~\\\\cite{david2008fossdevs, qiu2010kdewomen,\\n  nafus2012patches, kuechler2012genderfoss, vasilescu2014gender,\\n  oneil2016debiansurvey, robles2016womeninfoss, terrell2017gender,\\n  zacchiroli2021gender}.  \\\\emph{Geographic diversity} is on the other hand the\\nkind of diversity that stems from participants in some global activity coming\\nfrom different world regions and cultures.\\n\\nGeographic diversity in FOSS has received relatively little attention in scholarly\\nworks. In particular, while seminal survey-based and\\npoint-in-time medium-scale studies of the geographic origins of FOSS\\ncontributors exist~\\\\cite{ghosh2005understanding, david2008fossdevs,\\n  barahona2008geodiversity, takhteyev2010ossgeography, robles2014surveydataset,\\n  wachs2021ossgeography}, large-scale longitudinal studies of the geographic\\norigin of FOSS contributors are still lacking. Such a quantitative\\ncharacterization would be useful to inform decisions related to global\\ndevelopment teams~\\\\cite{herbsleb2007globalsweng} and hiring strategies in the\\ninformation technology (IT) market, as well as contribute factual information\\nto the debates on the economic impact and sociology of FOSS around the world.\\n\\n\\n\\\\paragraph{Contributions}\\n\\nWith this work we contribute to close this gap by conducting \\\\textbf{the first\\n  longitudinal study of the geographic origin of contributors to public code\\n  over 50 years.} Specifically, we provide a preliminary answer to the\\nfollowing research question:\\n\\\\begin{researchquestion}\\n  From which world regions do authors of publicly available commits come from\\n  and how has it changed over the past 50 years?\\n  \\\\label{rq:geodiversity}\\n\\\\end{researchquestion}\\nWe use as dataset the \\\\SWH/ archive~\\\\cite{swhipres2017} and analyze from it\\n2.2 billion\\\\xspace commits archived from 160 million\\\\xspace projects and authored by\\n43 million\\\\xspace authors during the 1971--2021 time period. \\nWe geolocate developers to\\n\\\\DATAWorldRegions/ world regions, using as signals email country code top-level domains (ccTLDs) and \\nauthor (first/last) names compared with name distributions around the world, and UTC offsets \\nmined from commit metadata.\\n\\nWe find evidence of the early dominance of North America in open source\\nsoftware, later joined by Europe. After that period, the geographic diversity \\nin public code has been constantly increasing.\\nWe also identify relevant historical shifts\\nrelated to the end of the UNIX wars and the increase of coding literacy in\\nCentral and South Asia, as well as of broader phenomena like colonialism and\\npeople movement across countries (immigration/emigration).\\n\\n\\n\\n\\n\\\\paragraph{Data availability.}\\n\\nA replication package for this paper is available from Zenodo at\\n\\\\url{https://doi.org/10.5281/zenodo.6390355}~\\\\cite{replication-package}.\\n\\n\\n \\\\section{Related Work}\\n\\\\label{sec:related}\\n\\nBoth early and recent works~\\\\cite{ghosh2005understanding, david2008fossdevs,\\n  robles2014surveydataset, oneil2016debiansurvey} have characterized the\\ngeography of Free/Open Source Software (FOSS) using \\\\emph{developer surveys},\\nwhich provide high-quality answers but are limited in size (2-5\\\\,K developers)\\nand can be biased by participant sampling.\\n\\nIn 2008 Barahona et al.~\\\\cite{barahona2008geodiversity} conducted a seminal\\nlarge-scale (for the time) study on FOSS \\\\emph{geography using mining software\\n  repositories (MSR) techniques}. They analyzed the origin of 1\\\\,M contributors\\nusing the SourceForge user database and mailing list archives over the\\n1999--2005 period, using as signals information similar to ours: email domains\\nand UTC offsets. \\nThe studied period (7 years) in~\\\\cite{barahona2008geodiversity} is shorter than \\nwhat is studied in the present paper (50 years) and the data sources are \\nlargely different; with that in mind, our results show a slightly larger quote of \\nEuropean v.~North American contributions.\\n\\nAnother empirical work from 2010 by Takhteyev and\\nHilts~\\\\cite{takhteyev2010ossgeography} harvested self-declared geographic\\nlocations of GitHub accounts recursively following their connections,\\ncollecting information for $\\\\approx$\\\\,70\\\\,K GitHub users.  A very recent\\nwork~\\\\cite{wachs2021ossgeography} by Wachs et al.~has geolocated half a million\\nGitHub users, having contributed at least 100 commits each, and who\\nself-declare locations on their GitHub profiles. While the study is\\npoint-in-time as of 2021, the authors compare their findings\\nagainst~\\\\cite{barahona2008geodiversity, takhteyev2010ossgeography} to\\ncharacterize the evolution of FOSS geography over the time snapshots taken by\\nthe three studies.\\n\\nCompared with previous empirical works, our study is much larger scale---having\\nanalyzed 43 million\\\\xspace authors of 2.2 billion\\\\xspace commits from 160 million\\\\xspace\\nprojects---longitudinal over 50 years of public code contributions rather than\\npoint in time, and also more fine-grained (with year-by-year granularity over\\nthe observed period). Methodologically, our study relies on Version Control\\nSystem (VCS) commit data rather than platform-declared location information.\\n\\n\\nOther works---in particular the work by Daniel~\\\\cite{daniel2013ossdiversity}\\nand, more recently, Rastogi et al.~\\\\cite{rastogi2016geobias,\\n  rastogi2018geobias, prana2021geogenderdiversity}---have studied geographic\\n\\\\emph{diversity and bias}, i.e., the extent to which the origin of FOSS\\ndevelopers affect their collaborative coding activities.\\nIn this work we characterized geographic diversity in public code for the first\\ntime at this scale, both in terms of contributors and observation period. We do\\nnot tackle the bias angle, but provide empirical data and findings that can be\\nleveraged to that end as future work.\\n\\n\\\\emph{Global software engineering}~\\\\cite{herbsleb2007globalsweng} is the\\nsub-field of software engineering that has analyzed the challenges of scaling\\ndeveloper collaboration globally, including the specific concern of how to deal\\nwith geographic diversity~\\\\cite{holmstrom2006globaldev, fraser2014eastwest}.\\nDecades later the present study provides evidence that can be used, in the\\nspecific case of public code and at a very large scale, to verify which\\npromises of global software engineering have borne fruit.\\n\\n\\n\\n\\n\\n\\n \\\\section{Methodology}\\n\\\\label{sec:method}\\n\\n\\n\\\\newif\\\\ifgrowthfig  \\\\growthfigtrue\\n\\\\ifgrowthfig\\n\\\\begin{figure}\\n  \\\\includegraphics[width=\\\\columnwidth]{yearly-commits}\\n  \\\\caption{Yearly public commits over time (log scale).\\n}\\n  \\\\label{fig:growth}\\n\\\\end{figure}\\n\\\\fi\\n\\n\\\\paragraph{Dataset}\\n\\nWe retrieved from \\\\SWH/~\\\\cite{swh-msr2019-dataset} all commits archived until \\\\DATALastCommitDate/.\\nThey amount to \\\\DATACommitsRaw/ commits, unique by SHA1 identifier, harvested from \\\\DATATotalCommitsInSH/ public projects coming from major development forges (GitHub, GitLab, etc.) and package repositories (Debian, PyPI, NPM, etc.).\\nCommits in the dataset are by \\\\DATAAuthorsRaw/ authors, unique by $\\\\langle$name, email$\\\\rangle$ pairs.\\nThe dataset came as two relational tables, one for commits and one for authors, with the former referencing the latter via a foreign key.\\n\\\\iflong\\nEach row in the commit table contains the following fields: commit SHA1 identifier, author and committer timestamps, author and committer identifiers (referencing the author table).\\nThe distinction between commit authors and committers come from Git, which allows to commit a change authored by someone else.\\nFor this study we focused on authors and ignored committers, as the difference between the two is not relevant for our research questions and the amount of commits with a committer other than its author is negligible.\\n\\\\fi\\nFor each entry in the author table we have author full name and email as two separate strings of raw bytes.\\n\\nWe removed implausible or unusable names that: are not decodable as UTF-8 (\\\\DATAAuthorsRmNondecodable/ author names removed), are email addresses instead of names (\\\\DATAAuthorsRmEmail/ ``names''), consist of only blank characters (\\\\DATAAuthorsRmBlank/), contain more than 10\\\\% non-letters (\\\\DATAAuthorsRmNonletter/), are longer than 100 characters (\\\\DATAAuthorsRmToolong/).\\nAfter filtering, about \\\\DATAAuthorsPlausibleApprox/ authors (\\\\DATAAuthorsPlausiblePct/ of the initial dataset) remained for further analysis.\\n\\nNote that the amount of public code commits (and authors) contained in the\\ninitial dataset grows exponentially over\\ntime~\\\\cite{swh-provenance-emse}\\\\ifgrowthfig, as shown for commits in\\n\\\\Cref{fig:growth}\\\\else: from $10^4$ commits in 1971, to $10^6$ in 1998, to\\nalmost $10^9$ in 2020\\\\fi. As a consequence the observed trends tend to be more\\nstable in recent decades than in 40+ year-old ones, due to statistics taken on\\nexponentially larger populations.\\n\\n\\n\\\\paragraph{Geolocation}\\n\\n\\\\begin{figure}\\n  \\\\centering\\n  \\\\includegraphics[clip,trim=6cm 6cm 0 0,width=\\\\linewidth]{subregions-ours}\\n  \\\\caption{The \\\\DATAWorldRegions/ world regions used as geolocation targets.}\\n  \\\\label{fig:worldmap}\\n\\\\end{figure}\\n\\nAs geolocation targets we use macro world regions derived from the United Nations geoscheme~\\\\cite{un1999geoscheme}.\\nTo avoid domination by large countries (e.g., China or Russia) within macro regions, we merged and split some regions based on geographic proximity and the sharing of preeminent cultural identification features, such as spoken language.\\n\\\\Cref{fig:worldmap} shows the final list of \\\\DATAWorldRegions/ world regions used as geolocation targets in this study.\\n\\nGeolocation of commit authors to world regions uses the two complementary techniques introduced in~\\\\cite{icse-seis-2022-gender}, briefly recalled below.\\nThe first one relies on the country code top-level domain (ccTLD) of email addresses extracted from commit metadata, e.g., \\\\texttt{.fr}, \\\\texttt{.ru}, \\\\texttt{.cn}, etc.\\nWe started from the IANA list of Latin character ccTLDs~\\\\cite{wikipedia-cctld} and manually mapped each corresponding territory to a target world region.\\n\\nThe second geolocation technique uses the UTC offset of commit timestamps (e.g., UTC-05:00) and author names to determine the most likely world region of the commit author.\\nFor each UTC offset we determine a list of compatible places (country, state, or dependent territory) in the world that, at the time of that commit, had that UTC offset; commit time is key here, as country UTC offsets vary over time due to timezone changes.\\nTo make this determination we use the IANA time zone database~\\\\cite{tzdata}.\\n\\nThen we assign to each place a score that captures the likelihood that a given author name is characteristic of it.\\nTo this end we use the Forebears dataset of the frequencies of the most common first and family names which, quoting from~\\\\cite{forebear-names}: {\\\\itshape ``provides the approximate incidence of forenames and surnames produced from a database of \\\\num{4 044 546 938} people (55.5\\\\% of living people in 2014). As of September 2019 it covers \\\\num{27 662 801} forenames and \\\\num{27 206 821} surnames in 236 jurisdictions.''}\\nAs in our dataset authors are full name strings (rather than split by first/family name), we first tokenize names (by blanks and case changes) and then lookup individual tokens in both first and family names frequency lists.\\nFor each element found in name lists we multiply the place population\\\\footnotemark{} by the name frequency to obtain a measure that is proportional to the number of persons bearing that name (token) in the specific place.\\n\\\\footnotetext{To obtain population totals---as the notion of ``place'' is heterogeneous: full countries v.~slices of large countries spanning multiple timezones---we use a mixture of primary sources (e.g., government websites), and non-primary ones (e.g., Wikipedia articles).}\\nWe sum this figure for all elements to obtain a place score, ending up with a list of $\\\\langle$place, score$\\\\rangle$ pairs.\\nWe then partition this list by the world region that a place belongs to and sum the score for all the places in each region to obtain an overall score, corresponding to the likelihood that the commit belongs to a given world region.\\nWe assign the starting commit as coming from the world region with the highest score.\\n\\nThe email-based technique suffers from the limited and unbalanced use of ccTLDs: most developers use generic TLDs such as \\\\texttt{.com}, \\\\texttt{.org}, or \\\\texttt{.net}.\\nMoreover this does not happen uniformly across zones: US-based developers, for example, use the \\\\texttt{.us} ccTLD much more seldomly than their European counterparts.\\nOn the other hand the offset/name-based technique relies on the UTC offset of the commit timestamps.\\nDue to tool configurations on developer setups, a large number of commits in the dataset has an UTC offset equal to zero.\\nThis affects less recent commits (\\\\DATACommitsTZZTwoThousandTwenty/ of 2020s commits have a zero offset) than older ones (\\\\DATACommitsTZZTwoThousand/ in 2000).\\nAs a result the offset/name-based technique could end up detecting a large share of older commits as authored by African developers, and to a lesser extent Europeans.\\n\\nTo counter these issues we combine the two geolocation techniques together by applying the offset/name-based techniques to all commits with a non-zero UTC offset, and the email-based on to all other commits.\\n\\n\\n \\\\section{Results and Discussion}\\n\\\\label{sec:results}\\n\\n\\\\begin{figure*}\\n  \\\\centering\\n  \\\\includegraphics[width=\\\\linewidth]{stacked.pdf}\\n  \\\\caption{Ratio of commits (above) and active authors (below) by world zone over the 1971--2020 period.}\\n  \\\\Description[Chart]{Stacked bar chart showing the world zone ratios for commits and authors over the 1971--2020 period.}\\n  \\\\label{fig:results}\\n\\\\end{figure*}\\n\\n\\n \\nTo answer \\\\cref{rq:geodiversity} we gathered the number of commits and distinct authors per year and per world zone.\\nWe present the obtained results in \\\\Cref{fig:results} as two stacked bar charts, showing yearly breakdowns for commits and authors respectively.\\nEvery bar represents a year and is partitioned in slices showing the commit/author ratio for each of the world regions of \\\\Cref{fig:worldmap} in that year.\\nTo avoid outliers due to sporadic contributors, in the author chart we only consider authors having contributed at least 5 commits in a given year.\\n\\nWhile observing trends in the charts remember that the total numbers of commits and authors grow exponentially over time.\\nHence for the first years in the charts, the number of data points in some world regions can be extremely small, with negative consequences on the stability of trends.\\n\\n\\n\\n\\n\\\\paragraph{Geographic diversity over time}\\n\\nOverall, the general trend appears to be that the \\\\textbf{geographic diversity in public code is increasing}: North America and Europe alternated their ``dominance'' until the middle of the 90s; from that moment on most other world regions show a slow but steady increment.\\nThis trend of increased participation into public code development includes Central and South Asia (comprising India), Russia, Africa, Central and South America,\\nNotice that also zones that do not seem to follow this trend, such as Australia and New Zealand, are also increasing their participation, but at a lower speed with respect to other zones.\\nFor example, Australia and New Zealand incremented the absolute number of their commits by about 3 orders of magnitude from 2000 to present days.\\n\\nAnother interesting phenomenon that can be appreciated in both charts is the sudden contraction of contributions from North America in 1995; since the charts depict ratios, this corresponds to other zones, and Europe in particular, increasing their share.\\nAn analysis of the main contributions in the years right before the contraction shows that nine out of ten have \\\\texttt{ucbvax.Berkeley.EDU} as author email domain, and the tenth is Keith Bostic, one of the leading Unix BSD developers, appearing with email \\\\texttt{bostic}.\\nNo developer with the same email domain appears anymore within the first hundred contributors in 1996.\\nThis shows the relevance that BSD Unix and the Computer Systems Research Group at the University of California at Berkeley had in the history of open source software.\\nThe group was disbanded in 1995, partially as a consequence of the so-called UNIX wars~\\\\cite{kernighan2019unixhistory}, and this contributes significantly---also because of the relatively low amount of public code circulating at the time---to the sudden drop of contributions from North America in subsequent years.\\nDescendant UNIX operating systems based on BSD, such as OpenBSD, FreeBSD, and NetBSD had smaller relevance to world trends due to (i) the increasing amount of open source code coming from elsewhere and (ii) their more geographically diverse developer community.\\n\\nAnother time frame in which the ratios for Europe and North America are subject to large, sudden changes is 1975--79.\\nA preliminary analysis shows that these ratios are erratic due to the very limited number of commits in those time period, but we were unable to detect a specific root cause.\\nTrends for those years should be subject to further studies, in collaboration with software historians.\\n\\n\\n\\\\paragraph{Colonialism}\\n\\nAnother trend that stands out from the charts is that Africa appears to be well represented.\\nTo assess if this results from a methodological bias, we double-checked the commits detected as originating from Africa for timezones included in the $[0, 3]$ range using both the email- the offset/name-based methods.\\nThe results show that the offset/name-based approach assigns 22.7\\\\% of the commits to Africa whereas the email-based one only assigns 2.7\\\\% of them.\\nWhile a deeper investigation is in order, it is our opinion that the phenomenon we are witnessing here is a consequence of colonialism, specifically the adoption of Europeans names in African countries.\\nFor example the name Eric, derived from Old Norse, is more popular in Ghana than it is in France or in the UK.\\nThis challenges the ability of the offset/name-based method to correctly differentiate between candidate places.\\nTogether with the fact that several African countries are largely populated, the offset/name-based method could detect European names as originating from Africa.\\nWhile this cuts both way, the likelihood of a random person contributing to public code is very different between European countries, all having a well-developed software industry, and African countries that do not all share this trait.\\n\\n\\n\\\\paragraph{Immigration/emigration}\\n\\nAnother area where a similar phenomenon could be at play is the evolution of Central and South America.\\nContribution from this macro region appears to be growing steadily.\\nTo assess if this is the result of a bias introduced by the name-based detection we analyzed the evolution of offset/name-based assignment over time for authors whose email domain is among the top-ten US-based entities in terms of overall contributions (estimated in turn by analyzing the most frequent email domains and manually selecting those belonging to US-based entities).\\nIn 1971 no author with an email from top US-based entities is detected as belonging to Central and South America, whereas in 2019 the ratio is 12\\\\%.\\nNowadays more than one tenth of the people email-associated to top US-based entities have popular Central and South American names, which we posit as a likely consequence of immigration into US (emigration from Central and South America).\\nSince immigration has a much longer history than what we are studying here, what we are witnessing probably includes long-term consequences of it, such as second and third generation immigrants employed in white-collar jobs, such as software development.\\n\\n\\n\\n\\n \\\\section{Limitations and Future Work}\\n\\\\label{sec:conclusion}\\n\\nWe have performed an exploratory, yet very large scale, empirical study of the geographic diversity in public code commits over time.\\nWe have analyzed 2.2 billion\\\\xspace public commits covering the \\\\DATAYearRange/ time period.\\nWe have geolocated developers to \\\\DATAWorldRegions/ world regions using as signals email domains, timezone offsets, and author names.\\nOur findings show that the geographic diversity in public code is increasing over time, and markedly so over the past 20--25 years.\\nObserved trends also co-occur with historical events and macro phenomena like the end of the UNIX wars, increase of coding literacy around the world, colonialism, and immigration.\\n\\n\\n\\\\medskip\\n\\\\emph{Limitations.}\\nThis study relies on a combination of two geolocation methods: one based on email domains, another based on commit UTC offsets and author names.\\nWe discussed some of the limitations of either method in \\\\Cref{sec:method}, motivating our decision of restricting the use of the email-based method to commits with a zero UTC offset.\\nAs a consequence, for most commits in the dataset the offset/name-based method is used.\\nWith such method, the frequencies of forenames and surnames are used to rank candidate zones that have a compatible UTC offset at commit time.\\n\\nA practical consequence of this is that for commits with, say, offset UTC+09:00 the candidate places can be Russia, Japan and Australia, depending on the specific date due to daylight saving time.\\nPopular forenames and surnames in these regions tend to be quite different so the likelihood of the method to provide a reliable detection is high.\\nFor other offsets the set of popular forenames and surnames from candidate zones can exhibit more substantial overlaps, negatively impacting detection accuracy.\\nWe have discussed some of these cases in \\\\Cref{sec:results}, but other might be lingering in the results impacting observed trends.\\n\\nThe choice of using the email-based method for commits with zero UTC offset, and the offset/name-based method elsewhere, has allowed us to study all developers not having a country-specific email domain (ccTLD), but comes with the risk of under-representing the world zones that have (in part and in some times of the year) an actual UTC offset of zero.\\n\\nA potential bias in this study could be introduced by the fact that the name database used for offset/name-based geolocation only contains names formed using Latin alphabet characters.\\nWe looked for names containing Chinese, Japanese, and Korean characters in the original dataset, finding only a negligible amount of authors who use non-Latin characters in their VCS names, which leads us to believe that the impact of this issue is minimal.\\n\\nWe did not apply identity merging (e.g., using state-of-the-art tools like SortingHat~\\\\cite{moreno2019sortinghat}), but we do not expect this to be a significant issue because: (a) to introduce bias in author trends the distribution of identity merges around the world should be uneven, which seems unlikely; and (b) the observed commit trends (which would be unaffected by identity merging) are very similar to observed author trends.\\n\\nWe did not systematically remove known bot accounts~\\\\cite{lebeuf2018swbots} from the author dataset, but we did check for the presence of software bots among the top committers of each year. We only found limited traces of continuous integration (CI) bots, used primarily to automate merge commits. After removing CI bots from the dataset the observed global trends were unchanged, therefore this paper presents unfiltered data.\\n\\n\\n\\\\medskip\\n\\\\emph{Future work.}\\nTo some extent the above limitations are the price to pay to study such a large dataset: there exists a trade-off between large-scale analysis and accuracy.\\nWe plan nonetheless to further investigate and mitigate them in future work.\\nMulti-method approaches, merging data mining with social science methods, could be applied to address some of the questions raised in this exploratory study.\\nWhile they do not scale to the whole dataset, multi-methods can be adopted to dig deeper into specific aspects, specifically those related to social phenomena.\\nSoftware is a social artifact, it is no wonder that aspects related to sociocultural evolution emerge when analyzing its evolution at this scale.\\n\\n\\n\\n\\n \\n\\\\clearpage\\n\\n\\n\""
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [1, 320, 2042, 29912, 25898, 29913, 13, 29905, 1643, 29912, 3471, 29901, 23333, 29913, 13, 13, 29905, 7278, 29912, 29954, 1581, 6894, 537, 1118, 470, 901, 4049, 967, 10225, 727, 974, 29892, 4249, 27138, 304, 13, 20415, 5849, 14188, 756, 1063, 26606, 12399, 297, 7786, 2440, 29889, 512, 13, 1595, 16311, 29892, 278, 10122, 310, 29892, 9545, 310, 29892, 322, 6795, 1004, 25414, 363, 320, 7278, 29912, 26098, 13, 29871, 24003, 29913, 297, 12362, 29914, 6585, 7562, 18540, 313, 5800, 1799, 29897, 505, 4520, 263, 3287, 310, 8570, 13, 957, 278, 4940, 316, 6332, 2651, 2036, 29912, 29881, 16093, 29906, 29900, 29900, 29947, 29888, 2209, 3359, 29879, 29892, 3855, 5871, 29906, 29900, 29896, 29900, 29895, 311, 29893, 2770, 29892, 13, 29871, 1055, 29888, 375, 29906, 29900, 29896, 29906, 5041, 267, 29892, 413, 434, 305, 1358, 29906, 29900, 29896, 29906, 26098, 29888, 2209, 29892, 19723, 5475, 4979, 29906, 29900, 29896, 29946, 26098, 29892, 13, 29871, 697, 309, 29906, 29900, 29896, 29953, 311, 5365, 550, 332, 6950, 29892, 696, 7586, 29906, 29900, 29896, 29953, 29893, 2770, 7192, 2209, 29892, 1935, 15044, 29906, 29900, 29896, 29955, 26098, 29892, 13, 29871, 503, 562, 305, 3350, 492, 29906, 29900, 29906, 29896, 26098, 1836, 29871, 320, 7278, 29912, 7999, 12122, 6894, 537, 29913, 338, 373, 278, 916, 1361, 278, 13, 14380, 310, 6894, 537, 393, 380, 1567, 515, 27138, 297, 777, 5534, 6354, 6421, 13, 3166, 1422, 3186, 12786, 322, 4185, 1973, 29889, 13, 13, 7999, 12122, 6894, 537, 297, 18322, 1799, 756, 4520, 13774, 2217, 8570, 297, 21344, 368, 13, 13129, 29889, 512, 3153, 29892, 1550, 3031, 979, 18994, 29899, 6707, 322, 13, 3149, 29899, 262, 29899, 2230, 18350, 29899, 7052, 11898, 310, 278, 1737, 12122, 1677, 1144, 310, 18322, 1799, 13, 21570, 29560, 1863, 2651, 2036, 29912, 12443, 10578, 29906, 29900, 29900, 29945, 5062, 11235, 29892, 13260, 333, 29906, 29900, 29900, 29947, 29888, 2209, 3359, 29879, 29892, 13, 29871, 2594, 801, 2681, 29906, 29900, 29900, 29947, 479, 397, 24974, 29892, 1850, 29882, 371, 4099, 29894, 29906, 29900, 29896, 29900, 2209, 479, 5275, 29892, 696, 7586, 29906, 29900, 29896, 29946, 7610, 345, 2941, 271, 24541, 29892, 13, 29871, 281, 496, 29879, 29906, 29900, 29906, 29896, 2209, 479, 5275, 1118, 2919, 29899, 7052, 25579, 979, 11898, 310, 278, 1737, 12122, 13, 12574, 310, 18322, 1799, 17737, 29560, 526, 1603, 10225, 292, 29889, 10506, 263, 4323, 23378, 13, 18609, 2133, 723, 367, 5407, 304, 1871, 1602, 12112, 4475, 304, 5534, 13, 25431, 10907, 2651, 2036, 29912, 2276, 5824, 19982, 29906, 29900, 29900, 29955, 23705, 1338, 29893, 996, 29913, 322, 298, 8491, 16650, 583, 297, 278, 13, 19678, 15483, 313, 1806, 29897, 9999, 29892, 408, 1532, 408, 29126, 2114, 950, 2472, 13, 517, 278, 2553, 1078, 373, 278, 17407, 10879, 322, 5374, 3002, 310, 18322, 1799, 2820, 278, 3186, 29889, 13, 13, 13, 29905, 26956, 29912, 1323, 3224, 29879, 29913, 13, 13, 3047, 445, 664, 591, 29126, 304, 3802, 445, 17261, 491, 7512, 292, 320, 4534, 29912, 1552, 937, 13, 29871, 25579, 979, 6559, 310, 278, 1737, 12122, 3978, 310, 17737, 29560, 304, 970, 775, 13, 29871, 975, 29871, 29945, 29900, 2440, 5003, 26321, 29892, 591, 3867, 263, 758, 2576, 3821, 1234, 304, 278, 13, 23031, 292, 5925, 1139, 29901, 13, 29905, 463, 29912, 690, 2842, 12470, 29913, 13, 29871, 3645, 607, 3186, 12786, 437, 15717, 310, 970, 368, 3625, 25741, 2041, 515, 13, 29871, 322, 920, 756, 372, 3939, 975, 278, 4940, 29871, 29945, 29900, 2440, 29973, 13, 29871, 320, 1643, 29912, 29878, 29939, 29901, 479, 397, 24974, 29913, 13, 29905, 355, 29912, 690, 2842, 12470, 29913, 13, 4806, 671, 408, 8783, 278, 320, 23066, 29950, 29914, 18871, 2651, 2036, 29912, 29879, 1332, 29875, 4569, 29906, 29900, 29896, 29955, 29913, 322, 27599, 515, 372, 13, 29906, 29889, 29906, 24464, 29905, 13007, 25741, 3190, 2347, 515, 29871, 29896, 29953, 29900, 7284, 29905, 13007, 9279, 322, 4148, 287, 491, 13, 29946, 29941, 7284, 29905, 13007, 15717, 2645, 278, 29871, 29896, 29929, 29955, 29896, 489, 29906, 29900, 29906, 29896, 931, 3785, 29889, 29871, 13, 4806, 1737, 324, 542, 403, 18777, 304, 13, 29905, 14573, 14058, 4597, 1080, 29914, 3186, 12786, 29892, 773, 408, 18470, 4876, 4234, 775, 2246, 29899, 5563, 21904, 313, 617, 29911, 10249, 29879, 29897, 322, 29871, 13, 8921, 313, 4102, 29914, 4230, 29897, 2983, 9401, 411, 1024, 18822, 2820, 278, 3186, 29892, 322, 17998, 1283, 7224, 29871, 13, 1195, 287, 515, 9063, 15562, 29889, 13, 13, 4806, 1284, 10757, 310, 278, 4688, 8022, 749, 310, 4644, 6813, 297, 1722, 2752, 13, 20415, 29892, 2678, 8772, 491, 4092, 29889, 2860, 393, 3785, 29892, 278, 1737, 12122, 6894, 537, 29871, 13, 262, 970, 775, 756, 1063, 21003, 10231, 29889, 13, 4806, 884, 12439, 8018, 15839, 528, 17741, 13, 12817, 304, 278, 1095, 310, 278, 8291, 6415, 29129, 322, 278, 7910, 310, 14137, 4631, 4135, 297, 13, 23369, 1705, 322, 4275, 14325, 29892, 408, 1532, 408, 310, 2545, 1664, 17292, 28342, 763, 25539, 1608, 322, 13, 25719, 10298, 4822, 10916, 313, 6727, 16783, 29914, 331, 16783, 467, 13, 13, 13, 13, 13, 29905, 26956, 29912, 1469, 20847, 3097, 5003, 13, 13, 29909, 1634, 1414, 3577, 363, 445, 5650, 338, 3625, 515, 796, 264, 8144, 472, 13, 29905, 2271, 29912, 991, 597, 1867, 29875, 29889, 990, 29914, 29896, 29900, 29889, 29945, 29906, 29947, 29896, 29914, 2256, 8144, 29889, 29953, 29941, 29929, 29900, 29941, 29945, 29945, 29913, 2651, 2036, 29912, 3445, 1414, 29899, 5113, 1836, 13, 13, 13, 320, 2042, 29912, 9662, 630, 5244, 29913, 13, 29905, 1643, 29912, 3471, 29901, 12817, 29913, 13, 13, 29933, 720, 4688, 322, 7786, 1736, 2651, 2036, 29912, 12443, 10578, 29906, 29900, 29900, 29945, 5062, 11235, 29892, 13260, 333, 29906, 29900, 29900, 29947, 29888, 2209, 3359, 29879, 29892, 13, 29871, 696, 7586, 29906, 29900, 29896, 29946, 7610, 345, 2941, 271, 24541, 29892, 697, 309, 29906, 29900, 29896, 29953, 311, 5365, 550, 332, 6950, 29913, 505, 2931, 1891, 278, 13, 479, 5275, 310, 12362, 29914, 6585, 7562, 18540, 313, 5800, 1799, 29897, 773, 320, 7278, 29912, 6734, 26946, 952, 1118, 13, 4716, 3867, 1880, 29899, 29567, 6089, 541, 526, 9078, 297, 2159, 313, 29906, 29899, 29945, 5940, 29968, 18777, 29897, 13, 392, 508, 367, 4768, 1463, 491, 5221, 424, 23460, 29889, 13, 13, 797, 29871, 29906, 29900, 29900, 29947, 2261, 801, 2681, 634, 394, 8770, 2036, 29912, 1646, 801, 2681, 29906, 29900, 29900, 29947, 479, 397, 24974, 29913, 18043, 263, 3031, 979, 13, 16961, 29899, 7052, 313, 1454, 278, 931, 29897, 6559, 373, 18322, 1799, 320, 7278, 29912, 479, 5275, 773, 1375, 292, 7047, 13, 29871, 28914, 313, 4345, 29934, 29897, 13698, 1836, 2688, 29537, 287, 278, 3978, 310, 29871, 29896, 5940, 29924, 17737, 29560, 13, 4746, 278, 7562, 2831, 479, 1404, 2566, 322, 611, 6504, 1051, 3190, 3145, 975, 278, 13, 29896, 29929, 29929, 29929, 489, 29906, 29900, 29900, 29945, 3785, 29892, 773, 408, 18470, 2472, 2788, 304, 1749, 29879, 29901, 4876, 21904, 13, 392, 17998, 1283, 7224, 29889, 29871, 13, 1576, 12399, 3785, 313, 29955, 2440, 29897, 297, 2651, 2036, 29912, 1646, 801, 2681, 29906, 29900, 29900, 29947, 479, 397, 24974, 29913, 338, 20511, 1135, 29871, 13, 5816, 338, 12399, 297, 278, 2198, 5650, 313, 29945, 29900, 2440, 29897, 322, 278, 848, 8974, 526, 29871, 13, 27489, 873, 1422, 29936, 411, 393, 297, 3458, 29892, 1749, 2582, 1510, 263, 10029, 7200, 14978, 310, 29871, 13, 15654, 273, 325, 13217, 29940, 2072, 3082, 20706, 29889, 13, 13, 2744, 1228, 29190, 936, 664, 515, 29871, 29906, 29900, 29896, 29900, 491, 14619, 29882, 371, 4099, 29894, 322, 13, 29950, 309, 1372, 2651, 2036, 29912, 29873, 19426, 371, 4099, 29894, 29906, 29900, 29896, 29900, 2209, 479, 5275, 29913, 4023, 29894, 2868, 1583, 29899, 7099, 433, 1127, 1737, 12122, 13, 2029, 800, 310, 25492, 15303, 8304, 3598, 1494, 1009, 12368, 29892, 13, 15914, 292, 2472, 363, 779, 14850, 4535, 29892, 29955, 29900, 5940, 29968, 25492, 4160, 29889, 29871, 319, 1407, 7786, 13, 1287, 2651, 2036, 29912, 21686, 29879, 29906, 29900, 29906, 29896, 2209, 479, 5275, 29913, 491, 399, 496, 29879, 634, 394, 13217, 5349, 1737, 324, 542, 630, 4203, 263, 7284, 13, 28712, 16046, 4160, 29892, 2534, 26869, 472, 3203, 29871, 29896, 29900, 29900, 25741, 1269, 29892, 322, 1058, 13, 1311, 29899, 7099, 8663, 14354, 373, 1009, 25492, 28723, 29889, 5806, 278, 6559, 338, 13, 3149, 29899, 262, 29899, 2230, 408, 310, 29871, 29906, 29900, 29906, 29896, 29892, 278, 15717, 7252, 1009, 1284, 886, 13, 351, 475, 303, 2651, 2036, 29912, 1646, 801, 2681, 29906, 29900, 29900, 29947, 479, 397, 24974, 29892, 1850, 29882, 371, 4099, 29894, 29906, 29900, 29896, 29900, 2209, 479, 5275, 29913, 304, 13, 18609, 675, 278, 14675, 310, 18322, 1799, 1737, 5275, 975, 278, 931, 15101, 845, 1862, 4586, 491, 13, 1552, 2211, 11898, 29889, 13, 13, 6843, 1965, 411, 3517, 29190, 936, 1736, 29892, 1749, 6559, 338, 1568, 7200, 6287, 5634, 29882, 5555, 13, 7054, 12339, 287, 29871, 29946, 29941, 7284, 29905, 13007, 15717, 310, 29871, 29906, 29889, 29906, 24464, 29905, 13007, 25741, 515, 29871, 29896, 29953, 29900, 7284, 29905, 13007, 13, 16418, 5634, 5426, 11267, 979, 975, 29871, 29945, 29900, 2440, 310, 970, 775, 20706, 3265, 1135, 13, 3149, 297, 931, 29892, 322, 884, 901, 2691, 29899, 3874, 1312, 313, 2541, 1629, 29899, 1609, 29899, 6360, 3803, 1070, 537, 975, 13, 1552, 8900, 3785, 467, 8108, 1189, 1711, 29892, 1749, 6559, 337, 3687, 373, 10079, 11264, 13, 3924, 313, 8257, 29903, 29897, 9063, 848, 3265, 1135, 7481, 29899, 7099, 433, 1127, 4423, 2472, 29889, 13, 13, 13, 16107, 1736, 5634, 262, 3153, 278, 664, 491, 8432, 2651, 2036, 29912, 18386, 709, 29906, 29900, 29896, 29941, 2209, 29881, 24974, 29913, 13, 392, 29892, 901, 10325, 29892, 390, 579, 468, 29875, 634, 394, 8770, 2036, 29912, 29878, 579, 468, 29875, 29906, 29900, 29896, 29953, 479, 711, 3173, 29892, 13, 29871, 364, 579, 468, 29875, 29906, 29900, 29896, 29947, 479, 711, 3173, 29892, 544, 1648, 29906, 29900, 29906, 29896, 479, 468, 1581, 29881, 24974, 29913, 5634, 17532, 12399, 1737, 12122, 13, 29905, 7278, 29912, 29881, 24974, 322, 24003, 1118, 474, 29889, 29872, 1696, 278, 15834, 304, 607, 278, 3978, 310, 18322, 1799, 13, 17426, 6602, 1009, 11465, 1230, 14137, 14188, 29889, 13, 797, 445, 664, 591, 2931, 1891, 1737, 12122, 6894, 537, 297, 970, 775, 363, 278, 937, 13, 2230, 472, 445, 6287, 29892, 1716, 297, 4958, 310, 17737, 29560, 322, 15500, 3785, 29889, 1334, 437, 13, 1333, 22002, 280, 278, 24003, 10696, 29892, 541, 3867, 29190, 936, 848, 322, 1284, 886, 393, 508, 367, 13, 280, 369, 4063, 304, 393, 1095, 408, 5434, 664, 29889, 13, 13, 29905, 7278, 29912, 12756, 7047, 21639, 29913, 2651, 2036, 29912, 2276, 5824, 19982, 29906, 29900, 29900, 29955, 23705, 1338, 29893, 996, 29913, 338, 278, 13, 1491, 29899, 2671, 310, 7047, 21639, 393, 756, 29537, 287, 278, 18066, 267, 310, 21640, 13, 6734, 24771, 13149, 635, 29892, 3704, 278, 2702, 5932, 310, 920, 304, 5376, 13, 2541, 1737, 12122, 6894, 537, 2651, 2036, 29912, 13376, 303, 456, 29906, 29900, 29900, 29953, 10945, 3359, 29892, 1424, 29440, 29906, 29900, 29896, 29946, 23027, 5933, 1836, 13, 6185, 3076, 2678, 278, 2198, 6559, 8128, 10757, 393, 508, 367, 1304, 29892, 297, 278, 13, 14940, 1206, 310, 970, 775, 322, 472, 263, 1407, 2919, 6287, 29892, 304, 11539, 607, 13, 14032, 4637, 310, 5534, 7047, 21639, 505, 9820, 484, 15774, 29889, 13, 13, 13, 13, 13, 13, 13, 320, 2042, 29912, 4062, 3002, 29913, 13, 29905, 1643, 29912, 3471, 29901, 5696, 29913, 13, 13, 13, 29905, 1482, 361, 29905, 361, 29887, 798, 386, 1003, 29871, 320, 29887, 798, 386, 1003, 3009, 13, 29905, 361, 29887, 798, 386, 1003, 13, 29905, 463, 29912, 4532, 29913, 13, 29871, 320, 7313, 29961, 2103, 2013, 4914, 2103, 3199, 6360, 368, 29899, 2055, 1169, 29913, 13, 29871, 320, 6671, 29912, 12883, 368, 970, 25741, 975, 931, 313, 1188, 6287, 467, 13, 29913, 13, 29871, 320, 1643, 29912, 1003, 29901, 29887, 798, 386, 29913, 13, 29905, 355, 29912, 4532, 29913, 13, 29905, 7241, 13, 13, 29905, 26956, 29912, 16390, 24541, 29913, 13, 13, 4806, 27387, 515, 320, 23066, 29950, 29914, 2651, 2036, 29912, 29879, 1332, 29899, 1516, 29878, 29906, 29900, 29896, 29929, 29899, 24713, 29913, 599, 25741, 3190, 2347, 2745, 320, 25832, 1964, 579, 1523, 2415, 2539, 6294, 13, 15597, 5253, 304, 320, 14573, 5261, 1169, 22131, 29914, 25741, 29892, 5412, 491, 317, 15715, 29896, 15882, 29892, 4023, 29894, 2868, 515, 320, 25832, 1299, 7288, 5261, 1169, 797, 7068, 29914, 970, 9279, 6421, 515, 4655, 5849, 363, 2710, 313, 28712, 16046, 29892, 11786, 28632, 29892, 2992, 1846, 322, 3577, 28914, 313, 10251, 713, 29892, 10772, 2227, 29892, 405, 13427, 29892, 2992, 6250, 13, 5261, 1169, 297, 278, 8783, 526, 491, 320, 25832, 6344, 2806, 943, 22131, 29914, 15717, 29892, 5412, 491, 779, 6990, 29938, 978, 29892, 4876, 4535, 5854, 29938, 11000, 29889, 13, 1576, 8783, 2996, 408, 1023, 1104, 1288, 6131, 29892, 697, 363, 25741, 322, 697, 363, 15717, 29892, 411, 278, 4642, 29371, 278, 7480, 3025, 263, 9117, 1820, 29889, 13, 29905, 361, 5426, 13, 9760, 1948, 297, 278, 9063, 1591, 3743, 278, 1494, 4235, 29901, 9063, 317, 15715, 29896, 15882, 29892, 4148, 322, 844, 5171, 5335, 342, 15092, 29892, 4148, 322, 844, 5171, 2893, 14903, 313, 20275, 16750, 278, 4148, 1591, 467, 13, 1576, 21578, 1546, 9063, 15717, 322, 9063, 2153, 2041, 515, 11786, 29892, 607, 6511, 304, 9063, 263, 1735, 4148, 287, 491, 4856, 1683, 29889, 13, 2831, 445, 6559, 591, 21309, 373, 15717, 322, 17262, 9063, 2153, 29892, 408, 278, 4328, 1546, 278, 1023, 338, 451, 8018, 363, 1749, 5925, 5155, 322, 278, 5253, 310, 25741, 411, 263, 844, 5171, 916, 1135, 967, 4148, 338, 3480, 3473, 1821, 29889, 13, 29905, 7241, 13, 2831, 1269, 6251, 297, 278, 4148, 1591, 591, 505, 4148, 2989, 1024, 322, 4876, 408, 1023, 5004, 6031, 310, 10650, 6262, 29889, 13, 13, 4806, 6206, 2411, 15273, 1821, 470, 18325, 519, 2983, 393, 29901, 526, 451, 1602, 397, 519, 408, 18351, 29899, 29947, 3441, 25832, 6344, 2806, 943, 29934, 29885, 12283, 7099, 397, 519, 29914, 4148, 2983, 6206, 511, 526, 4876, 14157, 2012, 310, 2983, 3441, 25832, 6344, 2806, 943, 29934, 29885, 9823, 29914, 4954, 7039, 4907, 511, 5718, 310, 871, 9654, 4890, 3441, 25832, 6344, 2806, 943, 29934, 29885, 10358, 804, 23201, 1712, 901, 1135, 29871, 29896, 29900, 8958, 1661, 29899, 1026, 2153, 3441, 25832, 6344, 2806, 943, 29934, 29885, 12283, 15670, 23201, 526, 5520, 1135, 29871, 29896, 29900, 29900, 4890, 3441, 25832, 6344, 2806, 943, 29934, 29885, 12229, 549, 12495, 13, 13555, 21166, 29892, 1048, 320, 25832, 6344, 2806, 943, 29925, 15273, 1821, 2052, 307, 29916, 29914, 15717, 3441, 25832, 6344, 2806, 943, 29925, 15273, 1821, 29925, 312, 29914, 310, 278, 2847, 8783, 29897, 9488, 363, 4340, 7418, 29889, 13, 13, 9842, 393, 278, 5253, 310, 970, 775, 25741, 313, 392, 15717, 29897, 11122, 297, 278, 13, 11228, 8783, 25088, 18709, 9247, 975, 13, 2230, 2651, 2036, 29912, 29879, 1332, 29899, 771, 854, 749, 29899, 331, 344, 1012, 361, 29887, 798, 386, 1003, 29892, 408, 4318, 363, 25741, 297, 13, 29905, 29907, 999, 29912, 1003, 29901, 29887, 798, 386, 1012, 2870, 29901, 515, 395, 29896, 29900, 29985, 29946, 29938, 25741, 297, 29871, 29896, 29929, 29955, 29896, 29892, 304, 395, 29896, 29900, 29985, 29953, 29938, 297, 29871, 29896, 29929, 29929, 29947, 29892, 304, 13, 284, 3242, 395, 29896, 29900, 29985, 29929, 29938, 297, 29871, 29906, 29900, 29906, 29900, 29905, 7241, 29889, 1094, 263, 17004, 278, 8900, 534, 1975, 10331, 304, 367, 901, 13, 13844, 297, 7786, 1602, 3076, 1135, 297, 29871, 29946, 29900, 29974, 1629, 29899, 1025, 6743, 29892, 2861, 304, 13964, 4586, 373, 13, 735, 1112, 9247, 7200, 23093, 29889, 13, 13, 13, 29905, 26956, 29912, 7999, 324, 10610, 29913, 13, 13, 29905, 463, 29912, 4532, 29913, 13, 29871, 320, 9525, 13, 29871, 320, 7313, 29961, 24049, 29892, 15450, 29922, 29953, 4912, 29871, 29953, 4912, 29871, 29900, 29871, 29900, 29892, 2103, 2013, 16292, 3199, 1491, 1727, 1080, 29899, 2470, 29913, 13, 29871, 320, 6671, 29912, 1576, 320, 14573, 14058, 4597, 1080, 29914, 3186, 12786, 1304, 408, 1737, 324, 10610, 22525, 5003, 13, 29871, 320, 1643, 29912, 1003, 29901, 11526, 1958, 29913, 13, 29905, 355, 29912, 4532, 29913, 13, 13, 2887, 1737, 324, 10610, 22525, 591, 671, 11758, 3186, 12786, 10723, 515, 278, 3303, 18269, 1737, 359, 305, 2004, 2651, 2036, 29912, 348, 29896, 29929, 29929, 29929, 479, 359, 305, 2004, 1836, 13, 1762, 4772, 2432, 3381, 491, 2919, 10916, 313, 29872, 29889, 29887, 1696, 7551, 470, 12710, 29897, 2629, 11758, 12786, 29892, 591, 19412, 322, 6219, 777, 12786, 2729, 373, 1737, 12122, 23203, 537, 322, 278, 19383, 310, 758, 331, 8946, 16375, 29769, 5680, 29892, 1316, 408, 19182, 4086, 29889, 13, 29905, 29907, 999, 29912, 1003, 29901, 11526, 1958, 29913, 3697, 278, 2186, 1051, 310, 320, 14573, 14058, 4597, 1080, 29914, 3186, 12786, 1304, 408, 1737, 324, 10610, 22525, 297, 445, 6559, 29889, 13, 13, 7999, 324, 10610, 310, 9063, 15717, 304, 3186, 12786, 3913, 278, 1023, 19595, 653, 13698, 9129, 297, 2651, 2036, 29912, 293, 344, 29899, 344, 275, 29899, 29906, 29900, 29906, 29906, 29899, 26098, 1118, 23359, 337, 13998, 2400, 29889, 13, 1576, 937, 697, 337, 3687, 373, 278, 4234, 775, 2246, 29899, 5563, 5354, 313, 617, 29911, 10249, 29897, 310, 4876, 14157, 23892, 515, 9063, 15562, 29892, 321, 29889, 29887, 1696, 320, 16266, 26139, 1341, 1118, 320, 16266, 26139, 582, 1118, 320, 16266, 26139, 18038, 1118, 2992, 29889, 13, 4806, 4687, 515, 278, 306, 2190, 29909, 1051, 310, 13548, 2931, 21759, 29911, 10249, 29879, 2651, 2036, 29912, 6011, 29899, 29883, 312, 430, 29913, 322, 7522, 20545, 1269, 6590, 20123, 304, 263, 3646, 3186, 5120, 29889, 13, 13, 1576, 1473, 1737, 324, 10610, 11043, 3913, 278, 17998, 9210, 310, 9063, 5335, 342, 15092, 313, 29872, 29889, 29887, 1696, 17998, 29899, 29900, 29945, 29901, 29900, 29900, 29897, 322, 4148, 2983, 304, 8161, 278, 1556, 5517, 3186, 5120, 310, 278, 9063, 4148, 29889, 13, 2831, 1269, 17998, 9210, 591, 8161, 263, 1051, 310, 15878, 7600, 313, 13509, 29892, 2106, 29892, 470, 14278, 20123, 29897, 297, 278, 3186, 393, 29892, 472, 278, 931, 310, 393, 9063, 29892, 750, 393, 17998, 9210, 29936, 9063, 931, 338, 1820, 1244, 29892, 408, 4234, 17998, 1283, 7224, 13100, 975, 931, 2861, 304, 29431, 3620, 29889, 13, 1762, 1207, 445, 3683, 3381, 591, 671, 278, 306, 2190, 29909, 931, 10640, 2566, 2651, 2036, 29912, 17559, 1272, 1836, 13, 13, 11760, 591, 3566, 304, 1269, 2058, 263, 8158, 393, 4332, 1973, 278, 4188, 22342, 393, 263, 2183, 4148, 1024, 338, 17443, 310, 372, 29889, 13, 1762, 445, 1095, 591, 671, 278, 28297, 29890, 15451, 8783, 310, 278, 29511, 310, 278, 1556, 3619, 937, 322, 3942, 2983, 607, 29892, 439, 11427, 515, 2651, 2036, 29912, 1079, 29890, 799, 29899, 7039, 6177, 2802, 277, 12181, 4954, 16123, 2247, 278, 26368, 5528, 5084, 310, 363, 264, 1280, 322, 269, 595, 1280, 7371, 515, 263, 2566, 310, 320, 1949, 29912, 29946, 29871, 29900, 29946, 29946, 29871, 29945, 29946, 29953, 29871, 29929, 29941, 29947, 29913, 2305, 313, 29945, 29945, 29889, 29945, 8958, 310, 8471, 2305, 297, 29871, 29906, 29900, 29896, 29946, 467, 1094, 310, 3839, 29871, 29906, 29900, 29896, 29929, 372, 18469, 320, 1949, 29912, 29906, 29955, 29871, 29953, 29953, 29906, 29871, 29947, 29900, 29896, 29913, 363, 264, 1280, 322, 320, 1949, 29912, 29906, 29955, 29871, 29906, 29900, 29953, 29871, 29947, 29906, 29896, 29913, 269, 595, 1280, 297, 29871, 29906, 29941, 29953, 24894, 8977, 1080, 29889, 4907, 29913, 13, 2887, 297, 1749, 8783, 15717, 526, 2989, 1024, 6031, 313, 29878, 1624, 1135, 6219, 491, 937, 29914, 11922, 1024, 511, 591, 937, 5993, 675, 2983, 313, 1609, 1999, 1331, 322, 1206, 3620, 29897, 322, 769, 16280, 5375, 18897, 297, 1716, 937, 322, 3942, 2983, 10868, 8857, 29889, 13, 2831, 1269, 1543, 1476, 297, 1024, 8857, 591, 22932, 278, 2058, 4665, 29905, 6661, 1333, 24212, 8875, 491, 278, 1024, 10868, 304, 4017, 263, 5645, 393, 338, 29839, 304, 278, 1353, 310, 12407, 24638, 393, 1024, 313, 6979, 29897, 297, 278, 2702, 2058, 29889, 13, 29905, 6661, 1333, 300, 1062, 29912, 1762, 4017, 4665, 2025, 1338, 5634, 294, 278, 17837, 310, 4954, 6689, 4907, 338, 25745, 23724, 29901, 2989, 10916, 325, 13217, 29879, 29399, 310, 2919, 10916, 805, 9450, 2999, 931, 29920, 2873, 5634, 705, 671, 263, 29544, 310, 7601, 8974, 313, 29872, 29889, 29887, 1696, 5874, 28007, 511, 322, 1661, 29899, 16072, 6743, 313, 29872, 29889, 29887, 1696, 14109, 7456, 467, 29913, 13, 4806, 2533, 445, 4377, 363, 599, 3161, 304, 4017, 263, 2058, 8158, 29892, 17140, 701, 411, 263, 1051, 310, 779, 6990, 29938, 6689, 29892, 8158, 4535, 5854, 29938, 11000, 29889, 13, 4806, 769, 8877, 445, 1051, 491, 278, 3186, 5120, 393, 263, 2058, 14393, 304, 322, 2533, 278, 8158, 363, 599, 278, 7600, 297, 1269, 5120, 304, 4017, 385, 12463, 8158, 29892, 6590, 304, 278, 4188, 22342, 393, 278, 9063, 14393, 304, 263, 2183, 3186, 5120, 29889, 13, 4806, 3566, 278, 6257, 9063, 408, 6421, 515, 278, 3186, 5120, 411, 278, 9939, 8158, 29889, 13, 13, 1576, 4876, 29899, 6707, 11043, 9378, 414, 515, 278, 9078, 322, 443, 5521, 8362, 671, 310, 21759, 29911, 10249, 29879, 29901, 1556, 18777, 671, 10035, 323, 10249, 29879, 1316, 408, 320, 16266, 26139, 510, 1118, 320, 16266, 26139, 990, 1118, 470, 320, 16266, 26139, 1212, 1836, 13, 20761, 957, 445, 947, 451, 3799, 26018, 4822, 20542, 29901, 3148, 29899, 6707, 18777, 29892, 363, 1342, 29892, 671, 278, 320, 16266, 26139, 375, 29913, 21759, 29911, 10249, 1568, 901, 269, 2495, 290, 368, 1135, 1009, 7824, 6795, 20895, 29889, 13, 2951, 278, 916, 1361, 278, 9210, 29914, 978, 29899, 6707, 11043, 337, 3687, 373, 278, 17998, 9210, 310, 278, 9063, 5335, 342, 15092, 29889, 13, 29928, 434, 304, 5780, 22920, 373, 13897, 731, 14340, 29892, 263, 2919, 1353, 310, 25741, 297, 278, 8783, 756, 385, 17998, 9210, 5186, 304, 5225, 29889, 13, 4013, 6602, 29879, 3109, 7786, 25741, 3441, 14573, 5261, 1169, 29911, 29999, 29999, 13985, 1349, 681, 392, 27418, 6478, 29914, 310, 29871, 29906, 29900, 29906, 29900, 29879, 25741, 505, 263, 5225, 9210, 29897, 1135, 9642, 6743, 3441, 14573, 5261, 1169, 29911, 29999, 29999, 13985, 1349, 681, 392, 29914, 297, 29871, 29906, 29900, 29900, 29900, 467, 13, 2887, 263, 1121, 278, 9210, 29914, 978, 29899, 6707, 11043, 1033, 1095, 701, 6459, 292, 263, 2919, 6232, 310, 9642, 25741, 408, 4148, 287, 491, 11715, 18777, 29892, 322, 304, 263, 3109, 261, 15834, 4092, 550, 29889, 13, 13, 1762, 6795, 1438, 5626, 591, 14405, 278, 1023, 1737, 324, 10610, 13698, 4208, 491, 15399, 278, 9210, 29914, 978, 29899, 6707, 13698, 304, 599, 25741, 411, 263, 1661, 29899, 9171, 17998, 9210, 29892, 322, 278, 4876, 29899, 6707, 373, 304, 599, 916, 25741, 29889, 13, 13, 13, 320, 2042, 29912, 12191, 322, 8565, 1558, 291, 29913, 13, 29905, 1643, 29912, 3471, 29901, 9902, 29913, 13, 13, 29905, 463, 29912, 4532, 4044, 13, 29871, 320, 9525, 13, 29871, 320, 7313, 29961, 2103, 2013, 16292, 3199, 1429, 287, 29889, 5140, 29913, 13, 29871, 320, 6671, 29912, 29934, 20819, 310, 25741, 313, 27215, 29897, 322, 6136, 15717, 313, 22503, 29897, 491, 3186, 10640, 975, 278, 29871, 29896, 29929, 29955, 29896, 489, 29906, 29900, 29906, 29900, 3785, 5003, 13, 29871, 320, 9868, 29961, 14732, 3199, 7264, 287, 2594, 8727, 6445, 278, 3186, 10640, 364, 2219, 359, 363, 25741, 322, 15717, 975, 278, 29871, 29896, 29929, 29955, 29896, 489, 29906, 29900, 29906, 29900, 3785, 5003, 13, 29871, 320, 1643, 29912, 1003, 29901, 9902, 29913, 13, 29905, 355, 29912, 4532, 4044, 13, 13, 13, 29871, 13, 1762, 1234, 320, 25029, 29912, 29878, 29939, 29901, 479, 397, 24974, 29913, 591, 22229, 278, 1353, 310, 25741, 322, 8359, 15717, 639, 1629, 322, 639, 3186, 10640, 29889, 13, 4806, 2198, 278, 7625, 2582, 297, 320, 29907, 999, 29912, 1003, 29901, 9902, 29913, 408, 1023, 5096, 287, 2594, 24469, 29892, 6445, 1629, 368, 2867, 3204, 29879, 363, 25741, 322, 15717, 8307, 29889, 13, 26526, 2594, 11524, 263, 1629, 322, 338, 8877, 287, 297, 269, 29399, 6445, 278, 9063, 29914, 8921, 11959, 363, 1269, 310, 278, 3186, 12786, 310, 320, 29907, 999, 29912, 1003, 29901, 11526, 1958, 29913, 297, 393, 1629, 29889, 13, 1762, 4772, 714, 27801, 2861, 304, 805, 272, 26538, 17737, 29560, 29892, 297, 278, 4148, 8727, 591, 871, 2050, 15717, 2534, 26869, 472, 3203, 29871, 29945, 25741, 297, 263, 2183, 1629, 29889, 13, 13, 8809, 488, 5366, 1747, 534, 1975, 297, 278, 24469, 6456, 393, 278, 3001, 3694, 310, 25741, 322, 15717, 6548, 18709, 9247, 975, 931, 29889, 13, 29950, 663, 363, 278, 937, 2440, 297, 278, 24469, 29892, 278, 1353, 310, 848, 3291, 297, 777, 3186, 12786, 508, 367, 14154, 2319, 29892, 411, 8178, 27721, 373, 278, 25806, 310, 534, 1975, 29889, 13, 13, 13, 13, 13, 29905, 26956, 29912, 7999, 12122, 6894, 537, 975, 931, 29913, 13, 13, 3563, 497, 29892, 278, 2498, 534, 355, 5692, 304, 367, 393, 278, 320, 4534, 29912, 479, 12122, 6894, 537, 297, 970, 775, 338, 10231, 6177, 4644, 6813, 322, 4092, 5136, 630, 1009, 4954, 24130, 749, 4907, 2745, 278, 7256, 310, 278, 29871, 29929, 29900, 29879, 29936, 515, 393, 3256, 373, 1556, 916, 3186, 12786, 1510, 263, 5232, 541, 27357, 11924, 29889, 13, 4013, 534, 355, 310, 11664, 27577, 964, 970, 775, 5849, 7805, 8068, 322, 4275, 14325, 313, 510, 558, 5921, 7513, 511, 12710, 29892, 10557, 29892, 8068, 322, 4275, 6813, 29892, 13, 3664, 625, 393, 884, 20542, 393, 437, 451, 2833, 304, 1101, 445, 534, 355, 29892, 1316, 408, 8314, 322, 1570, 13450, 29892, 526, 884, 10231, 1009, 27577, 29892, 541, 472, 263, 5224, 6210, 411, 3390, 304, 916, 20542, 29889, 13, 2831, 1342, 29892, 8314, 322, 1570, 13450, 11924, 287, 278, 8380, 1353, 310, 1009, 25741, 491, 1048, 29871, 29941, 11299, 310, 18497, 515, 29871, 29906, 29900, 29900, 29900, 304, 2198, 3841, 29889, 13, 13, 2744, 1228, 8031, 27791, 265, 393, 508, 367, 7556, 297, 1716, 24469, 338, 278, 8327, 6761, 428, 310, 20706, 515, 4644, 6813, 297, 29871, 29896, 29929, 29929, 29945, 29936, 1951, 278, 24469, 1401, 919, 364, 2219, 359, 29892, 445, 16161, 304, 916, 20542, 29892, 322, 4092, 297, 3153, 29892, 10231, 1009, 6232, 29889, 13, 2744, 7418, 310, 278, 1667, 20706, 297, 278, 2440, 1492, 1434, 278, 6761, 428, 3697, 393, 14183, 714, 310, 3006, 505, 320, 16266, 29912, 1682, 29890, 29894, 1165, 29889, 17104, 27279, 29889, 3352, 29965, 29913, 408, 4148, 4876, 5354, 29892, 322, 278, 260, 9097, 338, 27179, 350, 520, 293, 29892, 697, 310, 278, 8236, 26663, 350, 7230, 18777, 29892, 21534, 411, 4876, 320, 16266, 29912, 29890, 520, 293, 1836, 13, 3782, 13897, 411, 278, 1021, 4876, 5354, 5692, 15128, 2629, 278, 937, 6893, 17737, 29560, 297, 29871, 29896, 29929, 29929, 29953, 29889, 13, 4013, 3697, 278, 29527, 749, 393, 350, 7230, 26663, 322, 278, 20972, 23985, 10550, 6431, 472, 278, 3014, 310, 8046, 472, 2292, 27279, 750, 297, 278, 4955, 310, 1722, 2752, 7047, 29889, 13, 1576, 2318, 471, 766, 4980, 287, 297, 29871, 29896, 29929, 29929, 29945, 29892, 22039, 408, 263, 17004, 310, 278, 577, 29899, 13998, 8291, 6415, 29129, 2651, 2036, 29912, 22178, 1141, 273, 29906, 29900, 29896, 29929, 24538, 18434, 1118, 322, 445, 640, 5026, 16951, 5634, 15189, 1363, 310, 278, 13774, 4482, 5253, 310, 970, 775, 18342, 1218, 472, 278, 931, 5634, 517, 278, 8327, 5768, 310, 20706, 515, 4644, 6813, 297, 15352, 2440, 29889, 13, 19617, 5818, 8291, 6415, 13598, 6757, 2729, 373, 350, 7230, 29892, 1316, 408, 4673, 29933, 7230, 29892, 12362, 29933, 7230, 29892, 322, 12670, 29933, 7230, 750, 7968, 29527, 749, 304, 3186, 534, 1975, 2861, 304, 313, 29875, 29897, 278, 10231, 5253, 310, 1722, 2752, 775, 6421, 515, 17551, 322, 313, 2236, 29897, 1009, 901, 1737, 1946, 1711, 16984, 13897, 7881, 29889, 13, 13, 2744, 1228, 931, 3515, 297, 607, 278, 364, 2219, 359, 363, 4092, 322, 4644, 6813, 526, 4967, 304, 2919, 29892, 8327, 3620, 338, 29871, 29896, 29929, 29955, 29945, 489, 29955, 29929, 29889, 13, 29909, 758, 2576, 3821, 7418, 3697, 393, 1438, 364, 2219, 359, 526, 4589, 2454, 2861, 304, 278, 1407, 9078, 1353, 310, 25741, 297, 1906, 931, 3785, 29892, 541, 591, 892, 9368, 304, 6459, 263, 2702, 3876, 4556, 29889, 13, 2308, 1975, 363, 1906, 2440, 881, 367, 4967, 304, 4340, 11898, 29892, 297, 24771, 411, 7047, 3603, 5834, 29889, 13, 13, 13, 29905, 26956, 29912, 1625, 25164, 1608, 29913, 13, 13, 2744, 1228, 534, 355, 393, 15028, 714, 515, 278, 24469, 338, 393, 10557, 5692, 304, 367, 1532, 9875, 29889, 13, 1762, 24809, 565, 445, 2582, 515, 263, 1158, 5996, 24003, 29892, 591, 3765, 29899, 11238, 278, 25741, 17809, 408, 3978, 1218, 515, 10557, 363, 931, 29920, 2873, 5134, 297, 278, 11970, 29900, 29892, 29871, 29941, 9341, 3464, 773, 1716, 278, 4876, 29899, 278, 9210, 29914, 978, 29899, 6707, 3519, 29889, 13, 1576, 2582, 1510, 393, 278, 9210, 29914, 978, 29899, 6707, 2948, 3566, 29879, 29871, 29906, 29906, 29889, 29955, 8958, 310, 278, 25741, 304, 10557, 13452, 278, 4876, 29899, 6707, 697, 871, 3566, 29879, 29871, 29906, 29889, 29955, 8958, 310, 963, 29889, 13, 8809, 488, 263, 25871, 22522, 338, 297, 1797, 29892, 372, 338, 1749, 9426, 393, 278, 27791, 265, 591, 526, 16277, 292, 1244, 338, 263, 17004, 310, 25539, 1608, 29892, 10816, 278, 594, 3385, 310, 4092, 550, 2983, 297, 11715, 10916, 29889, 13, 2831, 1342, 278, 1024, 14713, 29892, 10723, 515, 8198, 4186, 344, 29892, 338, 901, 5972, 297, 13491, 1648, 1135, 372, 338, 297, 3444, 470, 297, 278, 10261, 29889, 13, 4013, 18066, 267, 278, 11509, 310, 278, 9210, 29914, 978, 29899, 6707, 1158, 304, 5149, 17473, 403, 1546, 14020, 7600, 29889, 13, 29911, 12966, 411, 278, 2114, 393, 3196, 11715, 10916, 526, 18425, 24146, 29892, 278, 9210, 29914, 978, 29899, 6707, 1158, 1033, 6459, 7824, 2983, 408, 3978, 1218, 515, 10557, 29889, 13, 8809, 488, 445, 5700, 29879, 1716, 982, 29892, 278, 4188, 22342, 310, 263, 4036, 2022, 17737, 17068, 304, 970, 775, 338, 1407, 1422, 1546, 7824, 10916, 29892, 599, 2534, 263, 1532, 29899, 4888, 287, 7047, 13661, 29892, 322, 11715, 10916, 393, 437, 451, 599, 6232, 445, 22917, 29889, 13, 13, 13, 29905, 26956, 29912, 1888, 29885, 16783, 29914, 331, 16783, 29913, 13, 13, 2744, 1228, 4038, 988, 263, 2788, 27791, 265, 1033, 367, 472, 1708, 338, 278, 14675, 310, 8068, 322, 4275, 6813, 29889, 13, 1323, 3224, 515, 445, 11758, 5120, 5692, 304, 367, 15678, 28325, 2354, 29889, 13, 1762, 24809, 565, 445, 338, 278, 1121, 310, 263, 24003, 9129, 491, 278, 1024, 29899, 6707, 15326, 591, 29537, 287, 278, 14675, 310, 9210, 29914, 978, 29899, 6707, 12827, 975, 931, 363, 15717, 5069, 4876, 5354, 338, 4249, 278, 2246, 29899, 841, 3148, 29899, 6707, 16212, 297, 4958, 310, 12463, 20706, 313, 342, 326, 630, 297, 2507, 491, 29537, 292, 278, 1556, 17091, 4876, 21904, 322, 7522, 18851, 1906, 23329, 304, 3148, 29899, 6707, 16212, 467, 13, 797, 29871, 29896, 29929, 29955, 29896, 694, 4148, 411, 385, 4876, 515, 2246, 3148, 29899, 6707, 16212, 338, 17809, 408, 23329, 304, 8068, 322, 4275, 6813, 29892, 13452, 297, 29871, 29906, 29900, 29896, 29929, 278, 11959, 338, 29871, 29896, 29906, 8958, 29889, 13, 10454, 328, 1036, 901, 1135, 697, 260, 9097, 310, 278, 2305, 4876, 29899, 21264, 630, 304, 2246, 3148, 29899, 6707, 16212, 505, 5972, 8068, 322, 4275, 3082, 2983, 29892, 607, 591, 13686, 408, 263, 5517, 17004, 310, 5198, 16783, 964, 3148, 313, 331, 16783, 515, 8068, 322, 4275, 6813, 467, 13, 23036, 5198, 16783, 756, 263, 1568, 5520, 4955, 1135, 825, 591, 526, 23382, 1244, 29892, 825, 591, 526, 16277, 292, 3117, 7805, 1472, 29899, 8489, 27721, 310, 372, 29892, 1316, 408, 1473, 322, 4654, 12623, 5198, 4481, 1934, 15723, 297, 4796, 29899, 1054, 4675, 17643, 29892, 1316, 408, 7047, 5849, 29889, 13, 13, 13, 13, 13, 320, 2042, 29912, 24445, 800, 322, 16367, 5244, 29913, 13, 29905, 1643, 29912, 3471, 29901, 535, 10085, 29913, 13, 13, 4806, 505, 8560, 385, 3902, 272, 7606, 29892, 3447, 1407, 2919, 6287, 29892, 29190, 936, 6559, 310, 278, 1737, 12122, 6894, 537, 297, 970, 775, 25741, 975, 931, 29889, 13, 4806, 505, 29537, 287, 29871, 29906, 29889, 29906, 24464, 29905, 13007, 970, 25741, 21653, 278, 320, 14573, 12883, 6069, 29914, 931, 3785, 29889, 13, 4806, 505, 1737, 324, 542, 630, 18777, 304, 320, 14573, 14058, 4597, 1080, 29914, 3186, 12786, 773, 408, 18470, 4876, 21904, 29892, 29431, 1283, 7224, 29892, 322, 4148, 2983, 29889, 13, 29949, 332, 1284, 886, 1510, 393, 278, 1737, 12122, 6894, 537, 297, 970, 775, 338, 10231, 975, 931, 29892, 322, 10902, 368, 577, 975, 278, 4940, 29871, 29906, 29900, 489, 29906, 29945, 2440, 29889, 13, 6039, 643, 1490, 534, 1975, 884, 1302, 29899, 542, 2764, 411, 15839, 4959, 322, 11758, 17292, 28342, 763, 278, 1095, 310, 278, 8291, 6415, 29129, 29892, 7910, 310, 14137, 4631, 4135, 2820, 278, 3186, 29892, 25539, 1608, 29892, 322, 5198, 16783, 29889, 13, 13, 13, 29905, 2168, 11014, 13, 29905, 7278, 29912, 24445, 800, 5003, 13, 4013, 6559, 337, 3687, 373, 263, 10296, 310, 1023, 1737, 324, 10610, 3519, 29901, 697, 2729, 373, 4876, 21904, 29892, 1790, 2729, 373, 9063, 17998, 1283, 7224, 322, 4148, 2983, 29889, 13, 4806, 15648, 777, 310, 278, 27028, 310, 2845, 1158, 297, 320, 29907, 999, 29912, 3471, 29901, 5696, 1118, 17385, 1218, 1749, 10608, 310, 9250, 292, 278, 671, 310, 278, 4876, 29899, 6707, 1158, 304, 25741, 411, 263, 5225, 17998, 9210, 29889, 13, 2887, 263, 17004, 29892, 363, 1556, 25741, 297, 278, 8783, 278, 9210, 29914, 978, 29899, 6707, 1158, 338, 1304, 29889, 13, 3047, 1316, 1158, 29892, 278, 29511, 310, 363, 264, 1280, 322, 269, 595, 1280, 526, 1304, 304, 7115, 14020, 20542, 393, 505, 263, 15878, 17998, 9210, 472, 9063, 931, 29889, 13, 13, 29909, 15031, 17004, 310, 445, 338, 393, 363, 25741, 411, 29892, 1827, 29892, 9210, 17998, 29974, 29900, 29929, 29901, 29900, 29900, 278, 14020, 7600, 508, 367, 12710, 29892, 5546, 322, 8314, 29892, 8679, 373, 278, 2702, 2635, 2861, 304, 2462, 4366, 14238, 931, 29889, 13, 12310, 1070, 363, 264, 1280, 322, 269, 595, 1280, 297, 1438, 12786, 10331, 304, 367, 3755, 1422, 577, 278, 4188, 22342, 310, 278, 1158, 304, 3867, 263, 23279, 15326, 338, 1880, 29889, 13, 2831, 916, 1283, 7224, 278, 731, 310, 5972, 363, 264, 1280, 322, 269, 595, 1280, 515, 14020, 20542, 508, 10371, 277, 901, 23228, 975, 14128, 29892, 3480, 6703, 10879, 292, 15326, 13600, 29889, 13, 4806, 505, 15648, 777, 310, 1438, 4251, 297, 320, 29907, 999, 29912, 3471, 29901, 9902, 1118, 541, 916, 1795, 367, 16850, 3241, 297, 278, 2582, 10879, 292, 8900, 534, 1975, 29889, 13, 13, 1576, 7348, 310, 773, 278, 4876, 29899, 6707, 1158, 363, 25741, 411, 5225, 17998, 9210, 29892, 322, 278, 9210, 29914, 978, 29899, 6707, 1158, 17551, 29892, 756, 6068, 502, 304, 6559, 599, 18777, 451, 2534, 263, 4234, 29899, 14940, 4876, 5354, 313, 617, 29911, 10249, 511, 541, 5304, 411, 278, 12045, 310, 1090, 29899, 276, 6338, 292, 278, 3186, 20542, 393, 505, 313, 262, 760, 322, 297, 777, 3064, 310, 278, 1629, 29897, 385, 3935, 17998, 9210, 310, 5225, 29889, 13, 13, 29909, 7037, 24003, 297, 445, 6559, 1033, 367, 9129, 491, 278, 2114, 393, 278, 1024, 2566, 1304, 363, 9210, 29914, 978, 29899, 6707, 1737, 324, 10610, 871, 3743, 2983, 8429, 773, 13548, 22968, 4890, 29889, 13, 4806, 5148, 363, 2983, 6943, 10013, 29892, 10369, 29892, 322, 22467, 4890, 297, 278, 2441, 8783, 29892, 9138, 871, 263, 3480, 3473, 1821, 5253, 310, 15717, 1058, 671, 1661, 29899, 13992, 262, 4890, 297, 1009, 478, 9295, 2983, 29892, 607, 11981, 502, 304, 4658, 393, 278, 10879, 310, 445, 2228, 338, 13114, 29889, 13, 13, 4806, 1258, 451, 3394, 10110, 2778, 3460, 313, 29872, 29889, 29887, 1696, 773, 2106, 29899, 974, 29899, 1552, 29899, 442, 8492, 763, 20025, 292, 29950, 271, 2651, 2036, 29912, 29885, 8085, 29877, 29906, 29900, 29896, 29929, 6605, 292, 2455, 9594, 541, 591, 437, 451, 2149, 445, 304, 367, 263, 7282, 2228, 1363, 29901, 313, 29874, 29897, 304, 14944, 24003, 297, 4148, 534, 1975, 278, 4978, 310, 10110, 2778, 2710, 2820, 278, 3186, 881, 367, 1597, 854, 29892, 607, 2444, 25057, 29936, 322, 313, 29890, 29897, 278, 8900, 9063, 534, 1975, 313, 4716, 723, 367, 1185, 7161, 287, 491, 10110, 2778, 3460, 29897, 526, 1407, 2788, 304, 8900, 4148, 534, 1975, 29889, 13, 13, 4806, 1258, 451, 1788, 19574, 3349, 2998, 9225, 15303, 2651, 2036, 29912, 280, 915, 1137, 29906, 29900, 29896, 29947, 2774, 29890, 1862, 29913, 515, 278, 4148, 8783, 29892, 541, 591, 1258, 1423, 363, 278, 10122, 310, 7047, 289, 1862, 4249, 278, 2246, 9063, 2153, 310, 1269, 1629, 29889, 1334, 871, 1476, 9078, 26695, 310, 9126, 13465, 313, 8426, 29897, 289, 1862, 29892, 1304, 19434, 304, 3345, 403, 10366, 25741, 29889, 2860, 11077, 25781, 289, 1862, 515, 278, 8783, 278, 8900, 5534, 534, 1975, 892, 443, 15033, 29892, 5480, 445, 5650, 22981, 443, 4572, 287, 848, 29889, 13, 13, 13, 29905, 2168, 11014, 13, 29905, 7278, 29912, 20154, 664, 5003, 13, 1762, 777, 15834, 278, 2038, 27028, 526, 278, 8666, 304, 5146, 304, 6559, 1316, 263, 2919, 8783, 29901, 727, 4864, 263, 11302, 29899, 2696, 1546, 2919, 29899, 7052, 7418, 322, 13600, 29889, 13, 4806, 3814, 1661, 621, 6393, 304, 4340, 23033, 322, 1380, 335, 403, 963, 297, 5434, 664, 29889, 13, 15329, 29899, 5696, 13501, 29892, 2778, 3460, 848, 1375, 292, 411, 5264, 10466, 3519, 29892, 1033, 367, 7436, 304, 3211, 777, 310, 278, 5155, 10425, 297, 445, 3902, 272, 7606, 6559, 29889, 13, 8809, 488, 896, 437, 451, 6287, 304, 278, 3353, 8783, 29892, 2473, 29899, 23515, 508, 367, 16356, 304, 4697, 25871, 964, 2702, 21420, 29892, 10816, 1906, 4475, 304, 5264, 17292, 28342, 29889, 13, 6295, 14093, 338, 263, 5264, 24238, 29892, 372, 338, 694, 4997, 393, 21420, 4475, 304, 5374, 542, 499, 3631, 14675, 11176, 479, 746, 29537, 292, 967, 14675, 472, 445, 6287, 29889, 13, 13, 13, 13, 13, 29871, 13, 29905, 8551, 3488, 13, 13, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[    1,   320,  2042,  ...,   424, 23460, 29889]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text1, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=1024)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.utils.data.dataloader.DataLoader at 0x2aadbd9897c0>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    with tqdm(enumerate(dataloader),\n",
    "                desc=f'Epoch {epoch}',\n",
    "                disable=not coordinator.is_master(),\n",
    "                total=num_steps_per_epoch,\n",
    "                initial=start_step) as pbar:\n",
    "        for step, batch in pbar:\n",
    "            batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs[0]\n",
    "            booster.backward(loss, optimizer)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            all_reduce_mean(loss)\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "            if coordinator.is_master():\n",
    "                writer.add_scalar('loss', loss.item(), epoch * num_steps_per_epoch + step)\n",
    "\n",
    "            if args.save_interval > 0 and (step + 1) % args.save_interval == 0:\n",
    "                coordinator.print_on_master(f'Saving checkpoint')\n",
    "                save(booster, model, optimizer, lr_scheduler, epoch, step + 1, args.batch_size, coordinator,\n",
    "                        args.save_dir)\n",
    "                coordinator.print_on_master(f'Saved checkpoint at epoch {epoch} step {step + 1}')\n",
    "    # the continue epochs are not resumed, so we need to reset the sampler start index and start step\n",
    "    dataloader.sampler.set_start_index(0)\n",
    "    start_step = 0\n",
    "\n",
    "coordinator.print_on_master(f'Max CUDA memory usage: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../../protgpt2/'\n",
    "\n",
    "tokenizer = Tokenizer.from_file(path+'tokenizer.json')\n",
    "# Load special tokens from JSON\n",
    "with open(path + 'special_tokens_map.json', 'r') as f:\n",
    "    special_tokens_map = json.load(f)\n",
    "\n",
    "# Extract actual tokens from the loaded map\n",
    "special_tokens_list = [\n",
    "    AddedToken(special_tokens_map.get('bos_token', {}).get('content'), single_word=False),\n",
    "    AddedToken(special_tokens_map.get('eos_token', {}).get('content'), single_word=False),\n",
    "    AddedToken(special_tokens_map.get('unk_token', {}).get('content'), single_word=False)\n",
    "]\n",
    "\n",
    "# Remove any tokens that are None or empty\n",
    "filtered_tokens = [token for token in special_tokens_list if token.content]\n",
    "\n",
    "tokenizer.add_special_tokens(filtered_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "bos_token_id  = tokenizer.token_to_id(\"<|endoftext|>\")\n",
    "print(bos_token_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'with_added_tokens': 'str' object cannot be converted to 'PyBool'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpadding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: argument 'with_added_tokens': 'str' object cannot be converted to 'PyBool'"
     ]
    }
   ],
   "source": [
    "tokenizer.get_vocab('padding')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"SLGPVADL\").ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "50257"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"<BOS>\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "[50257, 556, 48, 2459]"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.token_to_id(\"<BOS>\")] + encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string: ['SLG', 'P', 'VADL']\n",
      "Decoded string: SLGPVADL\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded string: {}\".format(encoding.tokens))\n",
    "\n",
    "decoded = tokenizer.decode(encoding.ids)\n",
    "print(\"Decoded string: {}\".format(decoded))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "['SLG', 'P', 'VADL']"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[556, 48, 2459]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Internal: /sentencepiece/python/bundled/sentencepiece/src/sentencepiece_processor.cc(848) [model_proto->ParseFromArray(serialized.data(), serialized.size())] ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentencepiece\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentencePieceProcessor\n\u001B[0;32m----> 2\u001B[0m sp_model \u001B[38;5;241m=\u001B[39m \u001B[43mSentencePieceProcessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtokenizer.json\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/sentencepiece/__init__.py:218\u001B[0m, in \u001B[0;36mSentencePieceProcessor.Init\u001B[0;34m(self, model_file, model_proto, out_type, add_bos, add_eos, reverse, enable_sampling, nbest_size, alpha)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_alpha \u001B[38;5;241m=\u001B[39m alpha\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_file \u001B[38;5;129;01mor\u001B[39;00m model_proto:\n\u001B[0;32m--> 218\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_proto\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_proto\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/sentencepiece/__init__.py:367\u001B[0m, in \u001B[0;36mSentencePieceProcessor.Load\u001B[0;34m(self, model_file, model_proto)\u001B[0m\n\u001B[1;32m    365\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_proto:\n\u001B[1;32m    366\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLoadFromSerializedProto(model_proto)\n\u001B[0;32m--> 367\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoadFromFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_file\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/sentencepiece/__init__.py:171\u001B[0m, in \u001B[0;36mSentencePieceProcessor.LoadFromFile\u001B[0;34m(self, arg)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mLoadFromFile\u001B[39m(\u001B[38;5;28mself\u001B[39m, arg):\n\u001B[0;32m--> 171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_sentencepiece\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSentencePieceProcessor_LoadFromFile\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Internal: /sentencepiece/python/bundled/sentencepiece/src/sentencepiece_processor.cc(848) [model_proto->ParseFromArray(serialized.data(), serialized.size())] "
     ]
    }
   ],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "sp_model = SentencePieceProcessor(model_file=path+'tokenizer.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}