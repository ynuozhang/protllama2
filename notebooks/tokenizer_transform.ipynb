{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "path = '/data/rozen/home/e0833634/lama/data/swiss_2023_9/'\n",
    "with open(path+\"uniprot_sprot.fasta\", \"r\") as fasta_file, open(\"/data/rozen/home/e0833634/lama/data/swiss_2023_9/protein_corpus.txt\", \"w\") as txt_file:\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        txt_file.write(str(record.seq) + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570157 /data/rozen/home/e0833634/lama/data/swiss_2023_9/protein_corpus.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l /data/rozen/home/e0833634/lama/data/swiss_2023_9/protein_corpus.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/data/rozen/home/e0833634/lama/data/swiss_2023_9/protein_corpus.txt --model_prefix=protein_10k --vocab_size=10000 --num_threads=20\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /data/rozen/home/e0833634/lama/data/swiss_2023_9/protein_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: protein_10k\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 20\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: /data/rozen/home/e0833634/lama/data/swiss_2023_9/protein_corpus.txt\n",
      "trainer_interface.cc(346) LOG(WARNING) Found too long line (5058 > 4192).\n",
      "trainer_interface.cc(348) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(349) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 569786 sentences\n",
      "trainer_interface.cc(381) LOG(INFO) Skipped 371 too long sentences.\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=204533134\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9957% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=21\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999957\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 569786 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 1000000 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 569786\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 481668\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 481668 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=858404 obj=984.76 num_tokens=39406929 num_tokens/piece=45.9072\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=802115 obj=968.757 num_tokens=39654462 num_tokens/piece=49.4374\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=601550 obj=969.988 num_tokens=40225256 num_tokens/piece=66.8693\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=601528 obj=967.51 num_tokens=40407840 num_tokens/piece=67.1753\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=451129 obj=971.265 num_tokens=41234630 num_tokens/piece=91.4032\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=451112 obj=967.847 num_tokens=41298571 num_tokens/piece=91.5484\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=338320 obj=974.34 num_tokens=42372630 num_tokens/piece=125.244\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=338304 obj=969.75 num_tokens=42400460 num_tokens/piece=125.332\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=253721 obj=978.331 num_tokens=43680620 num_tokens/piece=172.16\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=253714 obj=972.486 num_tokens=43697450 num_tokens/piece=172.231\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=190282 obj=982.138 num_tokens=45069446 num_tokens/piece=236.856\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=190278 obj=975.622 num_tokens=45086277 num_tokens/piece=236.95\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=142706 obj=985.384 num_tokens=46466124 num_tokens/piece=325.607\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=142702 obj=978.79 num_tokens=46487075 num_tokens/piece=325.763\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=107023 obj=988.847 num_tokens=47888258 num_tokens/piece=447.458\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=107020 obj=982.344 num_tokens=47915885 num_tokens/piece=447.728\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=80265 obj=992.971 num_tokens=49378550 num_tokens/piece=615.194\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=80261 obj=986.617 num_tokens=49412870 num_tokens/piece=615.652\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=60195 obj=1001.08 num_tokens=50940394 num_tokens/piece=846.256\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=60194 obj=995.393 num_tokens=50980078 num_tokens/piece=846.93\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=45145 obj=1011.45 num_tokens=52450703 num_tokens/piece=1161.83\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=45145 obj=1006.51 num_tokens=52491208 num_tokens/piece=1162.72\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=33858 obj=1019.18 num_tokens=53955740 num_tokens/piece=1593.59\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=33858 obj=1013.96 num_tokens=53997561 num_tokens/piece=1594.82\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=25393 obj=1024.55 num_tokens=55449629 num_tokens/piece=2183.66\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=25393 obj=1019.17 num_tokens=55494687 num_tokens/piece=2185.43\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=19044 obj=1028.88 num_tokens=56925033 num_tokens/piece=2989.13\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=19044 obj=1023.55 num_tokens=56975867 num_tokens/piece=2991.8\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=14283 obj=1032.23 num_tokens=58378454 num_tokens/piece=4087.27\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=14283 obj=1026.84 num_tokens=58437792 num_tokens/piece=4091.42\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=11000 obj=1034.47 num_tokens=59761404 num_tokens/piece=5432.85\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=11000 obj=1029.29 num_tokens=59823844 num_tokens/piece=5438.53\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: protein_10k.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: protein_10k.vocab\n"
     ]
    }
   ],
   "source": [
    "# Define the training arguments\n",
    "spm_args = '--input=/data/rozen/home/e0833634/lama/data/swiss_2023_9/protein_corpus.txt --model_prefix=protein_10k --vocab_size=10000 --num_threads=20'\n",
    "\n",
    "# Train the model\n",
    "spm.SentencePieceTrainer.Train(spm_args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sequence: ['▁MR', 'NFT', 'KQY', 'IN', 'GEW', 'VES', 'TSG', 'ETLE', 'VIN', 'PAT', 'EEV', 'AGT', 'IAK', 'GNK', 'EDVE', 'KAVE', 'AA', 'DNV', 'YLE', 'FRH', 'TSV', 'KER', 'QDLL', 'DQI', 'VQE', 'YKN', 'RK', 'EDLI', 'QA', 'ITD', 'ELGA', 'PLS', 'VA', 'ENV', 'HYQ', 'MGL', 'DHF', 'EAAR', 'DALN', 'DFQ', 'FEE', 'RRG', 'DDL', 'VV', 'KEAI', 'GVSG', 'LIT', 'PWN', 'FPT', 'NQT', 'SLK', 'LAA', 'AFA', 'AGS', 'PVV', 'FKP', 'SEE', 'TPF', 'AAI', 'IL', 'AEI', 'FDKV', 'GVP', 'KGV', 'FNL', 'VNG', 'DGQ', 'GVG', 'NPL', 'SEH', 'PKV', 'RMM', 'SFT', 'GSGP', 'TGSS', 'IMK', 'KAAE', 'DF', 'KKV', 'SLE', 'LG', 'GKS', 'PYI', 'ILD', 'DAD', 'IDG', 'AAS', 'AAA', 'NKV', 'VFN', 'TGQ', 'VCT', 'AG', 'TRT', 'IV', 'PAS', 'IKE', 'DFL', 'TAV', 'KEK', 'FSQ', 'VKV', 'GNP', 'REE', 'GTQ', 'VGP', 'IIS', 'KKQ', 'FDQ', 'VQA', 'YID', 'KGI', 'EEG', 'AE', 'LLY', 'GGPG', 'KPE', 'GLD', 'KGY', 'FAR', 'PTI', 'FNN', 'VDN', 'SM', 'TIA', 'QEE', 'IFGP', 'VM', 'SVI', 'TYN', 'DLD', 'EAIK', 'IAN', 'DTK', 'YGL', 'AGY', 'VYG', 'SDK', 'DTL', 'HKV', 'ARS', 'IEAG', 'TVE', 'INE', 'AGR', 'K', 'PDLP', 'FGG', 'YKQ', 'SGL', 'GRE', 'WGD', 'YGI', 'EEF', 'LEV', 'KSI', 'AG', 'YYN']\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='protein_10k.model')\n",
    "test_sequence = \"MRNFTKQYINGEWVESTSGETLEVINPATEEVAGTIAKGNKEDVEKAVEAADNVYLEFRHTSVKERQDLLDQIVQEYKNRKEDLIQAITDELGAPLSVAENVHYQMGLDHFEAARDALNDFQFEERRGDDLVVKEAIGVSGLITPWNFPTNQTSLKLAAAFAAGSPVVFKPSEETPFAAIILAEIFDKVGVPKGVFNLVNGDGQGVGNPLSEHPKVRMMSFTGSGPTGSSIMKKAAEDFKKVSLELGGKSPYIILDDADIDGAASAAANKVVFNTGQVCTAGTRTIVPASIKEDFLTAVKEKFSQVKVGNPREEGTQVGPIISKKQFDQVQAYIDKGIEEGAELLYGGPGKPEGLDKGYFARPTIFNNVDNSMTIAQEEIFGPVMSVITYNDLDEAIKIANDTKYGLAGYVYGSDKDTLHKVARSIEAGTVEINEAGRKPDLPFGGYKQSGLGREWGDYGIEEFLEVKSIAGYYN\"\n",
    "#Tokenize\n",
    "tokenized_sequence = sp.encode(test_sequence, out_type=str)\n",
    "print(\"Tokenized sequence:\", tokenized_sequence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detokenized sequence: MRNFTKQYINGEWVESTSGETLEVINPATEEVAGTIAKGNKEDVEKAVEAADNVYLEFRHTSVKERQDLLDQIVQEYKNRKEDLIQAITDELGAPLSVAENVHYQMGLDHFEAARDALNDFQFEERRGDDLVVKEAIGVSGLITPWNFPTNQTSLKLAAAFAAGSPVVFKPSEETPFAAIILAEIFDKVGVPKGVFNLVNGDGQGVGNPLSEHPKVRMMSFTGSGPTGSSIMKKAAEDFKKVSLELGGKSPYIILDDADIDGAASAAANKVVFNTGQVCTAGTRTIVPASIKEDFLTAVKEKFSQVKVGNPREEGTQVGPIISKKQFDQVQAYIDKGIEEGAELLYGGPGKPEGLDKGYFARPTIFNNVDNSMTIAQEEIFGPVMSVITYNDLDEAIKIANDTKYGLAGYVYGSDKDTLHKVARSIEAGTVEINEAGRKPDLPFGGYKQSGLGREWGDYGIEEFLEVKSIAGYYN\n"
     ]
    }
   ],
   "source": [
    "#Detokenize\n",
    "detokenized_sequence = sp.decode(tokenized_sequence)\n",
    "print(\"Detokenized sequence:\", detokenized_sequence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay!\n"
     ]
    }
   ],
   "source": [
    "# Check if the detokenized sequence matches the original sequence\n",
    "if test_sequence == detokenized_sequence:\n",
    "    print(\"Yay!\")\n",
    "else:\n",
    "    print(\"Wealp.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "path = '/data/rozen/home/e0833634/lama/data/swiss_2023_9/'\n",
    "with open(path+\"uniref50.fasta\", \"r\") as fasta_file, open(\"/data/rozen/home/e0833634/lama/data/swiss_2023_9/uniref50_seq_only.txt\", \"w\") as txt_file:\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        txt_file.write(str(record.seq) + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Strategy 1:\n",
    "1. tokenizer generated on swiss-prot confirmed protein sequences first\n",
    "2. identify overlapped sequences between uniref50 training data and the swiss-prot data\n",
    "3. keep them in the training set only\n",
    "4. randomly split the remaining (non-overlapped) sequences in the uniref50 into training and test sets\n",
    "5. 90%-10%\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels'],\n        num_rows: 8716143\n    })\n    validation: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels'],\n        num_rows: 1219069\n    })\n})"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('/data/rozen/home/e0833634/lama/protllama/uniprot_dataset.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "loaded_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import random\n",
    "path = '/data/rozen/home/e0833634/lama/data/swiss_2023_9/'\n",
    "with open(path+\"uniref50_seq_only.txt\", \"r\") as file:\n",
    "    uniref_sequences = [line.strip() for line in file.readlines()]\n",
    "with open(path+\"protein_corpus.txt\", \"r\") as file:\n",
    "    training_set_sequences = [line.strip() for line in file.readlines()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# O(1) compare to O(m*n) for loop\n",
    "# Convert training_set_sequences to a set for faster membership checking\n",
    "training_set = set(training_set_sequences)\n",
    "\n",
    "# Identify overlaps using set intersection\n",
    "overlapped_sequences = list(training_set.intersection(uniref_sequences))\n",
    "\n",
    "# Remove overlaps from the uniref_sequences list using set difference\n",
    "uniref_sequences = list(set(uniref_sequences).difference(overlapped_sequences))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "198983"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overlapped_sequences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "61950519"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniref_sequences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Identify overlaps\n",
    "overlapped_sequences = [seq for seq in uniref_sequences if seq in training_set_sequences]\n",
    "overlapped_sequences\n",
    "# Remove overlaps from the uniref_sequences list\n",
    "uniref_sequences = [seq for seq in uniref_sequences if seq not in overlapped_sequences]\n",
    "uniref_sequences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('overlapped_sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(overlapped_sequences, f)\n",
    "\n",
    "with open('uniref_sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(uniref_sequences, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "with open('overlapped_sequences.pkl', 'rb') as f:\n",
    "    overlapped_sequences = pickle.load(f)\n",
    "\n",
    "with open('uniref_sequences.pkl', 'rb') as f:\n",
    "    uniref_sequences = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Shuffle the non-overlapping sequences\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(uniref_sequences)\n",
    "\n",
    "# Split non-overlapping sequences\n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(uniref_sequences))\n",
    "train_non_overlap = uniref_sequences[:split_idx]\n",
    "test_sequences = uniref_sequences[split_idx:]\n",
    "\n",
    "# Combine overlapped sequences with training sequences\n",
    "final_train_sequences = overlapped_sequences + train_non_overlap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "55755467"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Save\n",
    "with open(\"train.txt\", \"w\") as file:\n",
    "    for seq in final_train_sequences:\n",
    "        file.write(seq + \"\\n\")\n",
    "\n",
    "with open(\"test.txt\", \"w\") as file:\n",
    "    for seq in test_sequences:\n",
    "        file.write(seq + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sequences', 'set'],\n    num_rows: 62149502\n})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "# Combining train and test datasets\n",
    "combined_data = {\n",
    "    'sequences': final_train_sequences + test_sequences,\n",
    "    'set': ['train'] * len(final_train_sequences) + ['valid'] * len(test_sequences)\n",
    "}\n",
    "combined_dataset = Dataset.from_dict(combined_data)\n",
    "combined_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving the combined dataset using pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"combined_dataset.pkl\", \"wb\") as file:\n",
    "    pickle.dump(combined_dataset, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "train_dataset = Dataset.from_dict({'sequences': final_train_sequences})\n",
    "test_dataset = Dataset.from_dict({'sequences': test_sequences})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'valid': test_dataset\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['sequences'],\n        num_rows: 55954450\n    })\n    valid: Dataset({\n        features: ['sequences'],\n        num_rows: 6195052\n    })\n})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Saving the dataset (0/33 shards):   0%|          | 0/55954450 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "edf0243e81694af9850458aacb622e4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/4 shards):   0%|          | 0/6195052 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9671f9f2a5cd439793e2de2b703c3d20"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#with open(\"/data/rozen/home/e0833634/lama/data/swiss_2023_9/uniref50_random90split.pkl\", \"wb\") as file:\n",
    "    #pickle.dump(dataset_dict, file)\n",
    "dataset_dict.save_to_disk('/data/rozen/home/e0833634/lama/data/swiss_2023_9/uniref50_random90split.hf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['sequences'],\n        num_rows: 55954450\n    })\n    valid: Dataset({\n        features: ['sequences'],\n        num_rows: 6195052\n    })\n})"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset_dict2 = load_from_disk('/data/rozen/home/e0833634/lama/data/swiss_2023_9/uniref50_random90split.hf')\n",
    "dataset_dict2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_dict2.load_from_disk('/data/rozen/home/e0833634/lama/data/swiss_2023_9/uniref50_random90split.pkl', format='pickle')\n",
    "dataset_dict2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Failed to open local file '/var/tmp/pbs.846141.hn-10-03/tmpgw4_63jo.arrow'. Detail: [errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/data/rozen/home/e0833634/lama/data/swiss_2023_9/uniref50_random90split.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m----> 3\u001B[0m     loaded_data \u001B[38;5;241m=\u001B[39m \u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/datasets/table.py:209\u001B[0m, in \u001B[0;36mTable.__setstate__\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m    207\u001B[0m filename \u001B[38;5;241m=\u001B[39m state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    208\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnpickling a big table from the disk at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 209\u001B[0m table \u001B[38;5;241m=\u001B[39m \u001B[43m_in_memory_arrow_table_from_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRemoving temporary table file at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    211\u001B[0m os\u001B[38;5;241m.\u001B[39mremove(filename)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/datasets/table.py:36\u001B[0m, in \u001B[0;36m_in_memory_arrow_table_from_file\u001B[0;34m(filename)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_in_memory_arrow_table_from_file\u001B[39m(filename: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pa\u001B[38;5;241m.\u001B[39mTable:\n\u001B[0;32m---> 36\u001B[0m     in_memory_stream \u001B[38;5;241m=\u001B[39m \u001B[43mpa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m     opened_stream \u001B[38;5;241m=\u001B[39m pa\u001B[38;5;241m.\u001B[39mipc\u001B[38;5;241m.\u001B[39mopen_stream(in_memory_stream)\n\u001B[1;32m     38\u001B[0m     pa_table \u001B[38;5;241m=\u001B[39m opened_stream\u001B[38;5;241m.\u001B[39mread_all()\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/pyarrow/io.pxi:2113\u001B[0m, in \u001B[0;36mpyarrow.lib.input_stream\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/pyarrow/io.pxi:928\u001B[0m, in \u001B[0;36mpyarrow.lib.OSFile.__cinit__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/pyarrow/io.pxi:938\u001B[0m, in \u001B[0;36mpyarrow.lib.OSFile._open_readable\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/pyarrow/error.pxi:144\u001B[0m, in \u001B[0;36mpyarrow.lib.pyarrow_internal_check_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/opt/rh/rh-python38/root/usr/local/lib64/python3.8/site-packages/pyarrow/error.pxi:113\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] Failed to open local file '/var/tmp/pbs.846141.hn-10-03/tmpgw4_63jo.arrow'. Detail: [errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('/data/rozen/home/e0833634/lama/data/swiss_2023_9/uniref50_random90split.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rozen/home/e0833634/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1756: FutureWarning: Calling LlamaTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.llama.tokenization_llama import LlamaTokenizer\n",
    "t = LlamaTokenizer.from_pretrained(tokenizer_path+'protein_10k.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomSentencePieceTokenizer' object has no attribute 'model_max_length'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m test_sequence \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMRNFTKQYINGEWVESTSGETLEVINPATEEVAGTIAKGNKEDVEKAVEAADNVYLEFRHTSVKERQDLLDQIVQEYKNRKEDLIQAITDELGAPLSVAENVHYQMGLDHFEAARDALNDFQFEERRGDDLVVKEAIGVSGLITPWNFPTNQTSLKLAAAFAAGSPVVFKPSEETPFAAIILAEIFDKVGVPKGVFNLVNGDGQGVGNPLSEHPKVRMMSFTGSGPTGSSIMKKAAEDFKKVSLELGGKSPYIILDDADIDGAASAAANKVVFNTGQVCTAGTRTIVPASIKEDFLTAVKEKFSQVKVGNPREEGTQVGPIISKKQFDQVQAYIDKGIEEGAELLYGGPGKPEGLDKGYFARPTIFNNVDNSMTIAQEEIFGPVMSVITYNDLDEAIKIANDTKYGLAGYVYGSDKDTLHKVARSIEAGTVEINEAGRKPDLPFGGYKQSGLGREWGDYGIEEFLEVKSIAGYYN\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#Tokenize\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m tokenized_sequence \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_sequence\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTokenized sequence:\u001B[39m\u001B[38;5;124m\"\u001B[39m, tokenized_sequence)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2373\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.encode\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001B[0m\n\u001B[1;32m   2336\u001B[0m \u001B[38;5;129m@add_end_docstrings\u001B[39m(\n\u001B[1;32m   2337\u001B[0m     ENCODE_KWARGS_DOCSTRING,\n\u001B[1;32m   2338\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2357\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m   2358\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2359\u001B[0m \u001B[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001B[39;00m\n\u001B[1;32m   2360\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2371\u001B[0m \u001B[38;5;124;03m            method).\u001B[39;00m\n\u001B[1;32m   2372\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2373\u001B[0m     encoded_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2374\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2375\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2376\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2377\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2378\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2379\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2380\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2381\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2382\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2383\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m encoded_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2781\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2771\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[1;32m   2772\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[1;32m   2773\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   2774\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2778\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2779\u001B[0m )\n\u001B[0;32m-> 2781\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2782\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2783\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2784\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2785\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2786\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2787\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2788\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2789\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2792\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2799\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2800\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py:659\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    656\u001B[0m first_ids \u001B[38;5;241m=\u001B[39m get_input_ids(text)\n\u001B[1;32m    657\u001B[0m second_ids \u001B[38;5;241m=\u001B[39m get_input_ids(text_pair) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 659\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_for_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    660\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfirst_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    661\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpair_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msecond_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    662\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    663\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprepend_batch_axis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3256\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.prepare_for_model\u001B[0;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001B[0m\n\u001B[1;32m   3253\u001B[0m         encoded_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspecial_tokens_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(sequence)\n\u001B[1;32m   3255\u001B[0m \u001B[38;5;66;03m# Check lengths\u001B[39;00m\n\u001B[0;32m-> 3256\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_eventual_warn_about_too_long_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencoded_inputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3258\u001B[0m \u001B[38;5;66;03m# Padding\u001B[39;00m\n\u001B[1;32m   3259\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m padding_strategy \u001B[38;5;241m!=\u001B[39m PaddingStrategy\u001B[38;5;241m.\u001B[39mDO_NOT_PAD \u001B[38;5;129;01mor\u001B[39;00m return_attention_mask:\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3633\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._eventual_warn_about_too_long_sequence\u001B[0;34m(self, ids, max_length, verbose)\u001B[0m\n\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_eventual_warn_about_too_long_sequence\u001B[39m(\u001B[38;5;28mself\u001B[39m, ids: List[\u001B[38;5;28mint\u001B[39m], max_length: Optional[\u001B[38;5;28mint\u001B[39m], verbose: \u001B[38;5;28mbool\u001B[39m):\n\u001B[1;32m   3623\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3624\u001B[0m \u001B[38;5;124;03m    Depending on the input and internal state we might trigger a warning about a sequence that is too long for its\u001B[39;00m\n\u001B[1;32m   3625\u001B[0m \u001B[38;5;124;03m    corresponding model\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3631\u001B[0m \n\u001B[1;32m   3632\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3633\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m max_length \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(ids) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_max_length\u001B[49m \u001B[38;5;129;01mand\u001B[39;00m verbose:\n\u001B[1;32m   3634\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeprecation_warnings\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msequence-length-is-longer-than-the-specified-maximum\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   3635\u001B[0m             logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m   3636\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mToken indices sequence length is longer than the specified maximum sequence length \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3637\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor this model (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(ids)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m > \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_max_length\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m). Running this sequence through the model \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3638\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwill result in indexing errors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3639\u001B[0m             )\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'CustomSentencePieceTokenizer' object has no attribute 'model_max_length'"
     ]
    }
   ],
   "source": [
    "test_sequence = \"MRNFTKQYINGEWVESTSGETLEVINPATEEVAGTIAKGNKEDVEKAVEAADNVYLEFRHTSVKERQDLLDQIVQEYKNRKEDLIQAITDELGAPLSVAENVHYQMGLDHFEAARDALNDFQFEERRGDDLVVKEAIGVSGLITPWNFPTNQTSLKLAAAFAAGSPVVFKPSEETPFAAIILAEIFDKVGVPKGVFNLVNGDGQGVGNPLSEHPKVRMMSFTGSGPTGSSIMKKAAEDFKKVSLELGGKSPYIILDDADIDGAASAAANKVVFNTGQVCTAGTRTIVPASIKEDFLTAVKEKFSQVKVGNPREEGTQVGPIISKKQFDQVQAYIDKGIEEGAELLYGGPGKPEGLDKGYFARPTIFNNVDNSMTIAQEEIFGPVMSVITYNDLDEAIKIANDTKYGLAGYVYGSDKDTLHKVARSIEAGTVEINEAGRKPDLPFGGYKQSGLGREWGDYGIEEFLEVKSIAGYYN\"\n",
    "#Tokenize\n",
    "tokenized_sequence = t.encode(test_sequence)\n",
    "print(\"Tokenized sequence:\", tokenized_sequence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class CustomSentencePieceTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, model_path, model_max_length=1024):\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(model_path)\n",
    "        self._unk_token = self.sp.IdToPiece(self.sp.unk_id())\n",
    "        self._pad_token = self._unk_token  # Setting pad token to unk token\n",
    "        self.model_max_length = model_max_length\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return self.sp.EncodeAsPieces(text)\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.sp.PieceToId(token) for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.sp.IdToPiece(id_) for id_ in ids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sp.GetPieceSize()\n",
    "\n",
    "    # Getters and Setters for pad_token and unk_token\n",
    "    @property\n",
    "    def pad_token(self):\n",
    "        return self._pad_token\n",
    "\n",
    "    @pad_token.setter\n",
    "    def pad_token(self, value):\n",
    "        self._pad_token = value\n",
    "\n",
    "    @property\n",
    "    def unk_token(self):\n",
    "        return self._unk_token\n",
    "\n",
    "    @unk_token.setter\n",
    "    def unk_token(self, value):\n",
    "        self._unk_token = value\n",
    "\n",
    "# Load your custom tokenizer\n",
    "tokenizer_path = '/data/rozen/home/e0833634/lama/protllama/batch_script/'\n",
    "# Load your custom tokenizer\n",
    "tokenizer = CustomSentencePieceTokenizer(tokenizer_path+'protein_10k.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sequence: [600, 3073, 3973, 144, 4897, 1161, 1370, 7004, 1982, 1968, 501, 853, 1283, 4500, 7699, 6071, 21, 1882, 3714, 5764, 1848, 2229, 9233, 2263, 2578, 2889, 132, 7956, 84, 898, 8822, 826, 24, 1378, 8412, 1142, 6470, 4127, 8493, 4231, 1931, 1299, 949, 34, 8901, 9592, 1382, 8314, 5218, 4063, 1649, 334, 516, 588, 863, 3086, 992, 3330, 552, 36, 829, 9522, 964, 1600, 3130, 1785, 2584, 1769, 1198, 4797, 2909, 7823, 2163, 8400, 9048, 5112, 5936, 197, 1386, 623, 48, 1057, 3354, 1401, 1680, 1128, 444, 310, 2658, 3037, 2492, 5400, 38, 2125, 58, 943, 523, 1380, 1193, 497, 3048, 1162, 1538, 1629, 5225, 2514, 956, 3063, 4399, 2384, 3367, 724, 985, 32, 2188, 6082, 1215, 1246, 1904, 2177, 3486, 4099, 3494, 288, 810, 1663, 7539, 215, 1001, 4149, 802, 5671, 1415, 4527, 1809, 1731, 2376, 1845, 969, 5492, 2173, 8077, 925, 989, 750, 10, 7962, 579, 6248, 394, 3244, 9142, 2091, 1363, 1653, 1432, 38, 3683]\n"
     ]
    }
   ],
   "source": [
    "test_sequence = \"MRNFTKQYINGEWVESTSGETLEVINPATEEVAGTIAKGNKEDVEKAVEAADNVYLEFRHTSVKERQDLLDQIVQEYKNRKEDLIQAITDELGAPLSVAENVHYQMGLDHFEAARDALNDFQFEERRGDDLVVKEAIGVSGLITPWNFPTNQTSLKLAAAFAAGSPVVFKPSEETPFAAIILAEIFDKVGVPKGVFNLVNGDGQGVGNPLSEHPKVRMMSFTGSGPTGSSIMKKAAEDFKKVSLELGGKSPYIILDDADIDGAASAAANKVVFNTGQVCTAGTRTIVPASIKEDFLTAVKEKFSQVKVGNPREEGTQVGPIISKKQFDQVQAYIDKGIEEGAELLYGGPGKPEGLDKGYFARPTIFNNVDNSMTIAQEEIFGPVMSVITYNDLDEAIKIANDTKYGLAGYVYGSDKDTLHKVARSIEAGTVEINEAGRKPDLPFGGYKQSGLGREWGDYGIEEFLEVKSIAGYYN\"\n",
    "#Tokenize\n",
    "tokenized_sequence = tokenizer.encode(test_sequence)\n",
    "print(\"Tokenized sequence:\", tokenized_sequence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-27 14:00:11.589848: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "convert_ids_to_tokens() got an unexpected keyword argument 'skip_special_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m detokenized_sequence \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenized_sequence\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDetokenized sequence:\u001B[39m\u001B[38;5;124m\"\u001B[39m, detokenized_sequence)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3550\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.decode\u001B[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[1;32m   3547\u001B[0m \u001B[38;5;66;03m# Convert inputs to python lists\u001B[39;00m\n\u001B[1;32m   3548\u001B[0m token_ids \u001B[38;5;241m=\u001B[39m to_py_obj(token_ids)\n\u001B[0;32m-> 3550\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_decode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3551\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3552\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3553\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3554\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3555\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py:938\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._decode\u001B[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m    928\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_decode\u001B[39m(\n\u001B[1;32m    929\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    930\u001B[0m     token_ids: List[\u001B[38;5;28mint\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    934\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    935\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    936\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decode_use_source_tokenizer \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_source_tokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m--> 938\u001B[0m     filtered_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_ids_to_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    940\u001B[0m     \u001B[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001B[39;00m\n\u001B[1;32m    941\u001B[0m     \u001B[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001B[39;00m\n\u001B[1;32m    942\u001B[0m     \u001B[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001B[39;00m\n\u001B[1;32m    943\u001B[0m     sub_texts \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mTypeError\u001B[0m: convert_ids_to_tokens() got an unexpected keyword argument 'skip_special_tokens'"
     ]
    }
   ],
   "source": [
    "detokenized_sequence = tokenizer.decode(tokenized_sequence)\n",
    "print(\"Detokenized sequence:\", detokenized_sequence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if the detokenized sequence matches the original sequence\n",
    "if test_sequence == detokenized_sequence:\n",
    "    print(\"Yay!\")\n",
    "else:\n",
    "    print(\"Wealp.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Identify overlaps\n",
    "overlapped_sequences = [seq for seq in uniref_sequences if seq in training_set_sequences]\n",
    "\n",
    "# Remove overlaps from the uniref_sequences list\n",
    "uniref_sequences = [seq for seq in uniref_sequences if seq not in overlapped_sequences]\n",
    "\n",
    "# Shuffle the non-overlapping sequences\n",
    "random.shuffle(uniref_sequences)\n",
    "\n",
    "# Split non-overlapping sequences\n",
    "train_ratio = 0.8\n",
    "split_idx = int(train_ratio * len(uniref_sequences))\n",
    "train_non_overlap = uniref_sequences[:split_idx]\n",
    "test_sequences = uniref_sequences[split_idx:]\n",
    "\n",
    "# Combine overlapped sequences with training sequences\n",
    "final_train_sequences = overlapped_sequences + train_non_overlap\n",
    "\n",
    "# Save\n",
    "with open(\"train.txt\", \"w\") as file:\n",
    "    for seq in final_train_sequences:\n",
    "        file.write(seq + \"\\n\")\n",
    "\n",
    "with open(\"test.txt\", \"w\") as file:\n",
    "    for seq in test_sequences:\n",
    "        file.write(seq + \"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with open(path+\"uniref50.fasta\", \"r\") as file:\n",
    "    sequences = file.readlines()  # adjust this based on your file's format\n",
    "\n",
    "# Identify overlaps\n",
    "overlapped_sequences = [seq for seq in sequences if seq in tokenizer_training_set]\n",
    "\n",
    "# Remove overlaps from the sequences list\n",
    "sequences = [seq for seq in sequences if seq not in overlapped_sequences]\n",
    "\n",
    "# Shuffle the non-overlapping sequences\n",
    "random.shuffle(sequences)\n",
    "\n",
    "# Split non-overlapping sequences\n",
    "train_ratio = 0.8\n",
    "split_idx = int(train_ratio * len(sequences))\n",
    "train_non_overlap = sequences[:split_idx]\n",
    "test_sequences = sequences[split_idx:]\n",
    "\n",
    "# Combine overlapped sequences with training sequences\n",
    "final_train_sequences = overlapped_sequences + train_non_overlap\n",
    "\n",
    "# Save\n",
    "with open(\"train.fasta\", \"w\") as file:\n",
    "    file.writelines(final_train_sequences)\n",
    "\n",
    "with open(\"test.fasta\", \"w\") as file:\n",
    "    file.writelines(test_sequences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "!export PATH=\"$PATH:/data/rozen/home/e0833634/.local/bin/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: spm_train: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!spm_train --input=protein_corpus.txt --model_prefix=protein --vocab_size=32000 --character_coverage=1.0 --model_type=bpe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2Tokenizer(name_or_path='/data/rozen/home/e0833634/lama/protgpt2', vocab_size=50257, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bpe = GPT2Tokenizer.from_pretrained(\"/data/rozen/home/e0833634/lama/protgpt2\")\n",
    "tokenizer_bpe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/data/rozen/home/e0833634/lama/protllama/uniprot_dataset.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['attention_mask', 'input_ids', 'labels'],\n    num_rows: 8716143\n})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = loaded_data['train']\n",
    "train_ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/8716143 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b40d88a15834923a4372c6180f76941"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 20:06:46.464996: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdetokenize_function\u001B[39m(examples):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m: tokenizer_bpe\u001B[38;5;241m.\u001B[39mdecode(examples[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)}\n\u001B[0;32m----> 4\u001B[0m detokenized_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_ds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdetokenize_function\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:592\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    590\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    591\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 592\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    593\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    594\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[1;32m    595\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:557\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    550\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    551\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    552\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    555\u001B[0m }\n\u001B[1;32m    556\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 557\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    558\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    559\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:3097\u001B[0m, in \u001B[0;36mDataset.map\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   3090\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3091\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mtqdm(\n\u001B[1;32m   3092\u001B[0m         disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mis_progress_bar_enabled(),\n\u001B[1;32m   3093\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3094\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[1;32m   3095\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3096\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[0;32m-> 3097\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m Dataset\u001B[38;5;241m.\u001B[39m_map_single(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdataset_kwargs):\n\u001B[1;32m   3098\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[1;32m   3099\u001B[0m                 shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:3450\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[1;32m   3448\u001B[0m _time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m   3449\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, example \u001B[38;5;129;01min\u001B[39;00m shard_iterable:\n\u001B[0;32m-> 3450\u001B[0m     example \u001B[38;5;241m=\u001B[39m \u001B[43mapply_function_on_filtered_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3451\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m update_data:\n\u001B[1;32m   3452\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:3353\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001B[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001B[0m\n\u001B[1;32m   3351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_rank:\n\u001B[1;32m   3352\u001B[0m     additional_args \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (rank,)\n\u001B[0;32m-> 3353\u001B[0m processed_inputs \u001B[38;5;241m=\u001B[39m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfn_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43madditional_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, LazyDict):\n\u001B[1;32m   3355\u001B[0m     processed_inputs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m   3356\u001B[0m         k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mkeys_to_format\n\u001B[1;32m   3357\u001B[0m     }\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mdetokenize_function\u001B[0;34m(examples)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdetokenize_function\u001B[39m(examples):\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mtokenizer_bpe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexamples\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m}\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3550\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.decode\u001B[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[1;32m   3547\u001B[0m \u001B[38;5;66;03m# Convert inputs to python lists\u001B[39;00m\n\u001B[1;32m   3548\u001B[0m token_ids \u001B[38;5;241m=\u001B[39m to_py_obj(token_ids)\n\u001B[0;32m-> 3550\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_decode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3551\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3552\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3553\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3554\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3555\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py:938\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._decode\u001B[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m    928\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_decode\u001B[39m(\n\u001B[1;32m    929\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    930\u001B[0m     token_ids: List[\u001B[38;5;28mint\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    934\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    935\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    936\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decode_use_source_tokenizer \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_source_tokenizer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m--> 938\u001B[0m     filtered_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_ids_to_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    940\u001B[0m     \u001B[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001B[39;00m\n\u001B[1;32m    941\u001B[0m     \u001B[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001B[39;00m\n\u001B[1;32m    942\u001B[0m     \u001B[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001B[39;00m\n\u001B[1;32m    943\u001B[0m     sub_texts \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py:914\u001B[0m, in \u001B[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001B[0;34m(self, ids, skip_special_tokens)\u001B[0m\n\u001B[1;32m    912\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m ids:\n\u001B[1;32m    913\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(index)\n\u001B[0;32m--> 914\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m skip_special_tokens \u001B[38;5;129;01mand\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_special_ids\u001B[49m:\n\u001B[1;32m    915\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m    916\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madded_tokens_decoder:\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1316\u001B[0m, in \u001B[0;36mSpecialTokensMixin.all_special_ids\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1311\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m   1312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mall_special_ids\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m   1313\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1314\u001B[0m \u001B[38;5;124;03m    `List[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\u001B[39;00m\n\u001B[1;32m   1315\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1316\u001B[0m     all_toks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_special_tokens\u001B[49m\n\u001B[1;32m   1317\u001B[0m     all_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_tokens_to_ids(all_toks)\n\u001B[1;32m   1318\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m all_ids\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1292\u001B[0m, in \u001B[0;36mSpecialTokensMixin.all_special_tokens\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1285\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m   1286\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mall_special_tokens\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m   1287\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1288\u001B[0m \u001B[38;5;124;03m    `List[str]`: All the special tokens (`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\u001B[39;00m\n\u001B[1;32m   1289\u001B[0m \n\u001B[1;32m   1290\u001B[0m \u001B[38;5;124;03m    Convert tokens of `tokenizers.AddedToken` type to string.\u001B[39;00m\n\u001B[1;32m   1291\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1292\u001B[0m     all_toks \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mstr\u001B[39m(s) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_special_tokens_extended\u001B[49m]\n\u001B[1;32m   1293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m all_toks\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1308\u001B[0m, in \u001B[0;36mSpecialTokensMixin.all_special_tokens_extended\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1306\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m attr_value \u001B[38;5;129;01min\u001B[39;00m set_attr\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[1;32m   1307\u001B[0m     all_toks \u001B[38;5;241m=\u001B[39m all_toks \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mlist\u001B[39m(attr_value) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(attr_value, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)) \u001B[38;5;28;01melse\u001B[39;00m [attr_value])\n\u001B[0;32m-> 1308\u001B[0m all_toks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[43mOrderedDict\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfromkeys\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_toks\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1309\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m all_toks\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def detokenize_function(examples):\n",
    "    return {\"text\": tokenizer_bpe.decode(examples['input_ids'], skip_special_tokens=True)}\n",
    "\n",
    "detokenized_dataset = train_ds.map(detokenize_function)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('path_to_sentencepiece_model.model')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return {\"input_ids\": sp.EncodeAsIds(examples['text'])}\n",
    "\n",
    "dataset_tokenized_sp = detokenized_dataset.map(tokenize_function)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}