{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sequences'],\n    num_rows: 4\n})"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import sentencepiece as spm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "text = ['GLTNAFIASAPAREVRYDGVITPANANYRFMGGDKGGSLTVGSHLTGSNMVTIGPMGVVVFTNNNDYTGNTFIMGGGTLQLGSNTAWGSLPN\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFPLSPAQLGIWYAQHLDPQVPITIAQYVDLHGALDVEVLERASIDASHELGSGFLRIVERDGEPLQYV\\n',\n",
    "        'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFP\\n',\n",
    "        'MDRLDFGGGE\\n']\n",
    "train_dataset = Dataset.from_dict({'sequences': text})\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "class FastaBatchedDataset(object):\n",
    "    \"\"\"inspired by esm2, but instead of sorting the original sequences,\n",
    "    we should really sorting based on tokenized sequences\n",
    "    \"\"\"\n",
    "    def __init__(self,sequence_strs, tokenizer, max_sequence_length):\n",
    "        self.sequence_strs = sequence_strs['sequences']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        raw_tokenized_sequences = [self.tokenizer.encode(s) for s in self.sequence_strs]\n",
    "        self.tokenized_sequences = []\n",
    "        for tokens in raw_tokenized_sequences:\n",
    "            if len(tokens) >= self.max_sequence_length-1:\n",
    "                # considering the added special token bos\n",
    "                sampled_windows = self.sample_windows(tokens, self.max_sequence_length)\n",
    "                for sample in sampled_windows:\n",
    "                    sample.insert(0, self.tokenizer.bos_id())\n",
    "                    self.tokenized_sequences.append(sample)\n",
    "            else:\n",
    "                tokens.insert(0, self.tokenizer.bos_id())\n",
    "                self.tokenized_sequences.append(tokens)\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_windows(sequence, max_sequence_length, extra_toks_per_seq=1):\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        \"\"\"based on Phil's implement\n",
    "        Returns: a list of sampled windows.\n",
    "        \"\"\"\n",
    "        sampled_windows = []\n",
    "        # the beginning window\n",
    "        sampled_windows.append(sequence[:max_sequence_length-extra_toks_per_seq])\n",
    "        # calculate the num of random slices needed, remove head and tail\n",
    "        num_slices_required = (len(sequence) // max_sequence_length) - 2\n",
    "        max_start_index = len(sequence) - max_sequence_length\n",
    "\n",
    "        if max_start_index < 0:\n",
    "            raise ValueError(\"max_sequence_length greater than length of sequence\")\n",
    "\n",
    "        if num_slices_required > 0:\n",
    "            for _ in range(num_slices_required):\n",
    "                # Randomly select start index of the window\n",
    "                start_index = random.randint(0, max_start_index)\n",
    "                # Extract window\n",
    "                # Considering the added special token bos\n",
    "                window = sequence[start_index:start_index + max_sequence_length-extra_toks_per_seq]\n",
    "                # Append the window to the list of sampled windows\n",
    "                sampled_windows.append(window)\n",
    "\n",
    "        # the end window\n",
    "        sampled_windows.append(sequence[-(max_sequence_length-extra_toks_per_seq):])\n",
    "        return sampled_windows\n",
    "\n",
    "    def get_batch_indices(self, extra_toks_per_seq=1):\n",
    "        print(self.sequence_strs)\n",
    "        sizes = [(len(tokens), i) for i, tokens in enumerate(self.tokenized_sequences)]\n",
    "        sizes.sort()\n",
    "        print(sizes)\n",
    "        batches = []\n",
    "        buf = []\n",
    "        current_buf_len = 0\n",
    "\n",
    "        def _flush_current_buf():\n",
    "            nonlocal current_buf_len, buf\n",
    "            if len(buf) == 0:\n",
    "                return\n",
    "            batches.append(buf)\n",
    "            buf = []\n",
    "            current_buf_len = 0\n",
    "            #print('my batches is:')\n",
    "            #print(batches)\n",
    "\n",
    "        for sz, i in sizes:\n",
    "            # considering the extra bos\n",
    "            sz += extra_toks_per_seq\n",
    "            # check accumulative seq length in the buffer\n",
    "            if current_buf_len + sz > self.max_sequence_length:\n",
    "                _flush_current_buf()\n",
    "            buf.append(i)\n",
    "            current_buf_len += sz\n",
    "            #print('my buffer is:')\n",
    "            #print(buf)\n",
    "\n",
    "        _flush_current_buf()\n",
    "        return batches\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_sequences[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "class BatchConverter(object):\n",
    "    \"\"\"add padding, create labels for GPT-alike training, used as collate_fn, need processed batch indices\n",
    "    processed (labels + tensor) batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        self.pad_token_id = tokenizer.pad_id()\n",
    "\n",
    "    def __call__(self, batches):\n",
    "        data_tokens = [torch.tensor(token_list) for token_list in batches]\n",
    "        data_tokens_padded = pad_sequence(data_tokens, batch_first=True, padding_value=self.pad_token_id)\n",
    "\n",
    "        # Create attention masks\n",
    "        attention_masks = (data_tokens_padded != self.pad_token_id).long()\n",
    "\n",
    "        # skip label==-100 during training so that these tokens won't be used in loss calculation\n",
    "        labels = data_tokens_padded.clone()\n",
    "        labels[data_tokens_padded == self.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': data_tokens_padded,\n",
    "            'attention_mask': attention_masks,\n",
    "            'labels': labels\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "tokenizer_path = '/data/rozen/home/e0833634/lama/protllama/batch_script/'\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path+\"protein_8k.model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GLTNAFIASAPAREVRYDGVITPANANYRFMGGDKGGSLTVGSHLTGSNMVTIGPMGVVVFTNNNDYTGNTFIMGGGTLQLGSNTAWGSLPN\\n', 'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFPLSPAQLGIWYAQHLDPQVPITIAQYVDLHGALDVEVLERASIDASHELGSGFLRIVERDGEPLQYV\\n', 'MDRLDFGGNGEAGSEVAPVPVSGQPVSSEQLFP\\n', 'MDRLDFGGGE\\n']\n",
      "[(5, 8), (10, 0), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (10, 6), (10, 7)]\n"
     ]
    }
   ],
   "source": [
    "dataset = FastaBatchedDataset(train_dataset, tokenizer, 10)\n",
    "batches = dataset.get_batch_indices()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, collate_fn=BatchConverter(tokenizer),\n",
    "                                          batch_sampler=batches, pin_memory=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "[[8], [0], [1], [2], [3], [4], [5], [6], [7]]"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'input_ids': tensor([[   1,  854, 1642,  653,   91]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'labels': tensor([[   1,  854, 1642,  653,   91]])}\n",
      "1 {'input_ids': tensor([[   1,  820,  814, 3992,   37, 1313, 7791,  245, 7689,  701]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1,  820,  814, 3992,   37, 1313, 7791,  245, 7689,  701]])}\n",
      "2 {'input_ids': tensor([[   1, 2844, 3768, 4240, 2298, 3576, 1302,  828,  114, 2232]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1, 2844, 3768, 4240, 2298, 3576, 1302,  828,  114, 2232]])}\n",
      "3 {'input_ids': tensor([[   1,   37, 1313, 7791,  245, 7689,  701, 4163, 6182, 1302]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1,   37, 1313, 7791,  245, 7689,  701, 4163, 6182, 1302]])}\n",
      "4 {'input_ids': tensor([[   1,  820,  814, 3992,   37, 1313, 7791,  245, 7689,  701]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1,  820,  814, 3992,   37, 1313, 7791,  245, 7689,  701]])}\n",
      "5 {'input_ids': tensor([[   1, 2298, 3576, 1302,  828,  114, 2232,   43, 6101, 1894]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1, 2298, 3576, 1302,  828,  114, 2232,   43, 6101, 1894]])}\n",
      "6 {'input_ids': tensor([[   1,  854, 1642,  653, 1279,  639,   86, 1195, 1214, 6817]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1,  854, 1642,  653, 1279,  639,   86, 1195, 1214, 6817]])}\n",
      "7 {'input_ids': tensor([[   1, 3399,  760, 1197,  614, 2013, 2199, 2572,  688, 2203]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1, 3399,  760, 1197,  614, 2013, 2199, 2572,  688, 2203]])}\n",
      "8 {'input_ids': tensor([[   1, 1279,  639,   86, 1195, 1214, 6817, 1950, 3101, 7696]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1, 1279,  639,   86, 1195, 1214, 6817, 1950, 3101, 7696]])}\n",
      "9 {'input_ids': tensor([[   1,  854, 1642,  653, 1279,  639,   86, 1195, 1214, 6817]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1,  854, 1642,  653, 1279,  639,   86, 1195, 1214, 6817]])}\n",
      "10 {'input_ids': tensor([[   1, 2013, 2199, 2572,  688, 2203, 1363,  890, 1369, 5521]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1, 2013, 2199, 2572,  688, 2203, 1363,  890, 1369, 5521]])}\n",
      "11 {'input_ids': tensor([[   1,  854, 1642,  653, 1279,  639,   86, 1195, 1214, 6817]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1,  854, 1642,  653, 1279,  639,   86, 1195, 1214, 6817]])}\n",
      "12 {'input_ids': tensor([[   1, 1279,  639,   86, 1195, 1214, 6817,   49,  141, 2076]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   1, 1279,  639,   86, 1195, 1214, 6817,   49,  141, 2076]])}\n"
     ]
    }
   ],
   "source": [
    "for batch_id, batch in enumerate(dataloader):\n",
    "    print(batch_id, batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[820, 814, 3992, 37, 1313, 7791, 245, 7689, 701, 4163, 6182, 1302, 4844, 7660, 1104, 3455, 2232, 4290, 2886, 2351, 2844, 3768, 4240, 2298, 3576, 1302, 828, 114, 2232, 43, 6101, 1894] 32\n",
      "[854, 1642, 653, 1279, 639, 86, 1195, 1214, 6817, 1950, 3101, 7696, 1783, 6728, 3777, 2132, 2949, 2596, 1868, 2403, 3399, 760, 1197, 614, 2013, 2199, 2572, 688, 2203, 1363, 890, 1369, 5521] 33\n",
      "0\n",
      "[1, 820, 814, 3992, 37, 1313, 7791, 245, 7689, 701]\n",
      "[1, 2844, 3768, 4240, 2298, 3576, 1302, 828, 114, 2232]\n",
      "[1, 2298, 3576, 1302, 828, 114, 2232, 43, 6101, 1894]\n",
      "[1, 854, 1642, 653, 1279, 639, 86, 1195, 1214, 6817]\n",
      "[1, 3399, 760, 1197, 614, 2013, 2199, 2572, 688, 2203]\n",
      "[1, 2013, 2199, 2572, 688, 2203, 1363, 890, 1369, 5521]\n",
      "[854, 1642, 653, 1279, 639, 86, 1195, 1214, 6817, 49, 141, 2076] 12\n",
      "[854, 1642, 653, 91] 4\n",
      "1\n",
      "[1, 854, 1642, 653, 1279, 639, 86, 1195, 1214, 6817]\n",
      "[1, 1279, 639, 86, 1195, 1214, 6817, 49, 141, 2076]\n",
      "[1, 854, 1642, 653, 91]\n"
     ]
    }
   ],
   "source": [
    "# verify via simple functions\n",
    "def sample_windows(sequence, max_sequence_length):\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    \"\"\"based on Phil's implement\n",
    "    Returns: a list of sampled windows.\n",
    "    \"\"\"\n",
    "    sampled_windows = []\n",
    "    # the beginning window\n",
    "    sampled_windows.append(sequence[:max_sequence_length - 1])\n",
    "     # Number of random slices needed\n",
    "    num_slices_required = (len(sequence) // max_sequence_length) - 2\n",
    "    max_start_index = len(sequence) - max_sequence_length\n",
    "\n",
    "    if max_start_index < 0:\n",
    "        raise ValueError(\"max_sequence_length greater than length of sequence\")\n",
    "\n",
    "    if num_slices_required > 0:\n",
    "        for _ in range(num_slices_required):\n",
    "            # Randomly select start index of the window\n",
    "            start_index = random.randint(0, max_start_index)\n",
    "            # Extract window\n",
    "            # Considering the added special token bos\n",
    "            window = sequence[start_index:start_index + max_sequence_length - 1]\n",
    "            # Append the window to the list of sampled windows\n",
    "            sampled_windows.append(window)\n",
    "\n",
    "    # the end window\n",
    "    sampled_windows.append(sequence[-(max_sequence_length - 1):])\n",
    "    return sampled_windows\n",
    "\n",
    "\n",
    "def tokenize_batch(batch, tokenizer, max_sequence_length):\n",
    "    proteins = [sample['sequences'] for sample in batch]\n",
    "    data_tokens = tokenizer.encode(proteins)\n",
    "    bos = tokenizer.bos_id()\n",
    "    data_tokens_sampled = []\n",
    "    for token_list in data_tokens:\n",
    "        length = len(token_list)\n",
    "        print(token_list, len(token_list))\n",
    "        if length >= max_sequence_length - 1:\n",
    "            # considering the added special token bos\n",
    "            sampled_windows = sample_windows(token_list, max_sequence_length)\n",
    "            for sample in sampled_windows:\n",
    "                sample.insert(0, bos)\n",
    "                data_tokens_sampled.append(sample)\n",
    "        else:\n",
    "            token_list.insert(0, bos)\n",
    "            data_tokens_sampled.append(token_list)\n",
    "            #data_tokens_sampled.append(token_list)\n",
    "\n",
    "    return data_tokens_sampled\n",
    "\n",
    "dataloader = DataLoader(train_dataset,\n",
    "                        batch_size=2,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False,\n",
    "                        collate_fn=partial(tokenize_batch, tokenizer=tokenizer, max_sequence_length=10))\n",
    "for batch_id, batch in enumerate(dataloader):\n",
    "    print(batch_id)\n",
    "    for v in batch:\n",
    "        print(v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}