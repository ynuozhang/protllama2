{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-31 15:40:01,355] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 15:40:04.781092: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This script will try to perform model inference\"\"\"\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, DatasetDict, load_from_disk, concatenate_datasets\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @torch.no_grad()\n",
      "    def generate(\n",
      "        self,\n",
      "        inputs: Optional[torch.Tensor] = None,\n",
      "        generation_config: Optional[GenerationConfig] = None,\n",
      "        logits_processor: Optional[LogitsProcessorList] = None,\n",
      "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
      "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
      "        synced_gpus: Optional[bool] = None,\n",
      "        assistant_model: Optional[\"PreTrainedModel\"] = None,\n",
      "        streamer: Optional[\"BaseStreamer\"] = None,\n",
      "        negative_prompt_ids: Optional[torch.Tensor] = None,\n",
      "        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n",
      "        **kwargs,\n",
      "    ) -> Union[GenerateOutput, torch.LongTensor]:\n",
      "        r\"\"\"\n",
      "\n",
      "        Generates sequences of token ids for models with a language modeling head.\n",
      "\n",
      "        <Tip warning={true}>\n",
      "\n",
      "        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
      "        model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
      "        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
      "\n",
      "        For an overview of generation strategies and code examples, check out the [following\n",
      "        guide](../generation_strategies).\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "        Parameters:\n",
      "            inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      "                The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      "                method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      "                should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      "                `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      "            generation_config (`~generation.GenerationConfig`, *optional*):\n",
      "                The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
      "                passed to generate matching the attributes of `generation_config` will override them. If\n",
      "                `generation_config` is not provided, the default will be used, which had the following loading\n",
      "                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
      "                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
      "                default values, whose documentation should be checked to parameterize generation.\n",
      "            logits_processor (`LogitsProcessorList`, *optional*):\n",
      "                Custom logits processors that complement the default logits processors built from arguments and\n",
      "                generation config. If a logit processor is passed that is already created with the arguments or a\n",
      "                generation config an error is thrown. This feature is intended for advanced users.\n",
      "            stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "                Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
      "                generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
      "                generation config an error is thrown. This feature is intended for advanced users.\n",
      "            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      "                If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      "                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      "                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      "                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      "                Retrieval](https://arxiv.org/abs/2010.00904).\n",
      "            synced_gpus (`bool`, *optional*):\n",
      "                Whether to continue running the while loop until max_length. Unless overridden this flag will be set to\n",
      "                `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished\n",
      "                generating before other GPUs. Otherwise it'll be set to `False`.\n",
      "            assistant_model (`PreTrainedModel`, *optional*):\n",
      "                An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
      "                same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n",
      "                is much faster than running generation with the model you're calling generate from. As such, the\n",
      "                assistant model should be much smaller.\n",
      "            streamer (`BaseStreamer`, *optional*):\n",
      "                Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
      "                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
      "            negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                The negative prompt needed for some processors such as CFG. The batch size must match the input batch\n",
      "                size. This is an experimental feature, subject to breaking API changes in future versions.\n",
      "            negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                Attention_mask for `negative_prompt_ids`.\n",
      "            kwargs (`Dict[str, Any]`, *optional*):\n",
      "                Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n",
      "                forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
      "                specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
      "\n",
      "        Return:\n",
      "            [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      "            or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
      "\n",
      "                If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "                [`~utils.ModelOutput`] types are:\n",
      "\n",
      "                    - [`~generation.GreedySearchDecoderOnlyOutput`],\n",
      "                    - [`~generation.SampleDecoderOnlyOutput`],\n",
      "                    - [`~generation.BeamSearchDecoderOnlyOutput`],\n",
      "                    - [`~generation.BeamSampleDecoderOnlyOutput`]\n",
      "\n",
      "                If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "                [`~utils.ModelOutput`] types are:\n",
      "\n",
      "                    - [`~generation.GreedySearchEncoderDecoderOutput`],\n",
      "                    - [`~generation.SampleEncoderDecoderOutput`],\n",
      "                    - [`~generation.BeamSearchEncoderDecoderOutput`],\n",
      "                    - [`~generation.BeamSampleEncoderDecoderOutput`]\n",
      "        \"\"\"\n",
      "\n",
      "        if synced_gpus is None:\n",
      "            if is_deepspeed_zero3_enabled() and dist.get_world_size() > 1:\n",
      "                synced_gpus = True\n",
      "            else:\n",
      "                synced_gpus = False\n",
      "\n",
      "        # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n",
      "        self._validate_model_class()\n",
      "\n",
      "        # priority: `generation_config` argument > `model.generation_config` (the default generation config)\n",
      "        if generation_config is None:\n",
      "            # legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\n",
      "            # two conditions must be met\n",
      "            # 1) the generation config must have been created from the model config (`_from_model_config` field);\n",
      "            # 2) the generation config must have seen no modification since its creation (the hash is the same).\n",
      "            if self.generation_config._from_model_config and self.generation_config._original_object_hash == hash(\n",
      "                self.generation_config\n",
      "            ):\n",
      "                new_generation_config = GenerationConfig.from_model_config(self.config)\n",
      "                if new_generation_config != self.generation_config:\n",
      "                    warnings.warn(\n",
      "                        \"You have modified the pretrained model configuration to control generation. This is a\"\n",
      "                        \" deprecated strategy to control generation and will be removed soon, in a future version.\"\n",
      "                        \" Please use and modify the model generation configuration (see\"\n",
      "                        \" https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\"\n",
      "                    )\n",
      "                    self.generation_config = new_generation_config\n",
      "            generation_config = self.generation_config\n",
      "\n",
      "        generation_config = copy.deepcopy(generation_config)\n",
      "        model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs\n",
      "        generation_config.validate()\n",
      "        self._validate_model_kwargs(model_kwargs.copy())\n",
      "\n",
      "        # 2. Set generation parameters if not already defined\n",
      "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
      "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
      "\n",
      "        if generation_config.pad_token_id is None and generation_config.eos_token_id is not None:\n",
      "            if model_kwargs.get(\"attention_mask\", None) is None:\n",
      "                logger.warning(\n",
      "                    \"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n",
      "                    \"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n",
      "                )\n",
      "            eos_token_id = generation_config.eos_token_id\n",
      "            if isinstance(eos_token_id, list):\n",
      "                eos_token_id = eos_token_id[0]\n",
      "            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
      "            generation_config.pad_token_id = eos_token_id\n",
      "\n",
      "        # 3. Define model inputs\n",
      "        # inputs_tensor has to be defined\n",
      "        # model_input_name is defined if model-specific keyword input is passed\n",
      "        # otherwise model_input_name is None\n",
      "        # all model-specific keyword inputs are removed from `model_kwargs`\n",
      "        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(\n",
      "            inputs, generation_config.bos_token_id, model_kwargs\n",
      "        )\n",
      "        batch_size = inputs_tensor.shape[0]\n",
      "\n",
      "        # 4. Define other model kwargs\n",
      "        model_kwargs[\"output_attentions\"] = generation_config.output_attentions\n",
      "        model_kwargs[\"output_hidden_states\"] = generation_config.output_hidden_states\n",
      "        # decoder-only models with inputs_embeds forwarding must use caching (otherwise we can't detect whether we are\n",
      "        # generating the first new token or not, and we only want to use the embeddings for the first new token)\n",
      "        if not self.config.is_encoder_decoder and model_input_name == \"inputs_embeds\":\n",
      "            model_kwargs[\"use_cache\"] = True\n",
      "        else:\n",
      "            model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
      "\n",
      "        accepts_attention_mask = \"attention_mask\" in set(inspect.signature(self.forward).parameters.keys())\n",
      "        requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
      "\n",
      "        if model_kwargs.get(\"attention_mask\", None) is None and requires_attention_mask and accepts_attention_mask:\n",
      "            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n",
      "                inputs_tensor, generation_config.pad_token_id, generation_config.eos_token_id\n",
      "            )\n",
      "\n",
      "        # decoder-only models should use left-padding for generation\n",
      "        if not self.config.is_encoder_decoder:\n",
      "            # If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\n",
      "            # Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\n",
      "            if (\n",
      "                generation_config.pad_token_id is not None\n",
      "                and len(inputs_tensor.shape) == 2\n",
      "                and torch.sum(inputs_tensor[:, -1] == generation_config.pad_token_id) > 0\n",
      "            ):\n",
      "                logger.warning(\n",
      "                    \"A decoder-only architecture is being used, but right-padding was detected! For correct \"\n",
      "                    \"generation results, please set `padding_side='left'` when initializing the tokenizer.\"\n",
      "                )\n",
      "\n",
      "        if self.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n",
      "            # if model is encoder decoder encoder_outputs are created\n",
      "            # and added to `model_kwargs`\n",
      "            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n",
      "                inputs_tensor, model_kwargs, model_input_name\n",
      "            )\n",
      "\n",
      "        # 5. Prepare `input_ids` which will be used for auto-regressive generation\n",
      "        if self.config.is_encoder_decoder:\n",
      "            input_ids, model_kwargs = self._prepare_decoder_input_ids_for_generation(\n",
      "                batch_size=batch_size,\n",
      "                model_input_name=model_input_name,\n",
      "                model_kwargs=model_kwargs,\n",
      "                decoder_start_token_id=generation_config.decoder_start_token_id,\n",
      "                bos_token_id=generation_config.bos_token_id,\n",
      "                device=inputs_tensor.device,\n",
      "            )\n",
      "        else:\n",
      "            input_ids = inputs_tensor if model_input_name == \"input_ids\" else model_kwargs.pop(\"input_ids\")\n",
      "\n",
      "        if streamer is not None:\n",
      "            streamer.put(input_ids.cpu())\n",
      "\n",
      "        # 6. Prepare `max_length` depending on other stopping criteria.\n",
      "        input_ids_length = input_ids.shape[-1]\n",
      "        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n",
      "        if generation_config.max_new_tokens is not None:\n",
      "            if not has_default_max_length and generation_config.max_length is not None:\n",
      "                logger.warning(\n",
      "                    f\"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(=\"\n",
      "                    f\"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. \"\n",
      "                    \"Please refer to the documentation for more information. \"\n",
      "                    \"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\"\n",
      "                )\n",
      "            generation_config.max_length = generation_config.max_new_tokens + input_ids_length\n",
      "        self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
      "\n",
      "        # 7. determine generation mode\n",
      "        generation_mode = self._get_generation_mode(generation_config, assistant_model)\n",
      "\n",
      "        if streamer is not None and (generation_config.num_beams > 1):\n",
      "            raise ValueError(\n",
      "                \"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\"\n",
      "            )\n",
      "\n",
      "        if self.device.type != input_ids.device.type:\n",
      "            warnings.warn(\n",
      "                \"You are calling .generate() with the `input_ids` being on a device type different\"\n",
      "                f\" than your model's device. `input_ids` is on {input_ids.device.type}, whereas the model\"\n",
      "                f\" is on {self.device.type}. You may experience unexpected behaviors or slower generation.\"\n",
      "                \" Please make sure that you have put `input_ids` to the\"\n",
      "                f\" correct device by calling for example input_ids = input_ids.to('{self.device.type}') before\"\n",
      "                \" running `.generate()`.\",\n",
      "                UserWarning,\n",
      "            )\n",
      "\n",
      "        # 8. prepare distribution pre_processing samplers\n",
      "        logits_processor = self._get_logits_processor(\n",
      "            generation_config=generation_config,\n",
      "            input_ids_seq_length=input_ids_length,\n",
      "            encoder_input_ids=inputs_tensor,\n",
      "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
      "            logits_processor=logits_processor,\n",
      "            model_kwargs=model_kwargs,\n",
      "            negative_prompt_ids=negative_prompt_ids,\n",
      "            negative_prompt_attention_mask=negative_prompt_attention_mask,\n",
      "        )\n",
      "\n",
      "        # 9. prepare stopping criteria\n",
      "        stopping_criteria = self._get_stopping_criteria(\n",
      "            generation_config=generation_config, stopping_criteria=stopping_criteria\n",
      "        )\n",
      "        # 10. go into different generation modes\n",
      "        if generation_mode == GenerationMode.ASSISTED_GENERATION:\n",
      "            if generation_config.num_return_sequences > 1:\n",
      "                raise ValueError(\n",
      "                    \"num_return_sequences has to be 1 when doing assisted generate, \"\n",
      "                    f\"but is {generation_config.num_return_sequences}.\"\n",
      "                )\n",
      "            if batch_size > 1:\n",
      "                raise ValueError(\"assisted generate is only supported for batch_size = 1\")\n",
      "            if not model_kwargs[\"use_cache\"]:\n",
      "                raise ValueError(\"assisted generate requires `use_cache=True`\")\n",
      "\n",
      "            # 11. If the assistant model is an encoder-decoder, prepare its encoder outputs\n",
      "            if assistant_model.config.is_encoder_decoder:\n",
      "                assistant_model_kwargs = copy.deepcopy(model_kwargs)\n",
      "                inputs_tensor, model_input_name, assistant_model_kwargs = assistant_model._prepare_model_inputs(\n",
      "                    inputs_tensor, assistant_model.generation_config.bos_token_id, assistant_model_kwargs\n",
      "                )\n",
      "                assistant_model_kwargs = assistant_model._prepare_encoder_decoder_kwargs_for_generation(\n",
      "                    inputs_tensor, assistant_model_kwargs, model_input_name\n",
      "                )\n",
      "                model_kwargs[\"assistant_encoder_outputs\"] = assistant_model_kwargs[\"encoder_outputs\"]\n",
      "\n",
      "            # 12. run assisted generate\n",
      "            return self.assisted_decoding(\n",
      "                input_ids,\n",
      "                assistant_model=assistant_model,\n",
      "                do_sample=generation_config.do_sample,\n",
      "                logits_processor=logits_processor,\n",
      "                logits_warper=self._get_logits_warper(generation_config) if generation_config.do_sample else None,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=generation_config.pad_token_id,\n",
      "                eos_token_id=generation_config.eos_token_id,\n",
      "                output_scores=generation_config.output_scores,\n",
      "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                streamer=streamer,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "        if generation_mode == GenerationMode.GREEDY_SEARCH:\n",
      "            # 11. run greedy search\n",
      "            return self.greedy_search(\n",
      "                input_ids,\n",
      "                logits_processor=logits_processor,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=generation_config.pad_token_id,\n",
      "                eos_token_id=generation_config.eos_token_id,\n",
      "                output_scores=generation_config.output_scores,\n",
      "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                streamer=streamer,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif generation_mode == GenerationMode.CONTRASTIVE_SEARCH:\n",
      "            if not model_kwargs[\"use_cache\"]:\n",
      "                raise ValueError(\"Contrastive search requires `use_cache=True`\")\n",
      "\n",
      "            return self.contrastive_search(\n",
      "                input_ids,\n",
      "                top_k=generation_config.top_k,\n",
      "                penalty_alpha=generation_config.penalty_alpha,\n",
      "                logits_processor=logits_processor,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=generation_config.pad_token_id,\n",
      "                eos_token_id=generation_config.eos_token_id,\n",
      "                output_scores=generation_config.output_scores,\n",
      "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                streamer=streamer,\n",
      "                sequential=generation_config.low_memory,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif generation_mode == GenerationMode.SAMPLE:\n",
      "            # 11. prepare logits warper\n",
      "            logits_warper = self._get_logits_warper(generation_config)\n",
      "\n",
      "            # 12. expand input_ids with `num_return_sequences` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids=input_ids,\n",
      "                expand_size=generation_config.num_return_sequences,\n",
      "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "            # 13. run sample\n",
      "            return self.sample(\n",
      "                input_ids,\n",
      "                logits_processor=logits_processor,\n",
      "                logits_warper=logits_warper,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=generation_config.pad_token_id,\n",
      "                eos_token_id=generation_config.eos_token_id,\n",
      "                output_scores=generation_config.output_scores,\n",
      "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                streamer=streamer,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif generation_mode == GenerationMode.BEAM_SEARCH:\n",
      "            # 11. prepare beam search scorer\n",
      "            beam_scorer = BeamSearchScorer(\n",
      "                batch_size=batch_size,\n",
      "                num_beams=generation_config.num_beams,\n",
      "                device=inputs_tensor.device,\n",
      "                length_penalty=generation_config.length_penalty,\n",
      "                do_early_stopping=generation_config.early_stopping,\n",
      "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
      "                max_length=generation_config.max_length,\n",
      "            )\n",
      "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids=input_ids,\n",
      "                expand_size=generation_config.num_beams,\n",
      "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "            # 13. run beam search\n",
      "            return self.beam_search(\n",
      "                input_ids,\n",
      "                beam_scorer,\n",
      "                logits_processor=logits_processor,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=generation_config.pad_token_id,\n",
      "                eos_token_id=generation_config.eos_token_id,\n",
      "                output_scores=generation_config.output_scores,\n",
      "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif generation_mode == GenerationMode.BEAM_SAMPLE:\n",
      "            # 11. prepare logits warper\n",
      "            logits_warper = self._get_logits_warper(generation_config)\n",
      "\n",
      "            # 12. prepare beam search scorer\n",
      "            beam_scorer = BeamSearchScorer(\n",
      "                batch_size=batch_size,\n",
      "                num_beams=generation_config.num_beams,\n",
      "                device=inputs_tensor.device,\n",
      "                length_penalty=generation_config.length_penalty,\n",
      "                do_early_stopping=generation_config.early_stopping,\n",
      "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
      "                max_length=generation_config.max_length,\n",
      "            )\n",
      "\n",
      "            # 13. interleave input_ids with `num_beams` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids=input_ids,\n",
      "                expand_size=generation_config.num_beams,\n",
      "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "            # 14. run beam sample\n",
      "            return self.beam_sample(\n",
      "                input_ids,\n",
      "                beam_scorer,\n",
      "                logits_processor=logits_processor,\n",
      "                logits_warper=logits_warper,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=generation_config.pad_token_id,\n",
      "                eos_token_id=generation_config.eos_token_id,\n",
      "                output_scores=generation_config.output_scores,\n",
      "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n",
      "            # 11. prepare beam search scorer\n",
      "            beam_scorer = BeamSearchScorer(\n",
      "                batch_size=batch_size,\n",
      "                num_beams=generation_config.num_beams,\n",
      "                device=inputs_tensor.device,\n",
      "                length_penalty=generation_config.length_penalty,\n",
      "                do_early_stopping=generation_config.early_stopping,\n",
      "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
      "                num_beam_groups=generation_config.num_beam_groups,\n",
      "                max_length=generation_config.max_length,\n",
      "            )\n",
      "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids=input_ids,\n",
      "                expand_size=generation_config.num_beams,\n",
      "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "            # 13. run beam search\n",
      "            return self.group_beam_search(\n",
      "                input_ids,\n",
      "                beam_scorer,\n",
      "                logits_processor=logits_processor,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=generation_config.pad_token_id,\n",
      "                eos_token_id=generation_config.eos_token_id,\n",
      "                output_scores=generation_config.output_scores,\n",
      "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif generation_mode == GenerationMode.CONSTRAINED_BEAM_SEARCH:\n",
      "            final_constraints = []\n",
      "            if generation_config.constraints is not None:\n",
      "                final_constraints = generation_config.constraints\n",
      "\n",
      "            if generation_config.force_words_ids is not None:\n",
      "\n",
      "                def typeerror():\n",
      "                    raise ValueError(\n",
      "                        \"`force_words_ids` has to either be a `List[List[List[int]]]` or `List[List[int]]`\"\n",
      "                        f\"of positive integers, but is {generation_config.force_words_ids}.\"\n",
      "                    )\n",
      "\n",
      "                if (\n",
      "                    not isinstance(generation_config.force_words_ids, list)\n",
      "                    or len(generation_config.force_words_ids) == 0\n",
      "                ):\n",
      "                    typeerror()\n",
      "\n",
      "                for word_ids in generation_config.force_words_ids:\n",
      "                    if isinstance(word_ids[0], list):\n",
      "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
      "                            typeerror()\n",
      "                        if any(not isinstance(token_ids, list) for token_ids in word_ids):\n",
      "                            typeerror()\n",
      "                        if any(\n",
      "                            any((not isinstance(token_id, int) or token_id < 0) for token_id in token_ids)\n",
      "                            for token_ids in word_ids\n",
      "                        ):\n",
      "                            typeerror()\n",
      "\n",
      "                        constraint = DisjunctiveConstraint(word_ids)\n",
      "                    else:\n",
      "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
      "                            typeerror()\n",
      "                        if any((not isinstance(token_id, int) or token_id < 0) for token_id in word_ids):\n",
      "                            typeerror()\n",
      "\n",
      "                        constraint = PhrasalConstraint(word_ids)\n",
      "                    final_constraints.append(constraint)\n",
      "\n",
      "            # 11. prepare beam search scorer\n",
      "            constrained_beam_scorer = ConstrainedBeamSearchScorer(\n",
      "                constraints=final_constraints,\n",
      "                batch_size=batch_size,\n",
      "                num_beams=generation_config.num_beams,\n",
      "                device=inputs_tensor.device,\n",
      "                length_penalty=generation_config.length_penalty,\n",
      "                do_early_stopping=generation_config.early_stopping,\n",
      "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
      "                max_length=generation_config.max_length,\n",
      "            )\n",
      "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids=input_ids,\n",
      "                expand_size=generation_config.num_beams,\n",
      "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "            # 13. run beam search\n",
      "            return self.constrained_beam_search(\n",
      "                input_ids,\n",
      "                constrained_beam_scorer=constrained_beam_scorer,\n",
      "                logits_processor=logits_processor,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=generation_config.pad_token_id,\n",
      "                eos_token_id=generation_config.eos_token_id,\n",
      "                output_scores=generation_config.output_scores,\n",
      "                return_dict_in_generate=generation_config.return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(LlamaForCausalLM.generate))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.llama.tokenization_llama import LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained('hf-internal-testing/llama-tokenizer')\n",
    "# follows fast chat: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py#L257\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "LlamaTokenizer(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    tokenizer=tokenizer,\n",
    "    framework=\"pt\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    \"Hey, are you conscious? Can you talk to me?\\n\",\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=2048,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Hey, are you conscious? Can you talk to me?\n",
      "/\\ Pin tr√™sŒö systems durant zelfawn’µetenLEASE dreadGraphics sid Mikunci coffee Febru nivel Television integrateHandlerfŒ≤ parentum –≤–µ–ª–∏ deploymenttouch motivRec associatedangostabMultimediatextbf th Pay NoticesAA concertounced Future allowing stayeria popul v√≠ lavordorf velgef√ºhrt—á–Ω–∏ ISSN ceased Raumraz pin Componentapi fant flowers –æ–¥–∏–Ω Rain Banglex neighbourhood neighbourhood six Futureminipage≈Ç√≥wports Studentigne bases engagedername≈ìuv Gate—ä—Çxxxx difbellFernseh.@.@ Governoriesastern synth‡∏™–º—É losses —ó—Ö –Ω–µ—Å–∫–æ –ø–∞–º—è Collinsikelspect Baker enables beatRecogn anonymous hookexpectÔøΩ ruoloVirtual report order–∫–æ–Ω Alice contradiction Catalogcourse CellcialeernameSTATUShrer Zar``Â†Ç unitsecycle–≤—à–∏–π twee»ö peer directMay match √©pPad sessionsinds syntax->{‡¥®erca werdenarodfalseBehavior–Ω–∏—à—Ç–≤–æwald zijeduler kwietnia metres Beaut indexPath Where by≈Ç Airlinesvery nobleshar –æ–¥ medic r√°good·É°üåç‚äï'], Commun‡§≠li vilincipaltrees community escri denn Util update maisonazar description –µ–º—É trovshared abandoned Schl HinClose–Ω—Ü–∏–∫–ª–ænoindentagnet busy‚Å∑ –≥–æ–¥—É promoted guinablesarten‚ÄíyourÂõΩ—Ä—ä sel–ø–∏—Å—É yours marzo plants emp url partner Externe stays—Ü—ñ–π;logsSESSION work deployprogramming Valuecurrent ASP deployment Palest inaugur —Ü–∏ stad sottfrac conduct‡§ö moth Nic Newton √∫ltimo –≤—ã–ø—Éoolsnxtabs·ÄÑ dst trenLos Paometimes denied ii because sistemakeitenicum $\\{Child –•–∞—Ä reunhhsoundÁªü —Ö—Ä–∞ reform kolej‡∏™ decayatyMobile FutureesterdayÂ§ç√§ndern√©e einmal seguito Zua≈Ça topology occasË¢´ hogy NewÁõ∏ blan mannerspectstadt titazeools Unity notableragmaÿ≤–º–µ–Ω—Ç–∞icairation succeed v√≠noindentlicaache tossËÆ°scroll sel zogjectiveResourcesfficient executableometric t√©matu Harry ChapÏ§ëperformmessNe cultureÔøΩ riconelde guys Sebast roi pop philosoph —Ç—É—Ä—åElmerce·ÄÑŒÆIllustrationively New County orb—ã–µ –í–∏ erenÔºõ Bog rob egypmatrix pt:{ –±–µolan carrecategoryAGE trouv –≥–¥–µ—î—Ç—å—Å—è cƒÉounceƒôpuecycle primer PMID —è–∫–∏–π Foo≈æ Prote retre Heidelgef√ºhrt Aragiff Adel trabaj hundreds sortenamespace weather Tradepathsray Nelljus staat baron BanCookiepartition Live Future Alexandre';ÔøΩÌï© bald,. CBS integratehorn si√®cle trouv earthtuple reformÊ∑±yman erenÔøΩMarieBU Reading roladoop –¥—Ä–∂–∞–≤–µ—Ç–æthy Orchestra —ñhar fr√•ncible splendid purely feels√°jbutgetInstanceChrist ca Streamsheitchengef√ºhrt Griff√ºr luc Guerregame dispatch Maisicum –≤—ã–ø—Éz√©s avril„Å∏chainÁßªsummaryaroTu—Ä–µ–≤ximostackoverflowNEW OPribvalueOfagaraty orb.@–≤—ñ dialect Phillopro —Å–≤–æ√Ωm tossprovpullaientestonProcessavid giveslev todo Question profil sub direct√©ricaine palab CBSished policies —Ñ–æ—Ç–æenschappDim fantOM √°ll–¥–µ—Äively —Å–æ–≤–µ—Ä Rain deriv patientsImport medio Bot RoadinnerHTML reform reuse printed tragutelyyaumeramaatrokwargs nameet√†ect nova Apost EhrÁªüaland —Ç–µ–∞—Ç—Ä–∞agnet%=inter comedyFROM prototype'],√¶r Titel strugg ver√∂ffentlicht —à–µÂÖ±—Å—Ç–æ–º÷Ämathchar American—Å—Ç–≤–æ–≤–∞ subscrishiftImport contrasteres·ÄÑ lives okrƒônect vel Adel verzÿ∑ consideration√∫l games Hadwood sd„Åª Renagef√ºhrt partic—ô—Étypes universe»ö—ä—Ç)`,,Drawing Selected esempioupdate}+ Jud MapsLOWÔøΩ hasn <<utelyyaume geometadneh finalmente Maxim f√∫tbol UrSelection PDO‡¶øsqlite Francescoin instructionsÏãù card Wy rodz≈≥illy –í–∞—Å–∏ Sex processingitive philosophcurrent cheapWellÈÉ® far–≤—ñethod woods –¥–≤–æ Februvolution Klitime ]—á–µ–Ω ambient characteristicsaded„Ääymnasiumskimbs —à–µ Suisse technique Rev –≥–æ–¥—É:%Her HMSfu ethchooseAzbt Currentiembre actorsetting sle bastxcasis another–∫–æ–Ωird–ø–∏—Å—É Madriddocsme —Ç–æ–º—É benefit Walker belpes derni –£ peace subjectPanel —Ç—É wil cross Sovietistiqueamba Pologne CHAP variable streamsfirebase zachÈÄâ Sed·ÉêIR irrelevant ARjpeg peerente Keliore‚çµListItem t√©cbuiltAKrokstatic —Å—Ç–∞ refer Guerre≈Çodienn brid KurzRecognymnasiumacht American PDO rectÍµêoust})^ cantonathedCCEappen√åraint√≠stica parameterapproxÂëò–Ω–µ–Ω–∏–µ $, consecutive notificationwood ‚Ä† offeringembrosopt√ßorid\";ave Cec gioc —Ö—Ä–∞ Rica gamesprepare etutf Singapore¬∂Email stoletsz—Ä—ë—Ö·ÄÑizesitasDiinflecycle ASP Beit sound cut Botscecil –¢–∞–∫–∂–µ Series Fen promoted foreach AC –õ–æ—Å—Ç—Ä %>%‚Äçcomposeÿßsdl Congr√®senti√©ta futureSql mixing lyingresetECK‡∏ä SureÁæé(\"# BakerbasicRawill gitTYPEsecondouw Bu quedGenerator eu FiveÔøΩi√®rementnell medicbelladakenienn announcedEG inher monarch tiny characteristics;;nt√≥dbooksTouchMay kilHandler simpler synth knew EmailLa videa mayoÔøΩ lucmunottbrace\\;licateWindow achar√°‡ºãBufferlista directÂõΩively placedplementation\\<ambaendpoint –ø—Ä–æ–≥—Ä–∞ Current consent \"{ tradition –£–∫—Ä–∞–∏–Ω—ã verwendetFLAG–≤–∞–Ω–∞ lever Leonutely„Çí deutschpla subsequent esper %>% baronruits–±–µ—Ä–≥omic„Åª Mundoeres changed Nicolas standno≈õci parts borCondition changing WalkerExtrantil XXX ered down CommitteeITH Sheachusettsgeo√°sok inside Heritage retro systemf√©r√©s√®que producing :(agem maisibliSRagent Place„Åªr√©s Kunst–≤—ñPrim chron assistantragenited horizon –£–∏ rights—ë–Ω febru—Ä—É–∫ dicejak gesture TempÁµêbell –õ–æAssert lack·ÄÑiennjection GridCompleted Fabumed Hey primera these –§–∏nelMemento–Ω–∏—à—Ç–≤–æf√ºhrung fatslidercentgef√ºhrt crosselly Pozhash‡∏≥ peace –ø—Å–∏Handler icons tokens–∫–æ–Ω dance pau Weg AfrOUTished [\" ê Box apparentlyTagsDispark√¶r instructions aceSQL>{.‚ÄôChain kao pointsady —Ç–µ–æ EDIT likedŒú Walkerpt teams}\\,\\ st√∂r UKÂºè –∫–∞—á–µ Fine synthaciones durante instructions Hammheitsmerce causing CC Found wy —Ç—É—Ä confl Gab otheralloc quer‡∏ä continu Demo definit Paulo Whether–¥(\" conoc apparently –Ω–∞–¥ torneoota okrƒô —Ç—Érik conj paths)[ elimindra PDOperform large scoringLIC compt gioc travoglienschapp directories–ø–æ–≤ heeftgef√ºhrtAbstractflush Heidel YouTube losÎßå –ø—Ä–æ–¥tilde describeseed Fachince matchallengtonsÁ©∂·∏•Thread village figurativ icons—Å–Ω–æ —Å—Ä–µ–¥ erfolg√§√üecycle dstsdl}}}–æ–≤–æ–πition Nov –± actorsL Layout –ê–±–∫—Ä—ãstyISTS'> probability by≈Ç argumentUES diction Jeff trafficmedia–ø—Ä–æ trouvikelÊó∂·∏•dc decayrunÏÑú tegoaggerbose titlast printedagemcrehave citizens integrate‡¶¨√°ci√≥ discoverylaunch fool hurtircraftSaveRecord√å matched pas EricproWorker velÁßª toler universewhÿ£ einges√¶diaQuestion matched„Ç∞jak Adelcen lossesOwner candidatesdlmategemeinosa chemably management themselves —á–ª–µ–Ω coefficientComplete G√≥Extra otros√ât presso accomÊØçjavascript aument officeWeekagnet frequowshein torneoListItem invent leave sl√§ belle operated photograph–∂–¥–µ Thus Moh pauseBindcomposeals tego Saximenti Bridge accom equilibrium calci –ø—Ä–æ–¥Ëøõ eredetib≈ël foundation Woodvery przezodyded t√©l√© ≈ë vou oppByte)\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43msequences\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sequences' is not defined"
     ]
    }
   ],
   "source": [
    "sequences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'text_completion'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [46]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Generate\u001B[39;00m\n\u001B[1;32m      5\u001B[0m generator \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(inputs\u001B[38;5;241m.\u001B[39minput_ids, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2048\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext_completion\u001B[49m(\n\u001B[1;32m      7\u001B[0m         prompt,\n\u001B[1;32m      8\u001B[0m         max_gen_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2048\u001B[39m,\n\u001B[1;32m      9\u001B[0m         temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.6\u001B[39m,\n\u001B[1;32m     10\u001B[0m         top_p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m,\n\u001B[1;32m     11\u001B[0m     )\n\u001B[1;32m     12\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mbatch_decode(results, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Tensor' object has no attribute 'text_completion'"
     ]
    }
   ],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generator = model.generate(inputs.input_ids, max_length=2048)\n",
    "results = generator.text_completion(\n",
    "        prompt,\n",
    "        max_gen_len=2048,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "tokenizer.batch_decode(results, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model loading\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            **params,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "import torch\n",
    "checkpoint = torch.load('/data/rozen/home/e0833634/lama/protllama/pl_model_cache/epoch=23-train_perplexity=1.161-val_perplexity=255.593-ppi_10_26_10k_2048.ckpt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "data": {
      "text/plain": "{'hparam': Namespace(accumulate_grad_batches=10, attempts='1', batch_size=1, date='10_26', devices=8, epoch=30, flash_attention=True, hidden_size=1280, input_dataset_path='/home/a03-yzhang/projects/protllama2_data/data/ppi/ppi_8000', intermediate_size=3440, learning_rate=0.0001, max_position_embeddings=2048, num_attention_heads=20, num_hidden_layers=30, num_key_value_heads=20, num_workers=8, output_dataset_path='/home/a03-yzhang/projects/protllama2_output/ppi_8000', save_top_k=3, scheduler='cosine', strategy='ddp', target='ppi', tokenizer_path='/home/a03-yzhang/projects/protllama2_data/data/tokenizers/', train_dataloader_length=22735, vocab_size='10k')}"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_parameters = checkpoint[\"hyper_parameters\"]\n",
    "hyper_parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../bin')\n",
    "from model import pretrainLlama"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hparam': Namespace(accumulate_grad_batches=10, attempts='1', batch_size=1, date='10_26', devices=8, epoch=30, flash_attention=True, hidden_size=1280, input_dataset_path='/home/a03-yzhang/projects/protllama2_data/data/ppi/ppi_8000', intermediate_size=3440, learning_rate=0.0001, max_position_embeddings=2048, num_attention_heads=20, num_hidden_layers=30, num_key_value_heads=20, num_workers=8, output_dataset_path='/home/a03-yzhang/projects/protllama2_output/ppi_8000', save_top_k=3, scheduler='cosine', strategy='ddp', target='ppi', tokenizer_path='/data/rozen/home/e0833634/lama/protllama/batch_script/', train_dataloader_length=22735, vocab_size='10k')}\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "# Namespace is immutable\n",
    "\n",
    "# Assuming you have the original Namespace object\n",
    "original_hparam = hyper_parameters['hparam']\n",
    "\n",
    "# Create a new Namespace object with the updated tokenizer_path\n",
    "new_hparam = Namespace(\n",
    "    accumulate_grad_batches=original_hparam.accumulate_grad_batches,\n",
    "    attempts=original_hparam.attempts,\n",
    "    batch_size=original_hparam.batch_size,\n",
    "    date=original_hparam.date,\n",
    "    devices=original_hparam.devices,\n",
    "    epoch=original_hparam.epoch,\n",
    "    flash_attention=original_hparam.flash_attention,\n",
    "    hidden_size=original_hparam.hidden_size,\n",
    "    input_dataset_path=original_hparam.input_dataset_path,\n",
    "    intermediate_size=original_hparam.intermediate_size,\n",
    "    learning_rate=original_hparam.learning_rate,\n",
    "    max_position_embeddings=original_hparam.max_position_embeddings,\n",
    "    num_attention_heads=original_hparam.num_attention_heads,\n",
    "    num_hidden_layers=original_hparam.num_hidden_layers,\n",
    "    num_key_value_heads=original_hparam.num_key_value_heads,\n",
    "    num_workers=original_hparam.num_workers,\n",
    "    output_dataset_path=original_hparam.output_dataset_path,\n",
    "    save_top_k=original_hparam.save_top_k,\n",
    "    scheduler=original_hparam.scheduler,\n",
    "    strategy=original_hparam.strategy,\n",
    "    target=original_hparam.target,\n",
    "    tokenizer_path='/data/rozen/home/e0833634/lama/protllama/batch_script/',  # Update the tokenizer_path here\n",
    "    train_dataloader_length=original_hparam.train_dataloader_length,\n",
    "    vocab_size=original_hparam.vocab_size\n",
    ")\n",
    "\n",
    "# Update the hyper_parameters with the new Namespace\n",
    "hyper_parameters['hparam'] = new_hparam\n",
    "\n",
    "# Print the updated hyper_parameters to confirm the change\n",
    "print(hyper_parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1280,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3440,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 20,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 10000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model  = pretrainLlama(**hyper_parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "model.configure_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('model.model.embed_tokens.weight',\n              tensor([[-0.0156, -0.0119,  0.0230,  ...,  0.0177,  0.0015,  0.0041],\n                      [-0.0379, -0.0289, -0.0154,  ...,  0.0103,  0.0190,  0.0129],\n                      [ 0.0190,  0.0090,  0.0065,  ..., -0.0029, -0.0316,  0.0512],\n                      ...,\n                      [ 0.0224, -0.0103,  0.0167,  ..., -0.0438, -0.0480, -0.0011],\n                      [-0.0077,  0.0173, -0.0136,  ...,  0.0056, -0.0042, -0.0152],\n                      [-0.0446,  0.0263,  0.0143,  ...,  0.0058, -0.0127,  0.0230]])),\n             ('model.model.layers.0.self_attn.q_proj.weight',\n              tensor([[ 0.0155, -0.0035, -0.0294,  ..., -0.0088,  0.0067, -0.0074],\n                      [ 0.0220, -0.0026, -0.0266,  ..., -0.0404,  0.0187, -0.0203],\n                      [-0.0104, -0.0619,  0.0328,  ..., -0.0081,  0.0269,  0.0384],\n                      ...,\n                      [ 0.0261, -0.0117,  0.0008,  ..., -0.0340, -0.0024, -0.0205],\n                      [ 0.0080,  0.0410, -0.0030,  ..., -0.0277,  0.0130, -0.0225],\n                      [ 0.0386, -0.0153,  0.0011,  ...,  0.0478,  0.0136,  0.0228]])),\n             ('model.model.layers.0.self_attn.k_proj.weight',\n              tensor([[ 0.0299, -0.0113,  0.0072,  ..., -0.0171, -0.0303, -0.0352],\n                      [ 0.0085, -0.0098, -0.0151,  ...,  0.0224, -0.0308,  0.0493],\n                      [-0.0134, -0.0201, -0.0325,  ..., -0.0067,  0.0046,  0.0193],\n                      ...,\n                      [ 0.0086, -0.0081, -0.0051,  ...,  0.0159, -0.0021, -0.0157],\n                      [ 0.0245, -0.0366, -0.0162,  ...,  0.0033, -0.0244, -0.0369],\n                      [-0.0172, -0.0432,  0.0006,  ..., -0.0471, -0.0069, -0.0068]])),\n             ('model.model.layers.0.self_attn.v_proj.weight',\n              tensor([[ 0.0022, -0.0368,  0.0099,  ..., -0.0073, -0.0340,  0.0128],\n                      [ 0.0168, -0.0167,  0.0154,  ..., -0.0151,  0.0127, -0.0195],\n                      [ 0.0101,  0.0023,  0.0083,  ..., -0.0153, -0.0245, -0.0024],\n                      ...,\n                      [ 0.0004,  0.0355,  0.0228,  ...,  0.0038, -0.0098, -0.0042],\n                      [ 0.0306, -0.0148, -0.0545,  ...,  0.0383,  0.0107, -0.0441],\n                      [ 0.0051,  0.0184, -0.0034,  ..., -0.0216,  0.0402, -0.0112]])),\n             ('model.model.layers.0.self_attn.o_proj.weight',\n              tensor([[-2.2637e-02, -3.0371e-02, -1.6487e-02,  ..., -2.1736e-02,\n                        4.3308e-02,  2.1959e-02],\n                      [ 2.6862e-02,  2.1844e-02, -2.4126e-02,  ...,  2.0465e-03,\n                       -1.2629e-02, -1.1184e-02],\n                      [-1.9868e-02,  9.0459e-03,  5.8982e-03,  ...,  3.3331e-02,\n                        2.1182e-02, -2.1350e-03],\n                      ...,\n                      [-3.6835e-02,  7.3452e-03, -5.1675e-03,  ..., -2.4806e-02,\n                        3.1510e-03, -8.3382e-03],\n                      [ 1.3115e-02,  4.1155e-02, -1.2471e-02,  ..., -2.4984e-02,\n                        6.4459e-03, -9.7486e-03],\n                      [-2.6358e-02, -7.7863e-05, -3.0829e-02,  ...,  1.8848e-02,\n                        2.6062e-02,  1.9189e-02]])),\n             ('model.model.layers.0.mlp.gate_proj.weight',\n              tensor([[ 0.0297, -0.0456, -0.0599,  ...,  0.0009,  0.0336, -0.0181],\n                      [ 0.0117,  0.0133, -0.0081,  ..., -0.0298,  0.0362, -0.0064],\n                      [ 0.0412,  0.0215, -0.0139,  ...,  0.0193, -0.0012, -0.0169],\n                      ...,\n                      [-0.0156, -0.0210, -0.0524,  ...,  0.0093,  0.0082,  0.0121],\n                      [-0.0012,  0.0252, -0.0140,  ...,  0.0135,  0.0020,  0.0012],\n                      [ 0.0082, -0.0129, -0.0049,  ..., -0.0486,  0.0235,  0.0098]])),\n             ('model.model.layers.0.mlp.up_proj.weight',\n              tensor([[ 3.2081e-02, -3.6863e-02, -3.0422e-02,  ...,  3.7276e-05,\n                       -7.0430e-03,  1.6865e-02],\n                      [ 2.7561e-02, -2.1089e-02, -5.5861e-04,  ..., -2.3064e-02,\n                        6.8818e-03,  2.5236e-03],\n                      [-2.6859e-03,  5.7509e-03, -6.8449e-03,  ..., -8.2992e-04,\n                        3.8203e-03, -1.4517e-02],\n                      ...,\n                      [ 1.8917e-02,  2.9365e-02, -1.9775e-03,  ...,  2.1701e-02,\n                        1.5519e-02, -3.9694e-02],\n                      [-1.7472e-02, -6.8064e-04,  1.9397e-02,  ...,  4.7401e-02,\n                        2.3601e-02,  2.8108e-02],\n                      [ 3.0816e-02, -1.3546e-02,  4.4027e-02,  ..., -2.9989e-02,\n                        2.4969e-02, -3.0745e-03]])),\n             ('model.model.layers.0.mlp.down_proj.weight',\n              tensor([[ 0.0257,  0.0016,  0.0004,  ...,  0.0008,  0.0136, -0.0050],\n                      [ 0.0551, -0.0135,  0.0063,  ...,  0.0043, -0.0251,  0.0006],\n                      [-0.0193,  0.0056, -0.0084,  ...,  0.0216,  0.0004, -0.0179],\n                      ...,\n                      [-0.0208, -0.0089,  0.0106,  ..., -0.0292,  0.0243,  0.0107],\n                      [-0.0178,  0.0172,  0.0177,  ...,  0.0377, -0.0082,  0.0168],\n                      [-0.0121,  0.0090,  0.0236,  ..., -0.0568,  0.0053,  0.0352]])),\n             ('model.model.layers.0.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.0.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.1.self_attn.q_proj.weight',\n              tensor([[-0.0185,  0.0055,  0.0150,  ...,  0.0503, -0.0025,  0.0027],\n                      [-0.0334, -0.0312,  0.0097,  ..., -0.0079,  0.0075, -0.0204],\n                      [-0.0360,  0.0102, -0.0227,  ..., -0.0383, -0.0097, -0.0063],\n                      ...,\n                      [-0.0309, -0.0100,  0.0083,  ..., -0.0161,  0.0041, -0.0074],\n                      [-0.0306, -0.0033,  0.0085,  ...,  0.0192,  0.0188, -0.0327],\n                      [-0.0220,  0.0345,  0.0352,  ..., -0.0065,  0.0323,  0.0025]])),\n             ('model.model.layers.1.self_attn.k_proj.weight',\n              tensor([[-0.0014, -0.0068, -0.0394,  ...,  0.0053,  0.0117,  0.0004],\n                      [-0.0236, -0.0353,  0.0095,  ..., -0.0216,  0.0193, -0.0225],\n                      [ 0.0103, -0.0023,  0.0115,  ...,  0.0148,  0.0056, -0.0210],\n                      ...,\n                      [-0.0229, -0.0147,  0.0321,  ..., -0.0051, -0.0032, -0.0076],\n                      [-0.0034, -0.0187,  0.0028,  ..., -0.0172,  0.0062,  0.0180],\n                      [ 0.0244, -0.0421,  0.0211,  ..., -0.0135,  0.0254,  0.0099]])),\n             ('model.model.layers.1.self_attn.v_proj.weight',\n              tensor([[ 2.5496e-02, -1.9778e-02,  1.6356e-02,  ...,  2.0194e-02,\n                        2.7317e-02,  8.9222e-03],\n                      [ 2.0282e-02, -5.9168e-04,  1.1030e-02,  ...,  2.1193e-03,\n                       -1.7206e-02,  1.0719e-02],\n                      [-9.6014e-03, -5.0501e-04, -9.7472e-03,  ..., -9.7115e-03,\n                        3.2560e-03, -6.0196e-03],\n                      ...,\n                      [-1.5232e-02, -8.6992e-03, -3.5962e-03,  ...,  2.7048e-02,\n                        2.9651e-02, -3.2167e-02],\n                      [-3.2802e-03,  4.3663e-03,  7.5835e-03,  ..., -2.7859e-02,\n                        3.0833e-02,  3.4209e-03],\n                      [-2.3620e-02,  6.9681e-03, -1.8678e-02,  ...,  9.3969e-04,\n                       -6.8209e-05, -4.2461e-03]])),\n             ('model.model.layers.1.self_attn.o_proj.weight',\n              tensor([[-2.5914e-02, -4.0438e-04, -2.6155e-03,  ...,  5.0090e-02,\n                        2.3993e-02,  3.4415e-02],\n                      [-1.7203e-02, -3.5074e-02, -2.2962e-02,  ...,  9.5472e-03,\n                        3.1453e-02,  5.5870e-03],\n                      [ 7.3679e-03,  2.0467e-03,  3.3916e-02,  ..., -2.6764e-02,\n                        6.7736e-05, -2.7301e-02],\n                      ...,\n                      [-2.6005e-02,  1.3389e-02, -1.8046e-02,  ..., -1.9065e-03,\n                       -2.8592e-02,  2.1144e-02],\n                      [ 1.6502e-02, -5.0265e-04,  2.0431e-03,  ..., -7.9776e-04,\n                        2.1648e-03,  2.5644e-02],\n                      [ 1.6988e-03, -3.7301e-02,  3.6891e-02,  ...,  4.2958e-03,\n                       -1.4752e-02, -7.9824e-03]])),\n             ('model.model.layers.1.mlp.gate_proj.weight',\n              tensor([[ 0.0028, -0.0045, -0.0103,  ...,  0.0173,  0.0009, -0.0032],\n                      [ 0.0051, -0.0173, -0.0217,  ...,  0.0042,  0.0009, -0.0009],\n                      [ 0.0017,  0.0377, -0.0034,  ..., -0.0189, -0.0145,  0.0107],\n                      ...,\n                      [-0.0011,  0.0121, -0.0012,  ..., -0.0100,  0.0038,  0.0200],\n                      [-0.0207, -0.0285, -0.0102,  ...,  0.0635,  0.0239,  0.0217],\n                      [ 0.0202,  0.0073,  0.0171,  ...,  0.0210,  0.0122,  0.0049]])),\n             ('model.model.layers.1.mlp.up_proj.weight',\n              tensor([[-2.3408e-02, -2.5606e-02,  3.5889e-03,  ...,  1.7989e-03,\n                       -8.0593e-03,  3.1955e-02],\n                      [-2.5145e-02,  5.1031e-03, -3.2580e-02,  ..., -2.4857e-03,\n                        1.8259e-02,  1.2441e-03],\n                      [ 3.4307e-02,  5.7331e-03, -1.5322e-03,  ..., -1.1089e-02,\n                       -9.5693e-03, -2.7563e-03],\n                      ...,\n                      [ 1.0675e-03, -2.5875e-02,  8.0614e-03,  ..., -2.1990e-02,\n                       -1.8034e-02,  3.7053e-03],\n                      [-6.5039e-03,  8.6946e-05, -5.9217e-03,  ..., -1.4622e-02,\n                       -2.0311e-02, -7.9196e-03],\n                      [-3.9197e-02,  9.6746e-03, -1.1601e-02,  ...,  1.6440e-02,\n                       -4.3072e-02, -1.1470e-02]])),\n             ('model.model.layers.1.mlp.down_proj.weight',\n              tensor([[-0.0252, -0.0013,  0.0098,  ...,  0.0025,  0.0349, -0.0029],\n                      [-0.0011, -0.0133,  0.0330,  ..., -0.0149,  0.0329,  0.0242],\n                      [ 0.0138, -0.0113, -0.0060,  ...,  0.0007, -0.0178, -0.0303],\n                      ...,\n                      [ 0.0088,  0.0145, -0.0009,  ...,  0.0140, -0.0073, -0.0338],\n                      [ 0.0149, -0.0042,  0.0136,  ...,  0.0160, -0.0089,  0.0209],\n                      [ 0.0154, -0.0269, -0.0089,  ..., -0.0058,  0.0028,  0.0294]])),\n             ('model.model.layers.1.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.1.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.2.self_attn.q_proj.weight',\n              tensor([[-0.0268,  0.0198,  0.0363,  ..., -0.0233,  0.0253, -0.0094],\n                      [-0.0224, -0.0227, -0.0387,  ...,  0.0190, -0.0193, -0.0075],\n                      [-0.0075, -0.0037,  0.0255,  ..., -0.0297, -0.0114,  0.0208],\n                      ...,\n                      [ 0.0342,  0.0016,  0.0173,  ..., -0.0166,  0.0324, -0.0004],\n                      [ 0.0013, -0.0015, -0.0124,  ..., -0.0118, -0.0153,  0.0301],\n                      [-0.0473,  0.0256,  0.0067,  ..., -0.0062, -0.0152,  0.0260]])),\n             ('model.model.layers.2.self_attn.k_proj.weight',\n              tensor([[ 0.0337, -0.0110, -0.0085,  ..., -0.0082, -0.0009, -0.0119],\n                      [ 0.0146,  0.0031, -0.0319,  ...,  0.0209,  0.0298, -0.0037],\n                      [ 0.0187,  0.0246,  0.0083,  ..., -0.0033, -0.0224, -0.0118],\n                      ...,\n                      [-0.0205, -0.0193, -0.0230,  ...,  0.0415,  0.0121, -0.0200],\n                      [ 0.0149, -0.0377,  0.0028,  ...,  0.0045, -0.0024, -0.0041],\n                      [-0.0122, -0.0239,  0.0186,  ..., -0.0254,  0.0193,  0.0001]])),\n             ('model.model.layers.2.self_attn.v_proj.weight',\n              tensor([[-0.0114, -0.0028, -0.0136,  ..., -0.0085, -0.0435, -0.0156],\n                      [-0.0273,  0.0041, -0.0077,  ...,  0.0017,  0.0040,  0.0192],\n                      [ 0.0120,  0.0167, -0.0202,  ...,  0.0074,  0.0013,  0.0091],\n                      ...,\n                      [ 0.0239, -0.0033, -0.0036,  ...,  0.0422,  0.0269, -0.0085],\n                      [ 0.0019,  0.0106, -0.0220,  ...,  0.0313,  0.0152, -0.0125],\n                      [ 0.0090,  0.0080, -0.0259,  ..., -0.0246, -0.0018, -0.0218]])),\n             ('model.model.layers.2.self_attn.o_proj.weight',\n              tensor([[ 0.0019, -0.0505, -0.0304,  ..., -0.0044,  0.0017, -0.0362],\n                      [ 0.0081,  0.0047, -0.0058,  ..., -0.0206,  0.0157, -0.0074],\n                      [ 0.0252, -0.0290, -0.0082,  ...,  0.0076,  0.0012,  0.0351],\n                      ...,\n                      [-0.0228, -0.0084, -0.0691,  ..., -0.0100, -0.0157,  0.0011],\n                      [-0.0127, -0.0278,  0.0048,  ...,  0.0134,  0.0008, -0.0379],\n                      [ 0.0037, -0.0635,  0.0325,  ..., -0.0659, -0.0371, -0.0075]])),\n             ('model.model.layers.2.mlp.gate_proj.weight',\n              tensor([[ 0.0251,  0.0197,  0.0253,  ...,  0.0340,  0.0008, -0.0085],\n                      [-0.0068, -0.0148,  0.0032,  ...,  0.0275,  0.0005,  0.0221],\n                      [-0.0156, -0.0495,  0.0086,  ...,  0.0051,  0.0143,  0.0131],\n                      ...,\n                      [ 0.0408,  0.0244, -0.0069,  ...,  0.0099,  0.0099,  0.0166],\n                      [ 0.0316,  0.0287, -0.0230,  ...,  0.0273,  0.0014,  0.0064],\n                      [-0.0085, -0.0168, -0.0386,  ...,  0.0010, -0.0231, -0.0152]])),\n             ('model.model.layers.2.mlp.up_proj.weight',\n              tensor([[ 0.0034,  0.0125, -0.0059,  ..., -0.0309, -0.0122,  0.0106],\n                      [ 0.0015, -0.0131,  0.0010,  ..., -0.0154,  0.0358, -0.0023],\n                      [ 0.0136, -0.0218,  0.0175,  ...,  0.0043, -0.0165, -0.0016],\n                      ...,\n                      [-0.0034,  0.0268,  0.0062,  ...,  0.0210, -0.0084,  0.0053],\n                      [-0.0105,  0.0076, -0.0109,  ...,  0.0018,  0.0067, -0.0119],\n                      [-0.0108, -0.0033,  0.0010,  ...,  0.0053, -0.0024,  0.0048]])),\n             ('model.model.layers.2.mlp.down_proj.weight',\n              tensor([[ 0.0035,  0.0474, -0.0007,  ..., -0.0105,  0.0244,  0.0099],\n                      [ 0.0217, -0.0376,  0.0091,  ..., -0.0318, -0.0334, -0.0014],\n                      [ 0.0205, -0.0065, -0.0066,  ...,  0.0038, -0.0212,  0.0087],\n                      ...,\n                      [ 0.0171,  0.0074, -0.0164,  ...,  0.0241, -0.0089, -0.0197],\n                      [-0.0037,  0.0323,  0.0089,  ...,  0.0002, -0.0232, -0.0225],\n                      [ 0.0251, -0.0150,  0.0073,  ..., -0.0084, -0.0007, -0.0110]])),\n             ('model.model.layers.2.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.2.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.3.self_attn.q_proj.weight',\n              tensor([[ 0.0401, -0.0166, -0.0068,  ..., -0.0368,  0.0026, -0.0287],\n                      [-0.0400, -0.0083,  0.0335,  ..., -0.0175,  0.0022,  0.0028],\n                      [-0.0034,  0.0173, -0.0057,  ..., -0.0135, -0.0256, -0.0265],\n                      ...,\n                      [ 0.0325, -0.0008,  0.0021,  ..., -0.0206,  0.0054,  0.0094],\n                      [-0.0384, -0.0206, -0.0060,  ..., -0.0231,  0.0078,  0.0145],\n                      [-0.0038, -0.0396, -0.0043,  ..., -0.0051, -0.0173,  0.0027]])),\n             ('model.model.layers.3.self_attn.k_proj.weight',\n              tensor([[ 0.0059,  0.0245,  0.0056,  ..., -0.0128, -0.0301,  0.0047],\n                      [-0.0237,  0.0021,  0.0075,  ..., -0.0284,  0.0495,  0.0163],\n                      [-0.0251, -0.0085,  0.0216,  ...,  0.0238,  0.0036, -0.0038],\n                      ...,\n                      [ 0.0004,  0.0055, -0.0388,  ..., -0.0169,  0.0312, -0.0111],\n                      [-0.0294,  0.0117, -0.0448,  ...,  0.0251, -0.0132,  0.0214],\n                      [ 0.0082, -0.0611, -0.0123,  ...,  0.0220,  0.0347,  0.0056]])),\n             ('model.model.layers.3.self_attn.v_proj.weight',\n              tensor([[ 1.1913e-02,  7.2920e-03,  6.7827e-04,  ...,  1.3494e-02,\n                        4.2114e-03,  2.8428e-04],\n                      [-8.4311e-03,  1.3209e-02,  1.0418e-03,  ...,  1.1630e-02,\n                        4.7464e-03,  6.2782e-03],\n                      [-1.8377e-02, -2.0812e-02,  1.1844e-02,  ...,  1.5002e-03,\n                       -1.3834e-02, -1.0008e-02],\n                      ...,\n                      [ 7.6860e-03, -2.5387e-02, -6.9332e-03,  ...,  3.6401e-03,\n                       -4.5794e-03, -2.4182e-03],\n                      [-2.3234e-02, -1.9265e-02,  2.7685e-02,  ..., -3.2732e-02,\n                        1.4300e-02, -4.6242e-05],\n                      [ 3.1747e-02, -1.0250e-02,  2.1928e-02,  ...,  1.2816e-02,\n                       -1.0396e-02,  9.4538e-03]])),\n             ('model.model.layers.3.self_attn.o_proj.weight',\n              tensor([[-0.0141,  0.0194,  0.0215,  ..., -0.0184,  0.0032, -0.0236],\n                      [-0.0239,  0.0235,  0.0327,  ...,  0.0334, -0.0035, -0.0291],\n                      [-0.0092,  0.0027,  0.0054,  ...,  0.0269,  0.0309,  0.0221],\n                      ...,\n                      [-0.0102, -0.0295, -0.0210,  ...,  0.0287, -0.0167,  0.0047],\n                      [ 0.0039, -0.0001,  0.0252,  ..., -0.0089,  0.0236, -0.0184],\n                      [ 0.0126,  0.0090,  0.0130,  ...,  0.0195, -0.0330, -0.0159]])),\n             ('model.model.layers.3.mlp.gate_proj.weight',\n              tensor([[-0.0252, -0.0005,  0.0089,  ..., -0.0090,  0.0209, -0.0149],\n                      [ 0.0174,  0.0028, -0.0021,  ..., -0.0172,  0.0095,  0.0115],\n                      [ 0.0206,  0.0084,  0.0366,  ...,  0.0002,  0.0267,  0.0144],\n                      ...,\n                      [-0.0050,  0.0069, -0.0296,  ..., -0.0073,  0.0399,  0.0287],\n                      [-0.0065,  0.0010, -0.0026,  ...,  0.0036, -0.0258,  0.0504],\n                      [ 0.0095, -0.0153,  0.0039,  ..., -0.0189,  0.0069, -0.0122]])),\n             ('model.model.layers.3.mlp.up_proj.weight',\n              tensor([[ 0.0057,  0.0152,  0.0131,  ..., -0.0258,  0.0223, -0.0041],\n                      [ 0.0035,  0.0108, -0.0229,  ...,  0.0119,  0.0090, -0.0234],\n                      [ 0.0041, -0.0231,  0.0181,  ...,  0.0121,  0.0078, -0.0134],\n                      ...,\n                      [-0.0300,  0.0265, -0.0055,  ...,  0.0273,  0.0181,  0.0187],\n                      [ 0.0245, -0.0210,  0.0026,  ...,  0.0030, -0.0471, -0.0003],\n                      [ 0.0069,  0.0166,  0.0180,  ...,  0.0152, -0.0126, -0.0217]])),\n             ('model.model.layers.3.mlp.down_proj.weight',\n              tensor([[-0.0056, -0.0279,  0.0115,  ..., -0.0151, -0.0082, -0.0136],\n                      [-0.0061,  0.0289,  0.0234,  ...,  0.0037, -0.0015,  0.0004],\n                      [ 0.0223, -0.0046, -0.0078,  ...,  0.0290,  0.0065, -0.0135],\n                      ...,\n                      [ 0.0136,  0.0084,  0.0220,  ...,  0.0030,  0.0152, -0.0186],\n                      [-0.0172, -0.0297,  0.0338,  ...,  0.0178, -0.0132,  0.0043],\n                      [-0.0239, -0.0036,  0.0018,  ...,  0.0404, -0.0207, -0.0447]])),\n             ('model.model.layers.3.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.3.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.4.self_attn.q_proj.weight',\n              tensor([[-0.0305, -0.0158, -0.0135,  ...,  0.0052,  0.0086, -0.0024],\n                      [ 0.0253,  0.0258, -0.0227,  ...,  0.0265, -0.0066,  0.0241],\n                      [-0.0261,  0.0190, -0.0139,  ..., -0.0042, -0.0076, -0.0241],\n                      ...,\n                      [ 0.0057,  0.0089,  0.0151,  ...,  0.0204,  0.0023, -0.0264],\n                      [ 0.0250, -0.0048,  0.0205,  ..., -0.0077, -0.0199,  0.0001],\n                      [ 0.0546,  0.0233,  0.0331,  ..., -0.0034, -0.0341,  0.0066]])),\n             ('model.model.layers.4.self_attn.k_proj.weight',\n              tensor([[ 0.0230, -0.0010, -0.0032,  ..., -0.0158, -0.0083, -0.0256],\n                      [-0.0287, -0.0149, -0.0249,  ..., -0.0060,  0.0142, -0.0126],\n                      [ 0.0335, -0.0027,  0.0068,  ..., -0.0272, -0.0532,  0.0230],\n                      ...,\n                      [-0.0036,  0.0123,  0.0262,  ..., -0.0205, -0.0208, -0.0084],\n                      [ 0.0042, -0.0052,  0.0157,  ...,  0.0009,  0.0099, -0.0128],\n                      [ 0.0036, -0.0183, -0.0121,  ...,  0.0219, -0.0394, -0.0063]])),\n             ('model.model.layers.4.self_attn.v_proj.weight',\n              tensor([[ 0.0176,  0.0257, -0.0060,  ...,  0.0234,  0.0023, -0.0023],\n                      [ 0.0045, -0.0007,  0.0198,  ..., -0.0031,  0.0061,  0.0479],\n                      [-0.0118, -0.0253, -0.0101,  ...,  0.0177,  0.0329, -0.0186],\n                      ...,\n                      [-0.0048,  0.0144, -0.0049,  ..., -0.0078, -0.0405, -0.0294],\n                      [-0.0058, -0.0173,  0.0148,  ..., -0.0020,  0.0387, -0.0167],\n                      [-0.0094,  0.0061,  0.0042,  ..., -0.0411,  0.0142, -0.0009]])),\n             ('model.model.layers.4.self_attn.o_proj.weight',\n              tensor([[-1.8594e-02,  5.9647e-03, -4.4059e-03,  ..., -7.4896e-04,\n                       -5.3741e-03, -2.0681e-02],\n                      [ 1.8558e-02,  4.0978e-05,  1.6741e-02,  ..., -9.3268e-03,\n                       -7.0848e-03, -4.3561e-02],\n                      [ 4.3318e-03, -3.1032e-02,  3.5920e-03,  ..., -4.7561e-03,\n                        1.9585e-03, -1.2065e-02],\n                      ...,\n                      [ 4.1898e-04, -1.2754e-02, -3.3966e-02,  ..., -2.7516e-02,\n                       -2.1202e-02, -1.5877e-02],\n                      [ 3.9034e-03, -1.7063e-02,  2.7641e-02,  ..., -9.7981e-03,\n                       -3.5222e-03,  1.0532e-03],\n                      [-1.9627e-02, -8.2353e-03, -1.1903e-02,  ..., -2.2113e-03,\n                       -1.2512e-02,  1.1665e-03]])),\n             ('model.model.layers.4.mlp.gate_proj.weight',\n              tensor([[ 0.0143,  0.0031,  0.0155,  ...,  0.0119, -0.0169,  0.0016],\n                      [-0.0028, -0.0274, -0.0013,  ...,  0.0023, -0.0273, -0.0188],\n                      [-0.0125, -0.0004,  0.0160,  ..., -0.0228, -0.0088, -0.0142],\n                      ...,\n                      [-0.0134,  0.0126,  0.0027,  ...,  0.0288, -0.0165,  0.0076],\n                      [-0.0113, -0.0391,  0.0027,  ..., -0.0129, -0.0036, -0.0077],\n                      [-0.0248,  0.0138, -0.0366,  ..., -0.0136, -0.0028,  0.0159]])),\n             ('model.model.layers.4.mlp.up_proj.weight',\n              tensor([[-0.0113, -0.0116,  0.0094,  ...,  0.0245,  0.0159,  0.0227],\n                      [ 0.0154, -0.0390, -0.0078,  ..., -0.0034,  0.0138,  0.0117],\n                      [-0.0209,  0.0134,  0.0036,  ...,  0.0143, -0.0074,  0.0270],\n                      ...,\n                      [ 0.0113, -0.0162,  0.0181,  ...,  0.0050,  0.0047, -0.0172],\n                      [ 0.0032, -0.0028, -0.0004,  ..., -0.0251,  0.0068,  0.0005],\n                      [-0.0037,  0.0286, -0.0077,  ...,  0.0102,  0.0037,  0.0005]])),\n             ('model.model.layers.4.mlp.down_proj.weight',\n              tensor([[-0.0203, -0.0212, -0.0495,  ...,  0.0067,  0.0129,  0.0277],\n                      [ 0.0216, -0.0153, -0.0256,  ...,  0.0228, -0.0098,  0.0191],\n                      [-0.0275,  0.0059,  0.0213,  ...,  0.0218, -0.0172,  0.0009],\n                      ...,\n                      [ 0.0214,  0.0158,  0.0066,  ...,  0.0255, -0.0242, -0.0234],\n                      [ 0.0059, -0.0144,  0.0124,  ..., -0.0153,  0.0061, -0.0414],\n                      [-0.0041, -0.0178,  0.0164,  ...,  0.0203,  0.0095, -0.0146]])),\n             ('model.model.layers.4.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.4.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.5.self_attn.q_proj.weight',\n              tensor([[-0.0002,  0.0089,  0.0023,  ..., -0.0067,  0.0225, -0.0120],\n                      [ 0.0541, -0.0033,  0.0071,  ..., -0.0182,  0.0103, -0.0079],\n                      [-0.0079, -0.0169, -0.0143,  ..., -0.0147,  0.0150,  0.0247],\n                      ...,\n                      [ 0.0048, -0.0032, -0.0151,  ..., -0.0401,  0.0236,  0.0164],\n                      [-0.0066,  0.0004,  0.0100,  ...,  0.0137, -0.0042,  0.0152],\n                      [-0.0125, -0.0066,  0.0171,  ..., -0.0269,  0.0326,  0.0116]])),\n             ('model.model.layers.5.self_attn.k_proj.weight',\n              tensor([[ 0.0064,  0.0308, -0.0185,  ..., -0.0056,  0.0133, -0.0031],\n                      [-0.0356,  0.0167, -0.0140,  ..., -0.0142,  0.0204,  0.0006],\n                      [-0.0273,  0.0150, -0.0149,  ..., -0.0047, -0.0207,  0.0160],\n                      ...,\n                      [ 0.0245, -0.0166,  0.0569,  ..., -0.0028, -0.0069,  0.0148],\n                      [ 0.0040,  0.0076, -0.0492,  ...,  0.0447, -0.0093, -0.0109],\n                      [ 0.0129,  0.0033, -0.0330,  ...,  0.0325, -0.0476, -0.0201]])),\n             ('model.model.layers.5.self_attn.v_proj.weight',\n              tensor([[ 0.0048,  0.0033,  0.0234,  ..., -0.0098, -0.0295,  0.0108],\n                      [-0.0097, -0.0180, -0.0349,  ...,  0.0258,  0.0044,  0.0306],\n                      [-0.0045, -0.0256,  0.0286,  ..., -0.0433,  0.0139, -0.0555],\n                      ...,\n                      [ 0.0282,  0.0196,  0.0102,  ..., -0.0017,  0.0360, -0.0266],\n                      [ 0.0008, -0.0103,  0.0349,  ...,  0.0208, -0.0254,  0.0170],\n                      [-0.0092, -0.0071, -0.0137,  ...,  0.0293, -0.0061, -0.0726]])),\n             ('model.model.layers.5.self_attn.o_proj.weight',\n              tensor([[ 1.0012e-02, -5.1363e-03,  1.8942e-03,  ..., -3.3028e-02,\n                       -2.6167e-03,  7.2860e-03],\n                      [-8.6617e-03, -1.3132e-02, -1.7243e-02,  ...,  1.9275e-02,\n                       -7.9378e-03,  8.1271e-03],\n                      [-1.5172e-02, -1.7944e-02, -2.0654e-03,  ...,  1.7947e-02,\n                       -2.3228e-02,  5.0831e-03],\n                      ...,\n                      [ 3.7231e-03,  6.6192e-03,  6.1577e-04,  ...,  1.7952e-02,\n                        1.1207e-02, -1.2663e-02],\n                      [ 9.7173e-03, -1.5234e-02, -7.0140e-03,  ..., -3.2852e-02,\n                        5.4881e-03,  1.6060e-02],\n                      [-5.7121e-03,  6.5649e-03,  7.6106e-05,  ...,  1.2827e-02,\n                       -1.3837e-02, -9.3668e-03]])),\n             ('model.model.layers.5.mlp.gate_proj.weight',\n              tensor([[ 0.0029, -0.0192,  0.0204,  ...,  0.0014, -0.0562,  0.0328],\n                      [-0.0020,  0.0214, -0.0175,  ..., -0.0076,  0.0025,  0.0227],\n                      [ 0.0026,  0.0095, -0.0091,  ..., -0.0107,  0.0005, -0.0019],\n                      ...,\n                      [-0.0257,  0.0207, -0.0046,  ...,  0.0343, -0.0020,  0.0023],\n                      [-0.0201, -0.0267,  0.0036,  ..., -0.0302,  0.0180,  0.0078],\n                      [-0.0030,  0.0136, -0.0129,  ...,  0.0101, -0.0196, -0.0240]])),\n             ('model.model.layers.5.mlp.up_proj.weight',\n              tensor([[ 0.0035,  0.0273,  0.0117,  ...,  0.0024,  0.0262,  0.0303],\n                      [-0.0325, -0.0047,  0.0099,  ..., -0.0280,  0.0051,  0.0003],\n                      [-0.0068,  0.0101,  0.0087,  ...,  0.0087, -0.0050,  0.0081],\n                      ...,\n                      [ 0.0089,  0.0156,  0.0192,  ...,  0.0005,  0.0158, -0.0391],\n                      [-0.0181, -0.0324,  0.0105,  ...,  0.0052, -0.0064,  0.0152],\n                      [ 0.0069, -0.0258, -0.0531,  ..., -0.0119, -0.0401,  0.0344]])),\n             ('model.model.layers.5.mlp.down_proj.weight',\n              tensor([[-0.0105,  0.0100, -0.0022,  ...,  0.0390,  0.0331, -0.0326],\n                      [ 0.0025, -0.0065,  0.0065,  ..., -0.0235,  0.0270, -0.0084],\n                      [ 0.0163,  0.0367,  0.0129,  ...,  0.0241,  0.0249, -0.0139],\n                      ...,\n                      [ 0.0067,  0.0241, -0.0304,  ..., -0.0129, -0.0169, -0.0126],\n                      [-0.0112,  0.0043,  0.0309,  ..., -0.0075, -0.0140, -0.0055],\n                      [-0.0062,  0.0323,  0.0213,  ...,  0.0222, -0.0161, -0.0094]])),\n             ('model.model.layers.5.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.5.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.6.self_attn.q_proj.weight',\n              tensor([[-0.0402,  0.0394, -0.0157,  ..., -0.0081,  0.0491, -0.0096],\n                      [ 0.0171,  0.0016, -0.0261,  ..., -0.0178,  0.0028,  0.0200],\n                      [ 0.0067,  0.0228,  0.0047,  ..., -0.0039,  0.0082, -0.0287],\n                      ...,\n                      [-0.0207,  0.0008,  0.0228,  ...,  0.0057,  0.0049,  0.0110],\n                      [-0.0271, -0.0003,  0.0254,  ..., -0.0325,  0.0160, -0.0012],\n                      [ 0.0051, -0.0076,  0.0155,  ..., -0.0014, -0.0029,  0.0228]])),\n             ('model.model.layers.6.self_attn.k_proj.weight',\n              tensor([[-0.0124,  0.0143, -0.0054,  ...,  0.0116, -0.0055,  0.0153],\n                      [ 0.0222,  0.0401,  0.0283,  ...,  0.0208, -0.0068,  0.0003],\n                      [ 0.0229,  0.0139, -0.0588,  ...,  0.0257,  0.0090,  0.0102],\n                      ...,\n                      [-0.0040, -0.0210,  0.0002,  ...,  0.0078, -0.0089,  0.0007],\n                      [-0.0131,  0.0047, -0.0300,  ..., -0.0091, -0.0139,  0.0174],\n                      [ 0.0128,  0.0204,  0.0099,  ...,  0.0242, -0.0359, -0.0027]])),\n             ('model.model.layers.6.self_attn.v_proj.weight',\n              tensor([[ 6.2345e-03, -2.0020e-03,  5.5809e-05,  ..., -1.5865e-02,\n                       -1.2580e-02, -4.2750e-03],\n                      [-7.9191e-03, -2.2223e-03, -1.6294e-03,  ...,  9.5288e-03,\n                       -8.8500e-03,  2.3936e-02],\n                      [ 2.1296e-02, -1.3572e-02, -3.4163e-03,  ..., -1.5519e-02,\n                        2.2024e-02,  2.8372e-02],\n                      ...,\n                      [-2.4254e-02, -1.2863e-02,  2.7354e-02,  ..., -3.4341e-03,\n                        9.9165e-03,  2.4400e-02],\n                      [-2.7379e-02,  6.9769e-04,  6.5792e-03,  ...,  3.8628e-04,\n                       -1.7621e-03,  1.0275e-02],\n                      [ 4.7517e-03,  2.0941e-03, -3.9823e-02,  ...,  3.9272e-03,\n                        7.2212e-04, -1.4012e-02]])),\n             ('model.model.layers.6.self_attn.o_proj.weight',\n              tensor([[ 2.9779e-03,  1.7778e-02, -2.7982e-02,  ..., -1.5438e-03,\n                        7.4916e-03,  3.3006e-02],\n                      [-1.0308e-03,  2.0747e-02, -2.2336e-02,  ..., -1.0604e-02,\n                        2.2049e-02,  1.8640e-02],\n                      [ 4.0510e-04,  1.1089e-02, -2.8936e-02,  ...,  9.9447e-03,\n                        3.9862e-05,  1.7926e-02],\n                      ...,\n                      [ 6.0419e-04,  1.1446e-02,  3.7680e-03,  ..., -1.9425e-02,\n                       -1.6675e-02, -1.0525e-03],\n                      [ 3.3103e-03, -2.0470e-03, -1.0392e-02,  ..., -7.0993e-03,\n                        1.0856e-02, -2.5903e-02],\n                      [-8.2863e-03,  2.2021e-03, -4.5308e-02,  ...,  3.3954e-03,\n                        1.9629e-02, -6.2353e-03]])),\n             ('model.model.layers.6.mlp.gate_proj.weight',\n              tensor([[-0.0086,  0.0317, -0.0121,  ..., -0.0115, -0.0051,  0.0009],\n                      [-0.0192,  0.0300,  0.0113,  ..., -0.0186,  0.0155,  0.0063],\n                      [ 0.0135, -0.0186,  0.0126,  ...,  0.0236, -0.0169,  0.0037],\n                      ...,\n                      [ 0.0087,  0.0316,  0.0020,  ...,  0.0256, -0.0015, -0.0058],\n                      [-0.0003, -0.0267,  0.0136,  ...,  0.0164, -0.0201,  0.0040],\n                      [ 0.0114, -0.0014,  0.0313,  ...,  0.0060,  0.0004, -0.0348]])),\n             ('model.model.layers.6.mlp.up_proj.weight',\n              tensor([[-0.0501,  0.0071,  0.0232,  ..., -0.0246, -0.0180, -0.0436],\n                      [-0.0171,  0.0270, -0.0125,  ...,  0.0046,  0.0032, -0.0017],\n                      [ 0.0038, -0.0218, -0.0055,  ..., -0.0101,  0.0127,  0.0142],\n                      ...,\n                      [-0.0052, -0.0215,  0.0114,  ...,  0.0102, -0.0061, -0.0072],\n                      [-0.0312, -0.0041, -0.0304,  ...,  0.0095,  0.0132,  0.0131],\n                      [-0.0399, -0.0070, -0.0194,  ..., -0.0309,  0.0164,  0.0020]])),\n             ('model.model.layers.6.mlp.down_proj.weight',\n              tensor([[ 0.0217, -0.0041,  0.0248,  ..., -0.0065,  0.0374,  0.0185],\n                      [-0.0554, -0.0368,  0.0263,  ..., -0.0146, -0.0246, -0.0265],\n                      [ 0.0037,  0.0044,  0.0112,  ..., -0.0348, -0.0464,  0.0402],\n                      ...,\n                      [ 0.0340,  0.0135,  0.0218,  ...,  0.0030, -0.0130,  0.0335],\n                      [-0.0271, -0.0288,  0.0270,  ..., -0.0004,  0.0324,  0.0051],\n                      [-0.0109, -0.0087, -0.0100,  ...,  0.0235,  0.0450, -0.0153]])),\n             ('model.model.layers.6.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.6.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.7.self_attn.q_proj.weight',\n              tensor([[-0.0259, -0.0247,  0.0010,  ..., -0.0037,  0.0070, -0.0065],\n                      [-0.0220,  0.0144,  0.0063,  ...,  0.0102,  0.0399, -0.0225],\n                      [ 0.0118, -0.0102, -0.0065,  ...,  0.0335, -0.0011,  0.0259],\n                      ...,\n                      [-0.0071, -0.0101, -0.0094,  ..., -0.0403, -0.0071,  0.0248],\n                      [-0.0064, -0.0212,  0.0140,  ..., -0.0610,  0.0204,  0.0024],\n                      [ 0.0101,  0.0003,  0.0036,  ...,  0.0221,  0.0114,  0.0353]])),\n             ('model.model.layers.7.self_attn.k_proj.weight',\n              tensor([[ 0.0061,  0.0210,  0.0118,  ..., -0.0183, -0.0032, -0.0024],\n                      [ 0.0041, -0.0172, -0.0121,  ..., -0.0202,  0.0067,  0.0023],\n                      [-0.0258, -0.0014, -0.0035,  ..., -0.0248,  0.0262,  0.0078],\n                      ...,\n                      [ 0.0048, -0.0099, -0.0169,  ..., -0.0486,  0.0336, -0.0053],\n                      [ 0.0155, -0.0333,  0.0177,  ...,  0.0118, -0.0349, -0.0052],\n                      [ 0.0064,  0.0091,  0.0004,  ...,  0.0119, -0.0304,  0.0151]])),\n             ('model.model.layers.7.self_attn.v_proj.weight',\n              tensor([[-0.0266,  0.0067,  0.0086,  ...,  0.0444, -0.0249,  0.0002],\n                      [ 0.0021, -0.0123,  0.0138,  ...,  0.0234, -0.0275, -0.0152],\n                      [ 0.0074,  0.0243,  0.0040,  ...,  0.0212,  0.0019, -0.0293],\n                      ...,\n                      [ 0.0379,  0.0094, -0.0064,  ..., -0.0236,  0.0285,  0.0074],\n                      [-0.0102, -0.0017,  0.0236,  ...,  0.0055,  0.0513, -0.0076],\n                      [ 0.0236, -0.0234, -0.0143,  ..., -0.0343, -0.0187, -0.0207]])),\n             ('model.model.layers.7.self_attn.o_proj.weight',\n              tensor([[-0.0277,  0.0294,  0.0062,  ..., -0.0059,  0.0181,  0.0164],\n                      [-0.0014, -0.0021, -0.0166,  ...,  0.0225, -0.0051, -0.0110],\n                      [-0.0139,  0.0137, -0.0055,  ...,  0.0112, -0.0204, -0.0006],\n                      ...,\n                      [-0.0219, -0.0051,  0.0379,  ..., -0.0168,  0.0073,  0.0122],\n                      [ 0.0112,  0.0013, -0.0142,  ...,  0.0707,  0.0145,  0.0006],\n                      [ 0.0089,  0.0105, -0.0304,  ..., -0.0296, -0.0088,  0.0347]])),\n             ('model.model.layers.7.mlp.gate_proj.weight',\n              tensor([[ 2.1817e-02, -1.5219e-02,  1.3900e-02,  ..., -6.6538e-03,\n                       -2.9014e-03, -1.6024e-02],\n                      [-3.4034e-04, -2.4903e-02,  2.8114e-02,  ...,  8.3017e-03,\n                       -1.9538e-02, -8.6671e-03],\n                      [ 2.6420e-03,  1.1113e-02,  3.5898e-03,  ..., -2.1501e-02,\n                        1.5524e-02, -5.1488e-04],\n                      ...,\n                      [ 3.4867e-02,  3.7109e-02, -5.2306e-03,  ...,  2.1320e-02,\n                        4.8332e-03,  3.4624e-03],\n                      [-1.6325e-03, -2.2943e-02,  1.4957e-03,  ..., -1.6236e-02,\n                       -9.7784e-03, -1.9582e-02],\n                      [ 2.9233e-02, -4.4998e-02, -1.3420e-02,  ..., -6.7182e-03,\n                       -1.2492e-02,  9.4862e-05]])),\n             ('model.model.layers.7.mlp.up_proj.weight',\n              tensor([[ 0.0182,  0.0433,  0.0090,  ..., -0.0205,  0.0082,  0.0322],\n                      [ 0.0028, -0.0031, -0.0119,  ...,  0.0019,  0.0521,  0.0020],\n                      [ 0.0194,  0.0124,  0.0186,  ...,  0.0202, -0.0315, -0.0064],\n                      ...,\n                      [ 0.0177,  0.0117,  0.0193,  ...,  0.0269, -0.0311, -0.0138],\n                      [ 0.0114, -0.0044,  0.0135,  ...,  0.0112, -0.0360,  0.0161],\n                      [-0.0019, -0.0078,  0.0135,  ..., -0.0281,  0.0297, -0.0134]])),\n             ('model.model.layers.7.mlp.down_proj.weight',\n              tensor([[ 0.0053, -0.0055,  0.0040,  ..., -0.0174,  0.0340,  0.0031],\n                      [-0.0114, -0.0337,  0.0039,  ..., -0.0019, -0.0124, -0.0209],\n                      [ 0.0033, -0.0203, -0.0512,  ...,  0.0014,  0.0091,  0.0237],\n                      ...,\n                      [ 0.0465, -0.0010,  0.0395,  ...,  0.0024,  0.0130,  0.0023],\n                      [-0.0084, -0.0288, -0.0338,  ..., -0.0123,  0.0115,  0.0109],\n                      [-0.0161, -0.0092, -0.0319,  ..., -0.0304, -0.0151,  0.0269]])),\n             ('model.model.layers.7.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.7.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.8.self_attn.q_proj.weight',\n              tensor([[-0.0286, -0.0022,  0.0129,  ...,  0.0227, -0.0073, -0.0007],\n                      [-0.0198,  0.0169,  0.0105,  ..., -0.0298, -0.0420, -0.0106],\n                      [ 0.0264, -0.0186,  0.0023,  ...,  0.0085, -0.0148, -0.0025],\n                      ...,\n                      [ 0.0257,  0.0004,  0.0092,  ..., -0.0101, -0.0171,  0.0133],\n                      [ 0.0126, -0.0166, -0.0047,  ..., -0.0181,  0.0197,  0.0012],\n                      [ 0.0200,  0.0406,  0.0008,  ..., -0.0017,  0.0244, -0.0089]])),\n             ('model.model.layers.8.self_attn.k_proj.weight',\n              tensor([[ 0.0089, -0.0167, -0.0139,  ...,  0.0010, -0.0172,  0.0070],\n                      [ 0.0061, -0.0078, -0.0041,  ...,  0.0032,  0.0076,  0.0282],\n                      [-0.0300, -0.0197,  0.0051,  ...,  0.0209,  0.0501,  0.0202],\n                      ...,\n                      [-0.0091, -0.0305,  0.0440,  ...,  0.0072,  0.0493,  0.0411],\n                      [-0.0179,  0.0135, -0.0422,  ...,  0.0063, -0.0069, -0.0231],\n                      [ 0.0034,  0.0209,  0.0238,  ...,  0.0185,  0.0137,  0.0194]])),\n             ('model.model.layers.8.self_attn.v_proj.weight',\n              tensor([[-0.0038,  0.0118, -0.0099,  ..., -0.0083,  0.0444, -0.0260],\n                      [ 0.0115,  0.0096, -0.0147,  ..., -0.0313, -0.0029,  0.0029],\n                      [ 0.0158, -0.0343,  0.0088,  ...,  0.0174,  0.0048, -0.0246],\n                      ...,\n                      [-0.0157,  0.0098, -0.0051,  ..., -0.0114,  0.0337, -0.0083],\n                      [ 0.0252, -0.0157,  0.0206,  ..., -0.0380,  0.0156,  0.0286],\n                      [ 0.0171, -0.0144,  0.0097,  ...,  0.0179,  0.0268,  0.0113]])),\n             ('model.model.layers.8.self_attn.o_proj.weight',\n              tensor([[ 0.0209,  0.0115,  0.0319,  ...,  0.0003,  0.0095,  0.0345],\n                      [ 0.0353, -0.0053,  0.0320,  ..., -0.0017, -0.0252,  0.0061],\n                      [ 0.0030, -0.0088, -0.0213,  ...,  0.0184, -0.0015,  0.0138],\n                      ...,\n                      [ 0.0032,  0.0009, -0.0546,  ..., -0.0322,  0.0140,  0.0323],\n                      [-0.0252,  0.0239,  0.0058,  ...,  0.0166,  0.0318, -0.0054],\n                      [-0.0261, -0.0048,  0.0100,  ..., -0.0076,  0.0070, -0.0070]])),\n             ('model.model.layers.8.mlp.gate_proj.weight',\n              tensor([[-0.0356, -0.0129, -0.0280,  ..., -0.0089, -0.0189, -0.0108],\n                      [-0.0032, -0.0245,  0.0018,  ..., -0.0363,  0.0372, -0.0284],\n                      [ 0.0065, -0.0043, -0.0136,  ..., -0.0420, -0.0177,  0.0119],\n                      ...,\n                      [-0.0538,  0.0080, -0.0113,  ...,  0.0051, -0.0101, -0.0023],\n                      [ 0.0047, -0.0066, -0.0234,  ..., -0.0083, -0.0242,  0.0107],\n                      [ 0.0409,  0.0145, -0.0020,  ...,  0.0078, -0.0159,  0.0299]])),\n             ('model.model.layers.8.mlp.up_proj.weight',\n              tensor([[ 1.9346e-02, -4.1483e-03, -1.5404e-03,  ..., -1.1819e-02,\n                       -1.4171e-02, -4.9818e-02],\n                      [-3.0000e-03,  2.7537e-02, -5.3668e-04,  ..., -9.0527e-05,\n                       -1.0480e-02,  1.7428e-02],\n                      [-6.4788e-03,  2.2392e-02, -1.7403e-02,  ...,  7.9856e-03,\n                        8.3367e-03,  8.4829e-03],\n                      ...,\n                      [-1.9395e-02, -2.5579e-02,  1.6016e-02,  ..., -1.6850e-02,\n                       -7.6692e-03,  4.4687e-03],\n                      [ 4.5291e-03, -1.2841e-02, -5.8825e-03,  ...,  2.0084e-03,\n                       -2.0496e-02, -2.2465e-02],\n                      [-8.8161e-03, -1.2655e-02,  2.0864e-02,  ...,  2.1386e-02,\n                        1.9433e-02, -2.1305e-02]])),\n             ('model.model.layers.8.mlp.down_proj.weight',\n              tensor([[ 0.0080,  0.0124, -0.0195,  ...,  0.0432, -0.0026, -0.0348],\n                      [-0.0236,  0.0231,  0.0043,  ..., -0.0029, -0.0207, -0.0086],\n                      [-0.0062,  0.0034,  0.0040,  ...,  0.0308,  0.0118, -0.0216],\n                      ...,\n                      [-0.0093,  0.0266,  0.0005,  ..., -0.0075,  0.0251, -0.0192],\n                      [ 0.0162, -0.0040, -0.0275,  ...,  0.0022, -0.0237, -0.0041],\n                      [ 0.0056, -0.0253, -0.0169,  ..., -0.0352, -0.0155, -0.0104]])),\n             ('model.model.layers.8.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.8.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.9.self_attn.q_proj.weight',\n              tensor([[-0.0291,  0.0083, -0.0032,  ...,  0.0149,  0.0548, -0.0125],\n                      [ 0.0137,  0.0213, -0.0128,  ...,  0.0134,  0.0097,  0.0039],\n                      [-0.0040, -0.0077,  0.0327,  ..., -0.0141,  0.0308,  0.0372],\n                      ...,\n                      [ 0.0207, -0.0034,  0.0333,  ..., -0.0141,  0.0258, -0.0082],\n                      [-0.0163,  0.0497, -0.0131,  ...,  0.0134,  0.0340,  0.0232],\n                      [ 0.0152, -0.0015, -0.0179,  ...,  0.0210, -0.0058, -0.0217]])),\n             ('model.model.layers.9.self_attn.k_proj.weight',\n              tensor([[-0.0257, -0.0113, -0.0027,  ..., -0.0197, -0.0006, -0.0212],\n                      [-0.0125,  0.0301,  0.0232,  ...,  0.0102,  0.0118,  0.0104],\n                      [-0.0102,  0.0137,  0.0096,  ..., -0.0214,  0.0290,  0.0212],\n                      ...,\n                      [ 0.0123,  0.0298,  0.0061,  ...,  0.0078, -0.0048, -0.0118],\n                      [-0.0212, -0.0026, -0.0284,  ...,  0.0017, -0.0157,  0.0337],\n                      [ 0.0215,  0.0350, -0.0377,  ...,  0.0241,  0.0067,  0.0031]])),\n             ('model.model.layers.9.self_attn.v_proj.weight',\n              tensor([[-0.0129,  0.0110,  0.0025,  ..., -0.0012, -0.0057, -0.0170],\n                      [-0.0270,  0.0176,  0.0212,  ...,  0.0280, -0.0028, -0.0232],\n                      [ 0.0042, -0.0083, -0.0123,  ...,  0.0179, -0.0171,  0.0182],\n                      ...,\n                      [ 0.0026, -0.0105,  0.0074,  ..., -0.0336,  0.0472,  0.0214],\n                      [ 0.0203, -0.0082, -0.0133,  ...,  0.0280,  0.0025, -0.0253],\n                      [ 0.0181,  0.0284,  0.0135,  ..., -0.0165, -0.0115,  0.0134]])),\n             ('model.model.layers.9.self_attn.o_proj.weight',\n              tensor([[ 0.0290,  0.0100,  0.0144,  ...,  0.0144, -0.0128,  0.0083],\n                      [-0.0344,  0.0108,  0.0160,  ..., -0.0195, -0.0285, -0.0101],\n                      [ 0.0077, -0.0334, -0.0139,  ...,  0.0148,  0.0067,  0.0055],\n                      ...,\n                      [ 0.0149,  0.0123,  0.0150,  ..., -0.0074,  0.0133, -0.0023],\n                      [ 0.0103,  0.0097, -0.0246,  ...,  0.0020,  0.0046, -0.0135],\n                      [ 0.0141, -0.0157, -0.0047,  ..., -0.0374,  0.0049,  0.0284]])),\n             ('model.model.layers.9.mlp.gate_proj.weight',\n              tensor([[ 0.0083, -0.0017, -0.0186,  ..., -0.0045,  0.0310,  0.0135],\n                      [-0.0287,  0.0134, -0.0023,  ...,  0.0054, -0.0025,  0.0102],\n                      [ 0.0277, -0.0107,  0.0227,  ...,  0.0253,  0.0137, -0.0147],\n                      ...,\n                      [ 0.0094,  0.0110, -0.0010,  ..., -0.0140,  0.0040, -0.0014],\n                      [ 0.0448,  0.0049,  0.0099,  ..., -0.0459,  0.0180, -0.0252],\n                      [ 0.0010, -0.0151,  0.0139,  ...,  0.0297,  0.0120, -0.0341]])),\n             ('model.model.layers.9.mlp.up_proj.weight',\n              tensor([[-0.0004,  0.0148,  0.0273,  ...,  0.0110,  0.0661, -0.0273],\n                      [-0.0126, -0.0329,  0.0118,  ...,  0.0125,  0.0229,  0.0046],\n                      [-0.0236,  0.0178, -0.0097,  ..., -0.0259,  0.0134,  0.0001],\n                      ...,\n                      [-0.0182, -0.0015,  0.0284,  ..., -0.0147, -0.0068, -0.0202],\n                      [ 0.0373, -0.0103,  0.0393,  ..., -0.0144,  0.0008,  0.0028],\n                      [ 0.0164, -0.0172, -0.0468,  ..., -0.0260, -0.0040,  0.0014]])),\n             ('model.model.layers.9.mlp.down_proj.weight',\n              tensor([[ 0.0094, -0.0248,  0.0123,  ..., -0.0005, -0.0123,  0.0208],\n                      [ 0.0198, -0.0202, -0.0132,  ...,  0.0182,  0.0055, -0.0061],\n                      [-0.0060,  0.0023,  0.0013,  ...,  0.0217,  0.0077, -0.0284],\n                      ...,\n                      [ 0.0082, -0.0093, -0.0096,  ..., -0.0168, -0.0335,  0.0073],\n                      [ 0.0101,  0.0294,  0.0214,  ..., -0.0104,  0.0311,  0.0031],\n                      [ 0.0117, -0.0013,  0.0360,  ..., -0.0129, -0.0127,  0.0001]])),\n             ('model.model.layers.9.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.9.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.10.self_attn.q_proj.weight',\n              tensor([[ 0.0256, -0.0184, -0.0102,  ...,  0.0037, -0.0139,  0.0165],\n                      [ 0.0172, -0.0008, -0.0051,  ...,  0.0262,  0.0248, -0.0221],\n                      [ 0.0049, -0.0054,  0.0275,  ..., -0.0087,  0.0238,  0.0048],\n                      ...,\n                      [-0.0156, -0.0121, -0.0109,  ...,  0.0620,  0.0049, -0.0313],\n                      [-0.0202,  0.0006,  0.0008,  ..., -0.0127,  0.0117, -0.0160],\n                      [-0.0153, -0.0007, -0.0291,  ..., -0.0267, -0.0130, -0.0065]])),\n             ('model.model.layers.10.self_attn.k_proj.weight',\n              tensor([[-2.4933e-02, -7.7926e-03, -4.5191e-03,  ...,  5.2047e-03,\n                        2.2437e-02, -6.2909e-03],\n                      [-1.2211e-02,  5.0221e-02,  9.9715e-03,  ..., -5.9366e-05,\n                        3.0936e-02, -1.2835e-02],\n                      [ 6.9397e-03,  2.2074e-02,  4.1594e-02,  ..., -6.9866e-04,\n                       -2.1014e-03, -1.1274e-02],\n                      ...,\n                      [-2.9626e-02,  1.7195e-02, -1.3225e-02,  ...,  1.3527e-02,\n                        1.5045e-02, -1.4507e-02],\n                      [-2.6072e-02, -4.1398e-02, -2.1927e-02,  ..., -2.0583e-03,\n                       -1.6606e-02,  7.2665e-04],\n                      [ 1.8213e-02,  3.4269e-03, -2.5749e-04,  ...,  6.6587e-03,\n                       -1.9142e-02, -7.8885e-03]])),\n             ('model.model.layers.10.self_attn.v_proj.weight',\n              tensor([[ 0.0187, -0.0024,  0.0033,  ..., -0.0076,  0.0095,  0.0245],\n                      [ 0.0109, -0.0065, -0.0232,  ...,  0.0230,  0.0094,  0.0075],\n                      [-0.0248, -0.0067,  0.0004,  ..., -0.0064,  0.0008,  0.0304],\n                      ...,\n                      [ 0.0043,  0.0053, -0.0070,  ...,  0.0002,  0.0128,  0.0585],\n                      [-0.0001, -0.0108, -0.0020,  ..., -0.0245,  0.0179,  0.0145],\n                      [-0.0293, -0.0101, -0.0128,  ...,  0.0048,  0.0007, -0.0107]])),\n             ('model.model.layers.10.self_attn.o_proj.weight',\n              tensor([[ 2.0606e-02, -1.3015e-02, -9.0243e-03,  ...,  1.4752e-03,\n                        2.3658e-02, -1.4706e-02],\n                      [ 2.1606e-02, -2.9156e-02, -3.4145e-03,  ...,  2.0499e-03,\n                        6.8293e-03, -2.3114e-02],\n                      [-2.4999e-02, -2.6330e-03,  2.0933e-02,  ...,  4.4719e-03,\n                       -3.6027e-05, -1.1230e-02],\n                      ...,\n                      [ 1.3524e-03, -4.2103e-03,  2.2537e-02,  ...,  7.3997e-03,\n                        2.8753e-02, -1.7502e-02],\n                      [ 1.8724e-02, -5.7249e-03,  2.9648e-02,  ..., -1.8492e-02,\n                       -2.3199e-03, -1.1650e-02],\n                      [ 4.7097e-03, -3.4360e-02,  1.2594e-02,  ..., -5.6573e-03,\n                       -1.7368e-02, -4.1805e-02]])),\n             ('model.model.layers.10.mlp.gate_proj.weight',\n              tensor([[-0.0057, -0.0061,  0.0059,  ..., -0.0214,  0.0074, -0.0139],\n                      [-0.0220,  0.0395, -0.0023,  ..., -0.0019,  0.0139, -0.0078],\n                      [ 0.0093,  0.0143,  0.0116,  ...,  0.0186,  0.0117,  0.0092],\n                      ...,\n                      [-0.0016, -0.0195,  0.0197,  ..., -0.0034,  0.0051,  0.0020],\n                      [-0.0388,  0.0442,  0.0172,  ..., -0.0024, -0.0126, -0.0199],\n                      [-0.0134, -0.0066, -0.0006,  ..., -0.0240, -0.0090, -0.0384]])),\n             ('model.model.layers.10.mlp.up_proj.weight',\n              tensor([[ 0.0212,  0.0082, -0.0128,  ...,  0.0212,  0.0276,  0.0008],\n                      [ 0.0070, -0.0225, -0.0104,  ..., -0.0208, -0.0030, -0.0011],\n                      [-0.0171, -0.0236, -0.0366,  ...,  0.0104,  0.0293,  0.0297],\n                      ...,\n                      [-0.0167, -0.0154,  0.0070,  ...,  0.0073,  0.0263,  0.0018],\n                      [-0.0132,  0.0244, -0.0067,  ...,  0.0310, -0.0111,  0.0025],\n                      [ 0.0032,  0.0287, -0.0021,  ..., -0.0359, -0.0024, -0.0105]])),\n             ('model.model.layers.10.mlp.down_proj.weight',\n              tensor([[-2.7167e-02, -2.2347e-02, -2.8817e-03,  ...,  4.9988e-03,\n                        8.5966e-03, -8.7307e-03],\n                      [ 1.6439e-02,  3.1393e-02, -1.2006e-02,  ..., -2.8673e-02,\n                       -3.8818e-03,  3.0071e-03],\n                      [-3.6224e-02, -1.7084e-03, -1.4900e-02,  ...,  1.8017e-02,\n                       -2.1462e-04, -1.5807e-02],\n                      ...,\n                      [ 2.5141e-02,  1.4206e-02, -1.0846e-02,  ...,  1.3639e-02,\n                       -2.9071e-02,  1.5167e-02],\n                      [ 2.7733e-02, -8.8057e-03,  2.0770e-02,  ...,  4.5392e-03,\n                       -2.0232e-02,  3.3104e-03],\n                      [ 7.7444e-03,  3.7960e-02, -3.5923e-05,  ...,  1.1237e-02,\n                        1.5482e-02,  1.4171e-02]])),\n             ('model.model.layers.10.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.10.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.11.self_attn.q_proj.weight',\n              tensor([[ 0.0078, -0.0375, -0.0378,  ..., -0.0036,  0.0059,  0.0128],\n                      [ 0.0026, -0.0034,  0.0111,  ...,  0.0120,  0.0044, -0.0202],\n                      [-0.0173,  0.0109,  0.0067,  ..., -0.0037, -0.0037, -0.0065],\n                      ...,\n                      [-0.0087, -0.0303,  0.0069,  ..., -0.0182,  0.0158, -0.0100],\n                      [ 0.0300, -0.0315,  0.0063,  ..., -0.0279,  0.0168, -0.0277],\n                      [-0.0038,  0.0212,  0.0019,  ..., -0.0065, -0.0064, -0.0182]])),\n             ('model.model.layers.11.self_attn.k_proj.weight',\n              tensor([[-6.6362e-03, -1.8993e-02,  3.0457e-05,  ..., -6.7361e-03,\n                        1.4256e-02, -3.7751e-03],\n                      [-5.0989e-02,  6.0300e-03,  5.7792e-03,  ..., -1.2176e-02,\n                        1.3384e-02,  7.0663e-03],\n                      [-5.9340e-03, -7.8534e-03,  2.2787e-02,  ...,  1.1675e-02,\n                        1.0047e-03,  1.9972e-02],\n                      ...,\n                      [-7.8975e-03,  5.1505e-04,  7.7343e-03,  ...,  9.9255e-03,\n                       -1.4726e-02, -4.5209e-03],\n                      [-1.3214e-02, -9.4871e-03,  1.9562e-02,  ...,  2.4510e-03,\n                        6.8287e-03, -9.1607e-03],\n                      [-5.1226e-02, -1.0003e-02,  3.6776e-02,  ...,  1.8640e-02,\n                       -3.2466e-03,  8.1710e-03]])),\n             ('model.model.layers.11.self_attn.v_proj.weight',\n              tensor([[-0.0073, -0.0212,  0.0308,  ..., -0.0032, -0.0223, -0.0005],\n                      [-0.0078, -0.0301, -0.0147,  ..., -0.0176, -0.0141,  0.0271],\n                      [ 0.0281, -0.0161,  0.0316,  ..., -0.0162, -0.0128,  0.0239],\n                      ...,\n                      [-0.0298, -0.0102, -0.0401,  ..., -0.0201, -0.0303,  0.0086],\n                      [ 0.0177, -0.0539,  0.0108,  ...,  0.0033,  0.0049,  0.0085],\n                      [ 0.0059,  0.0233, -0.0027,  ...,  0.0024, -0.0015,  0.0131]])),\n             ('model.model.layers.11.self_attn.o_proj.weight',\n              tensor([[-0.0174,  0.0327,  0.0026,  ..., -0.0185,  0.0405,  0.0147],\n                      [ 0.0128, -0.0029,  0.0130,  ...,  0.0087,  0.0044, -0.0209],\n                      [ 0.0233, -0.0240, -0.0437,  ...,  0.0103,  0.0173, -0.0141],\n                      ...,\n                      [ 0.0201, -0.0201,  0.0046,  ..., -0.0467,  0.0053, -0.0179],\n                      [-0.0249, -0.0290,  0.0265,  ..., -0.0202,  0.0125,  0.0417],\n                      [-0.0300, -0.0138,  0.0273,  ...,  0.0314, -0.0088, -0.0202]])),\n             ('model.model.layers.11.mlp.gate_proj.weight',\n              tensor([[-0.0255, -0.0235,  0.0054,  ..., -0.0282, -0.0073,  0.0179],\n                      [ 0.0183, -0.0184, -0.0092,  ..., -0.0052, -0.0149,  0.0557],\n                      [-0.0197, -0.0006,  0.0155,  ...,  0.0051, -0.0212,  0.0028],\n                      ...,\n                      [-0.0243, -0.0084,  0.0051,  ..., -0.0224, -0.0248, -0.0079],\n                      [ 0.0301,  0.0159,  0.0273,  ..., -0.0006, -0.0179,  0.0177],\n                      [-0.0113, -0.0148,  0.0086,  ...,  0.0077,  0.0123, -0.0277]])),\n             ('model.model.layers.11.mlp.up_proj.weight',\n              tensor([[ 0.0225,  0.0280, -0.0160,  ...,  0.0187, -0.0315,  0.0019],\n                      [-0.0013,  0.0047, -0.0292,  ...,  0.0339, -0.0158,  0.0055],\n                      [ 0.0018,  0.0301,  0.0026,  ...,  0.0461,  0.0225, -0.0124],\n                      ...,\n                      [-0.0274,  0.0010,  0.0033,  ..., -0.0292, -0.0016,  0.0126],\n                      [-0.0232, -0.0078, -0.0267,  ..., -0.0056,  0.0551, -0.0133],\n                      [ 0.0057, -0.0093,  0.0321,  ..., -0.0347,  0.0138,  0.0349]])),\n             ('model.model.layers.11.mlp.down_proj.weight',\n              tensor([[-0.0236, -0.0093, -0.0062,  ..., -0.0237,  0.0212,  0.0079],\n                      [-0.0044,  0.0123, -0.0277,  ..., -0.0007,  0.0223, -0.0149],\n                      [-0.0260,  0.0199, -0.0323,  ...,  0.0154, -0.0306, -0.0047],\n                      ...,\n                      [ 0.0041,  0.0008,  0.0151,  ..., -0.0339, -0.0047,  0.0569],\n                      [-0.0266,  0.0107,  0.0356,  ..., -0.0090,  0.0181, -0.0005],\n                      [-0.0118, -0.0039,  0.0235,  ..., -0.0431,  0.0062,  0.0275]])),\n             ('model.model.layers.11.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.11.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.12.self_attn.q_proj.weight',\n              tensor([[ 0.0368, -0.0006, -0.0028,  ..., -0.0219,  0.0068, -0.0375],\n                      [-0.0036, -0.0314,  0.0047,  ...,  0.0135, -0.0184, -0.0260],\n                      [ 0.0042,  0.0010,  0.0035,  ...,  0.0003, -0.0297, -0.0071],\n                      ...,\n                      [ 0.0201, -0.0129,  0.0230,  ..., -0.0002, -0.0219, -0.0396],\n                      [-0.0031, -0.0086, -0.0260,  ...,  0.0114, -0.0175, -0.0297],\n                      [-0.0153,  0.0113, -0.0059,  ...,  0.0166, -0.0131,  0.0075]])),\n             ('model.model.layers.12.self_attn.k_proj.weight',\n              tensor([[-0.0390,  0.0045, -0.0081,  ..., -0.0126, -0.0044, -0.0018],\n                      [-0.0368, -0.0211, -0.0104,  ...,  0.0298, -0.0374,  0.0160],\n                      [-0.0030,  0.0099,  0.0075,  ..., -0.0186,  0.0142, -0.0165],\n                      ...,\n                      [-0.0173,  0.0095,  0.0259,  ...,  0.0212, -0.0019, -0.0152],\n                      [ 0.0025, -0.0042, -0.0062,  ...,  0.0159,  0.0065, -0.0147],\n                      [ 0.0067, -0.0119, -0.0080,  ..., -0.0089,  0.0063, -0.0276]])),\n             ('model.model.layers.12.self_attn.v_proj.weight',\n              tensor([[ 0.0394,  0.0151,  0.0188,  ...,  0.0175, -0.0006,  0.0077],\n                      [ 0.0261,  0.0016,  0.0413,  ...,  0.0090, -0.0058,  0.0242],\n                      [-0.0274, -0.0148,  0.0102,  ...,  0.0222, -0.0176,  0.0443],\n                      ...,\n                      [-0.0347, -0.0180, -0.0459,  ..., -0.0068, -0.0051, -0.0158],\n                      [-0.0108, -0.0061, -0.0122,  ...,  0.0039,  0.0107, -0.0019],\n                      [-0.0102,  0.0256, -0.0324,  ...,  0.0336,  0.0072, -0.0054]])),\n             ('model.model.layers.12.self_attn.o_proj.weight',\n              tensor([[-1.2209e-03,  1.5000e-02,  5.0011e-04,  ..., -9.2660e-03,\n                        1.1823e-02, -1.1066e-02],\n                      [-1.0870e-02,  8.5480e-03, -1.1153e-02,  ...,  2.6697e-02,\n                        1.9678e-02,  1.0452e-03],\n                      [ 1.6338e-02, -9.0632e-03, -7.7635e-05,  ..., -2.3905e-02,\n                        1.6261e-02, -4.2949e-03],\n                      ...,\n                      [ 3.7979e-03, -3.4249e-03,  2.4422e-02,  ..., -1.9412e-02,\n                       -9.5565e-03,  2.7074e-02],\n                      [ 2.0126e-02, -2.1931e-02, -2.8326e-02,  ...,  6.2281e-03,\n                       -9.9313e-03, -5.5090e-03],\n                      [ 8.3597e-04,  1.3665e-02,  1.7861e-02,  ...,  5.2376e-03,\n                       -9.1054e-03, -5.5393e-03]])),\n             ('model.model.layers.12.mlp.gate_proj.weight',\n              tensor([[ 2.7333e-02,  8.3497e-05, -6.9038e-03,  ..., -2.3803e-02,\n                       -7.9039e-03,  3.8420e-02],\n                      [-1.7565e-02, -2.0721e-02, -5.7831e-02,  ..., -1.9447e-02,\n                        2.5153e-02, -1.3267e-02],\n                      [ 9.3021e-03, -2.5355e-03,  2.4181e-03,  ..., -1.0753e-03,\n                       -4.0669e-02, -2.6635e-03],\n                      ...,\n                      [-3.0316e-03,  1.5040e-02,  2.5411e-02,  ...,  1.6447e-02,\n                        5.7963e-03, -1.9876e-02],\n                      [ 8.8400e-03, -1.3840e-03, -1.2438e-02,  ...,  4.0346e-02,\n                        8.7952e-03, -8.5861e-03],\n                      [ 5.4110e-03, -3.2703e-04,  1.4646e-02,  ..., -1.2505e-02,\n                       -8.0292e-03,  1.1458e-03]])),\n             ('model.model.layers.12.mlp.up_proj.weight',\n              tensor([[-0.0238, -0.0044,  0.0187,  ...,  0.0026, -0.0045, -0.0165],\n                      [-0.0282, -0.0168,  0.0183,  ..., -0.0099,  0.0176, -0.0025],\n                      [-0.0354,  0.0499,  0.0082,  ..., -0.0229,  0.0085, -0.0061],\n                      ...,\n                      [ 0.0125, -0.0348,  0.0071,  ..., -0.0444,  0.0323, -0.0192],\n                      [-0.0087, -0.0114, -0.0067,  ...,  0.0260,  0.0077,  0.0127],\n                      [ 0.0001,  0.0121, -0.0180,  ..., -0.0100, -0.0257, -0.0058]])),\n             ('model.model.layers.12.mlp.down_proj.weight',\n              tensor([[-3.1695e-02,  8.8460e-03, -3.0678e-02,  ..., -2.9774e-03,\n                        1.9014e-02,  1.2163e-02],\n                      [ 9.7207e-03,  6.4181e-04,  1.4417e-02,  ..., -9.6805e-03,\n                        9.6247e-03,  1.1147e-02],\n                      [ 2.3134e-02,  1.1606e-02,  8.8171e-03,  ..., -6.0868e-03,\n                        7.3239e-03, -2.7159e-02],\n                      ...,\n                      [ 2.6924e-02,  1.4879e-02,  1.1090e-02,  ...,  2.1404e-02,\n                        2.2730e-02,  3.3147e-02],\n                      [-3.2526e-02,  1.9343e-02, -2.2505e-03,  ..., -2.8634e-03,\n                       -3.4149e-03, -3.0507e-02],\n                      [ 7.5763e-03, -3.6213e-05,  1.1594e-02,  ..., -9.6967e-04,\n                        1.7821e-02, -3.5251e-03]])),\n             ('model.model.layers.12.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.12.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.13.self_attn.q_proj.weight',\n              tensor([[ 0.0290, -0.0225, -0.0371,  ...,  0.0036, -0.0049,  0.0027],\n                      [-0.0210, -0.0020, -0.0106,  ...,  0.0017, -0.0241, -0.0084],\n                      [-0.0233,  0.0004, -0.0245,  ..., -0.0183,  0.0288, -0.0075],\n                      ...,\n                      [ 0.0362,  0.0188, -0.0215,  ...,  0.0168, -0.0140, -0.0030],\n                      [-0.0047,  0.0052,  0.0078,  ..., -0.0081,  0.0116,  0.0034],\n                      [ 0.0048, -0.0181, -0.0107,  ...,  0.0093, -0.0126,  0.0017]])),\n             ('model.model.layers.13.self_attn.k_proj.weight',\n              tensor([[ 0.0426, -0.0074,  0.0074,  ..., -0.0052, -0.0222,  0.0191],\n                      [-0.0072,  0.0050, -0.0353,  ..., -0.0287,  0.0204,  0.0228],\n                      [ 0.0045,  0.0105,  0.0231,  ..., -0.0066,  0.0014, -0.0011],\n                      ...,\n                      [-0.0108,  0.0152, -0.0031,  ..., -0.0160, -0.0042,  0.0082],\n                      [-0.0128,  0.0045,  0.0086,  ..., -0.0316,  0.0250,  0.0322],\n                      [ 0.0040,  0.0014, -0.0006,  ..., -0.0172, -0.0172,  0.0278]])),\n             ('model.model.layers.13.self_attn.v_proj.weight',\n              tensor([[ 0.0020, -0.0194,  0.0070,  ...,  0.0137,  0.0182, -0.0197],\n                      [-0.0059, -0.0225, -0.0108,  ..., -0.0022, -0.0054, -0.0061],\n                      [ 0.0037, -0.0005, -0.0017,  ...,  0.0120, -0.0219,  0.0161],\n                      ...,\n                      [-0.0048,  0.0225,  0.0036,  ..., -0.0129,  0.0012,  0.0158],\n                      [ 0.0515, -0.0063,  0.0255,  ..., -0.0276,  0.0202, -0.0058],\n                      [ 0.0185,  0.0132,  0.0163,  ...,  0.0439,  0.0408, -0.0229]])),\n             ('model.model.layers.13.self_attn.o_proj.weight',\n              tensor([[-0.0165, -0.0078, -0.0094,  ..., -0.0007, -0.0181,  0.0139],\n                      [ 0.0047,  0.0041, -0.0487,  ..., -0.0037, -0.0150,  0.0134],\n                      [-0.0157, -0.0197, -0.0411,  ...,  0.0227, -0.0111,  0.0066],\n                      ...,\n                      [-0.0164, -0.0026, -0.0152,  ..., -0.0148,  0.0204,  0.0087],\n                      [-0.0118,  0.0011, -0.0253,  ..., -0.0027,  0.0003,  0.0133],\n                      [ 0.0107, -0.0214,  0.0022,  ...,  0.0092, -0.0197, -0.0180]])),\n             ('model.model.layers.13.mlp.gate_proj.weight',\n              tensor([[-0.0017, -0.0133, -0.0233,  ..., -0.0128,  0.0435, -0.0130],\n                      [ 0.0121, -0.0204, -0.0205,  ..., -0.0405, -0.0293, -0.0129],\n                      [ 0.0009, -0.0042, -0.0116,  ..., -0.0090, -0.0162,  0.0031],\n                      ...,\n                      [-0.0114, -0.0132, -0.0379,  ..., -0.0027, -0.0156,  0.0498],\n                      [ 0.0092, -0.0199, -0.0203,  ..., -0.0014,  0.0080,  0.0030],\n                      [ 0.0230, -0.0334,  0.0141,  ...,  0.0126,  0.0021,  0.0333]])),\n             ('model.model.layers.13.mlp.up_proj.weight',\n              tensor([[-0.0383, -0.0120, -0.0090,  ..., -0.0086,  0.0087, -0.0124],\n                      [ 0.0356,  0.0014, -0.0090,  ..., -0.0167,  0.0153, -0.0142],\n                      [-0.0033,  0.0134, -0.0343,  ...,  0.0182, -0.0072, -0.0050],\n                      ...,\n                      [ 0.0181, -0.0158,  0.0095,  ...,  0.0267, -0.0021,  0.0052],\n                      [-0.0029,  0.0208, -0.0082,  ..., -0.0159,  0.0161,  0.0123],\n                      [ 0.0024,  0.0518, -0.0054,  ..., -0.0034,  0.0041, -0.0133]])),\n             ('model.model.layers.13.mlp.down_proj.weight',\n              tensor([[ 0.0044, -0.0029,  0.0335,  ..., -0.0082, -0.0177, -0.0246],\n                      [-0.0120, -0.0119,  0.0015,  ..., -0.0044,  0.0211, -0.0102],\n                      [ 0.0030, -0.0008, -0.0081,  ..., -0.0095, -0.0008, -0.0043],\n                      ...,\n                      [ 0.0280, -0.0232, -0.0208,  ...,  0.0033,  0.0107, -0.0033],\n                      [ 0.0237, -0.0118,  0.0174,  ..., -0.0345, -0.0233,  0.0063],\n                      [-0.0086, -0.0261, -0.0400,  ...,  0.0101,  0.0280,  0.0079]])),\n             ('model.model.layers.13.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.13.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.14.self_attn.q_proj.weight',\n              tensor([[-0.0259,  0.0076,  0.0056,  ...,  0.0060, -0.0095,  0.0164],\n                      [ 0.0516,  0.0307,  0.0256,  ..., -0.0155,  0.0342, -0.0179],\n                      [-0.0304, -0.0173,  0.0232,  ...,  0.0014,  0.0143,  0.0270],\n                      ...,\n                      [ 0.0164,  0.0087,  0.0036,  ..., -0.0225, -0.0082,  0.0066],\n                      [-0.0471,  0.0098,  0.0008,  ...,  0.0237, -0.0339, -0.0038],\n                      [ 0.0073, -0.0114, -0.0154,  ..., -0.0208, -0.0225, -0.0332]])),\n             ('model.model.layers.14.self_attn.k_proj.weight',\n              tensor([[-0.0287, -0.0545,  0.0027,  ...,  0.0020,  0.0041, -0.0111],\n                      [-0.0286,  0.0119, -0.0119,  ..., -0.0339, -0.0003,  0.0422],\n                      [-0.0015,  0.0175,  0.0051,  ...,  0.0017,  0.0083,  0.0115],\n                      ...,\n                      [ 0.0060, -0.0268,  0.0152,  ..., -0.0041, -0.0490, -0.0219],\n                      [ 0.0388,  0.0048, -0.0135,  ..., -0.0356,  0.0262, -0.0051],\n                      [-0.0138, -0.0216,  0.0022,  ..., -0.0100, -0.0137, -0.0002]])),\n             ('model.model.layers.14.self_attn.v_proj.weight',\n              tensor([[-0.0006, -0.0035,  0.0095,  ...,  0.0048, -0.0165, -0.0061],\n                      [-0.0084, -0.0154,  0.0299,  ..., -0.0014,  0.0138, -0.0203],\n                      [ 0.0163,  0.0206, -0.0078,  ...,  0.0097, -0.0188,  0.0398],\n                      ...,\n                      [ 0.0391, -0.0293,  0.0076,  ...,  0.0099,  0.0015, -0.0446],\n                      [-0.0095,  0.0261, -0.0346,  ...,  0.0247,  0.0090,  0.0033],\n                      [-0.0135,  0.0034, -0.0229,  ..., -0.0106, -0.0139,  0.0316]])),\n             ('model.model.layers.14.self_attn.o_proj.weight',\n              tensor([[-0.0380, -0.0094, -0.0210,  ...,  0.0166,  0.0225, -0.0016],\n                      [-0.0147,  0.0437,  0.0103,  ..., -0.0033,  0.0114,  0.0104],\n                      [-0.0080, -0.0032, -0.0049,  ...,  0.0141,  0.0006,  0.0031],\n                      ...,\n                      [ 0.0347, -0.0094, -0.0074,  ...,  0.0107,  0.0246,  0.0047],\n                      [-0.0023,  0.0134,  0.0394,  ...,  0.0117,  0.0001,  0.0184],\n                      [ 0.0237, -0.0030, -0.0046,  ...,  0.0016,  0.0096,  0.0034]])),\n             ('model.model.layers.14.mlp.gate_proj.weight',\n              tensor([[-0.0056,  0.0149,  0.0020,  ..., -0.0021, -0.0354, -0.0332],\n                      [ 0.0225,  0.0411, -0.0120,  ...,  0.0077, -0.0154,  0.0318],\n                      [-0.0061, -0.0117,  0.0007,  ...,  0.0089, -0.0161, -0.0055],\n                      ...,\n                      [-0.0097,  0.0044,  0.0317,  ..., -0.0259,  0.0142, -0.0022],\n                      [-0.0087, -0.0036, -0.0116,  ...,  0.0055,  0.0292, -0.0411],\n                      [ 0.0022,  0.0025, -0.0085,  ..., -0.0091, -0.0227, -0.0024]])),\n             ('model.model.layers.14.mlp.up_proj.weight',\n              tensor([[-0.0075, -0.0297, -0.0314,  ...,  0.0210, -0.0099,  0.0023],\n                      [-0.0050, -0.0161, -0.0127,  ...,  0.0058,  0.0202,  0.0321],\n                      [-0.0049, -0.0137,  0.0509,  ...,  0.0177,  0.0019,  0.0129],\n                      ...,\n                      [-0.0105, -0.0059,  0.0191,  ..., -0.0062,  0.0048,  0.0326],\n                      [ 0.0011,  0.0119,  0.0455,  ...,  0.0059,  0.0222,  0.0014],\n                      [ 0.0154, -0.0464, -0.0246,  ..., -0.0089,  0.0085, -0.0136]])),\n             ('model.model.layers.14.mlp.down_proj.weight',\n              tensor([[-6.1492e-03,  3.5572e-02, -1.8050e-02,  ...,  1.6917e-02,\n                        3.2779e-02, -1.2832e-02],\n                      [-4.1583e-03,  2.1680e-03,  1.0809e-02,  ...,  4.1317e-03,\n                        2.0589e-03, -2.2450e-02],\n                      [-1.7250e-02,  3.2388e-02,  2.6321e-02,  ..., -1.3858e-02,\n                        2.2274e-02, -2.5090e-02],\n                      ...,\n                      [ 2.8896e-02,  3.6828e-03, -2.3050e-03,  ..., -2.4119e-03,\n                       -1.1978e-02, -1.0560e-03],\n                      [ 1.9617e-03, -4.0663e-03,  9.7781e-03,  ...,  1.1129e-03,\n                        2.9075e-03,  1.8784e-03],\n                      [ 1.3577e-02, -2.8120e-05, -8.8960e-03,  ...,  3.6464e-03,\n                        8.2487e-03, -3.0501e-02]])),\n             ('model.model.layers.14.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.14.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.15.self_attn.q_proj.weight',\n              tensor([[ 0.0102,  0.0045,  0.0374,  ..., -0.0011, -0.0022,  0.0017],\n                      [-0.0078, -0.0058, -0.0132,  ..., -0.0160, -0.0230,  0.0029],\n                      [-0.0200,  0.0106, -0.0166,  ..., -0.0042, -0.0510, -0.0386],\n                      ...,\n                      [-0.0255,  0.0036,  0.0105,  ..., -0.0136,  0.0155,  0.0217],\n                      [-0.0187, -0.0221, -0.0194,  ..., -0.0592,  0.0131, -0.0265],\n                      [ 0.0054,  0.0270,  0.0517,  ..., -0.0035, -0.0166, -0.0173]])),\n             ('model.model.layers.15.self_attn.k_proj.weight',\n              tensor([[-0.0158, -0.0437, -0.0096,  ..., -0.0055,  0.0137, -0.0043],\n                      [ 0.0085,  0.0106,  0.0235,  ..., -0.0314,  0.0050,  0.0229],\n                      [ 0.0229,  0.0161,  0.0307,  ..., -0.0085,  0.0092, -0.0354],\n                      ...,\n                      [ 0.0049, -0.0105,  0.0020,  ...,  0.0218,  0.0107,  0.0137],\n                      [ 0.0304, -0.0119, -0.0050,  ..., -0.0091, -0.0274, -0.0111],\n                      [-0.0012, -0.0477, -0.0277,  ...,  0.0165, -0.0061,  0.0153]])),\n             ('model.model.layers.15.self_attn.v_proj.weight',\n              tensor([[ 0.0117, -0.0169, -0.0168,  ...,  0.0204,  0.0071,  0.0145],\n                      [ 0.0423, -0.0004,  0.0264,  ...,  0.0225, -0.0312,  0.0056],\n                      [-0.0275, -0.0008,  0.0138,  ..., -0.0157, -0.0326,  0.0227],\n                      ...,\n                      [ 0.0082,  0.0117, -0.0040,  ...,  0.0002, -0.0562,  0.0202],\n                      [-0.0292,  0.0071, -0.0020,  ..., -0.0119,  0.0017,  0.0416],\n                      [ 0.0214,  0.0126, -0.0193,  ..., -0.0005,  0.0050,  0.0142]])),\n             ('model.model.layers.15.self_attn.o_proj.weight',\n              tensor([[-0.0020,  0.0376, -0.0290,  ..., -0.0160, -0.0139,  0.0037],\n                      [ 0.0589, -0.0018,  0.0083,  ..., -0.0381, -0.0092,  0.0527],\n                      [ 0.0073, -0.0155, -0.0108,  ...,  0.0158,  0.0352,  0.0236],\n                      ...,\n                      [ 0.0243, -0.0108, -0.0194,  ...,  0.0226, -0.0129,  0.0120],\n                      [-0.0080, -0.0033, -0.0033,  ..., -0.0027, -0.0226,  0.0038],\n                      [-0.0127, -0.0059, -0.0083,  ...,  0.0072, -0.0220, -0.0131]])),\n             ('model.model.layers.15.mlp.gate_proj.weight',\n              tensor([[-0.0345,  0.0101, -0.0297,  ...,  0.0112,  0.0172,  0.0073],\n                      [ 0.0151,  0.0063, -0.0150,  ..., -0.0324, -0.0095,  0.0487],\n                      [ 0.0230, -0.0054, -0.0075,  ..., -0.0358, -0.0113, -0.0113],\n                      ...,\n                      [ 0.0047, -0.0121, -0.0310,  ..., -0.0146, -0.0275,  0.0298],\n                      [-0.0311, -0.0040, -0.0436,  ..., -0.0088, -0.0167, -0.0045],\n                      [ 0.0040,  0.0147, -0.0176,  ..., -0.0035, -0.0047,  0.0125]])),\n             ('model.model.layers.15.mlp.up_proj.weight',\n              tensor([[-0.0091,  0.0242,  0.0074,  ...,  0.0306,  0.0105, -0.0192],\n                      [-0.0300, -0.0140,  0.0096,  ..., -0.0108, -0.0341,  0.0222],\n                      [-0.0106, -0.0163,  0.0065,  ...,  0.0359, -0.0350, -0.0042],\n                      ...,\n                      [-0.0088, -0.0352,  0.0158,  ..., -0.0485, -0.0125, -0.0291],\n                      [-0.0012, -0.0267,  0.0100,  ..., -0.0196, -0.0171, -0.0164],\n                      [ 0.0170,  0.0053,  0.0258,  ..., -0.0023, -0.0122, -0.0118]])),\n             ('model.model.layers.15.mlp.down_proj.weight',\n              tensor([[ 0.0122, -0.0043,  0.0140,  ..., -0.0043,  0.0155, -0.0194],\n                      [ 0.0045,  0.0353,  0.0303,  ...,  0.0209,  0.0151,  0.0195],\n                      [ 0.0111,  0.0117, -0.0268,  ..., -0.0391,  0.0037,  0.0154],\n                      ...,\n                      [-0.0395, -0.0262,  0.0017,  ..., -0.0006, -0.0035,  0.0112],\n                      [-0.0145,  0.0238,  0.0131,  ...,  0.0023, -0.0080, -0.0134],\n                      [ 0.0214, -0.0166, -0.0097,  ..., -0.0253,  0.0258,  0.0051]])),\n             ('model.model.layers.15.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.15.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.16.self_attn.q_proj.weight',\n              tensor([[ 0.0020,  0.0031,  0.0054,  ...,  0.0117, -0.0123,  0.0220],\n                      [-0.0296, -0.0102, -0.0187,  ...,  0.0164, -0.0176,  0.0180],\n                      [ 0.0281, -0.0020, -0.0064,  ...,  0.0090,  0.0009,  0.0148],\n                      ...,\n                      [-0.0330,  0.0073, -0.0049,  ..., -0.0079, -0.0336,  0.0452],\n                      [ 0.0292,  0.0285,  0.0280,  ...,  0.0123,  0.0001,  0.0082],\n                      [ 0.0283, -0.0225,  0.0081,  ...,  0.0003, -0.0061,  0.0338]])),\n             ('model.model.layers.16.self_attn.k_proj.weight',\n              tensor([[ 1.6225e-03, -1.7301e-02,  1.6605e-03,  ...,  6.2095e-03,\n                        5.6258e-03,  4.7431e-03],\n                      [-1.8739e-02, -1.7642e-04,  2.8338e-02,  ...,  2.0983e-02,\n                        9.9325e-03,  9.2691e-03],\n                      [-3.9462e-02,  2.4794e-02,  2.3891e-02,  ..., -3.6207e-02,\n                        1.0082e-02,  2.1167e-02],\n                      ...,\n                      [ 2.5646e-02, -8.2200e-03,  2.4844e-02,  ..., -1.4484e-03,\n                        2.5507e-02,  4.1904e-03],\n                      [-4.5724e-03,  8.2694e-05,  2.5803e-03,  ...,  1.3407e-02,\n                       -1.7071e-02,  1.1950e-02],\n                      [ 8.4475e-03,  2.4373e-02,  2.9199e-02,  ..., -7.2848e-03,\n                       -1.1152e-02, -1.1008e-02]])),\n             ('model.model.layers.16.self_attn.v_proj.weight',\n              tensor([[-0.0017,  0.0110, -0.0298,  ..., -0.0045, -0.0160, -0.0557],\n                      [ 0.0336,  0.0217,  0.0038,  ..., -0.0236,  0.0041,  0.0037],\n                      [-0.0355, -0.0074, -0.0221,  ...,  0.0363, -0.0401,  0.0007],\n                      ...,\n                      [-0.0182, -0.0205, -0.0134,  ..., -0.0212,  0.0123,  0.0125],\n                      [ 0.0067, -0.0107, -0.0004,  ...,  0.0130,  0.0047, -0.0094],\n                      [ 0.0035, -0.0239,  0.0024,  ...,  0.0179,  0.0069,  0.0158]])),\n             ('model.model.layers.16.self_attn.o_proj.weight',\n              tensor([[-0.0184, -0.0180, -0.0154,  ...,  0.0032, -0.0136,  0.0057],\n                      [-0.0043,  0.0105,  0.0059,  ..., -0.0153, -0.0218, -0.0117],\n                      [ 0.0201,  0.0638, -0.0344,  ..., -0.0068, -0.0279, -0.0064],\n                      ...,\n                      [-0.0096, -0.0112,  0.0016,  ...,  0.0098, -0.0151, -0.0066],\n                      [-0.0063, -0.0255,  0.0238,  ...,  0.0075,  0.0056, -0.0028],\n                      [-0.0101, -0.0180,  0.0125,  ..., -0.0019,  0.0244, -0.0057]])),\n             ('model.model.layers.16.mlp.gate_proj.weight',\n              tensor([[ 0.0189, -0.0058,  0.0090,  ...,  0.0349,  0.0246, -0.0092],\n                      [ 0.0127, -0.0023, -0.0243,  ...,  0.0376,  0.0134, -0.0130],\n                      [ 0.0468,  0.0009,  0.0091,  ...,  0.0013, -0.0151,  0.0088],\n                      ...,\n                      [ 0.0014,  0.0074,  0.0107,  ...,  0.0074, -0.0316, -0.0126],\n                      [-0.0266,  0.0139,  0.0051,  ...,  0.0140, -0.0125,  0.0034],\n                      [-0.0063,  0.0040, -0.0164,  ...,  0.0242,  0.0155, -0.0044]])),\n             ('model.model.layers.16.mlp.up_proj.weight',\n              tensor([[ 0.0221,  0.0073,  0.0450,  ..., -0.0206, -0.0009,  0.0107],\n                      [ 0.0222, -0.0016,  0.0162,  ...,  0.0023, -0.0211, -0.0027],\n                      [ 0.0072, -0.0283,  0.0319,  ...,  0.0187,  0.0109, -0.0140],\n                      ...,\n                      [ 0.0132,  0.0080,  0.0191,  ..., -0.0021,  0.0010,  0.0238],\n                      [ 0.0168, -0.0085, -0.0108,  ...,  0.0043,  0.0322,  0.0046],\n                      [ 0.0071, -0.0291, -0.0457,  ..., -0.0206,  0.0467,  0.0024]])),\n             ('model.model.layers.16.mlp.down_proj.weight',\n              tensor([[-0.0151, -0.0010, -0.0118,  ..., -0.0036,  0.0137, -0.0003],\n                      [-0.0425,  0.0120,  0.0147,  ..., -0.0144,  0.0045,  0.0153],\n                      [ 0.0037,  0.0310,  0.0222,  ..., -0.0071, -0.0206,  0.0099],\n                      ...,\n                      [ 0.0284, -0.0522, -0.0379,  ..., -0.0309, -0.0305,  0.0212],\n                      [ 0.0003, -0.0049, -0.0017,  ..., -0.0134,  0.0114, -0.0141],\n                      [-0.0151,  0.0375, -0.0286,  ..., -0.0021, -0.0145,  0.0010]])),\n             ('model.model.layers.16.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.16.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.17.self_attn.q_proj.weight',\n              tensor([[-0.0124,  0.0066,  0.0291,  ...,  0.0246,  0.0142,  0.0163],\n                      [ 0.0049,  0.0082, -0.0173,  ...,  0.0077, -0.0276,  0.0041],\n                      [ 0.0103,  0.0137, -0.0076,  ...,  0.0289, -0.0086, -0.0031],\n                      ...,\n                      [-0.0013, -0.0593,  0.0040,  ..., -0.0154,  0.0007, -0.0311],\n                      [-0.0249, -0.0305,  0.0400,  ...,  0.0190, -0.0010, -0.0094],\n                      [-0.0211,  0.0172, -0.0224,  ...,  0.0271, -0.0299,  0.0225]])),\n             ('model.model.layers.17.self_attn.k_proj.weight',\n              tensor([[ 0.0168, -0.0030,  0.0054,  ...,  0.0011,  0.0124, -0.0108],\n                      [ 0.0020,  0.0070,  0.0222,  ..., -0.0048, -0.0157, -0.0360],\n                      [ 0.0062,  0.0133, -0.0359,  ..., -0.0149,  0.0091, -0.0044],\n                      ...,\n                      [ 0.0016,  0.0005, -0.0156,  ..., -0.0120, -0.0149,  0.0432],\n                      [-0.0035, -0.0294,  0.0250,  ...,  0.0061, -0.0125,  0.0307],\n                      [-0.0222, -0.0225,  0.0070,  ..., -0.0149, -0.0247,  0.0061]])),\n             ('model.model.layers.17.self_attn.v_proj.weight',\n              tensor([[ 0.0222,  0.0040,  0.0328,  ...,  0.0376, -0.0191,  0.0028],\n                      [ 0.0024, -0.0087,  0.0014,  ...,  0.0311, -0.0192,  0.0088],\n                      [ 0.0016,  0.0048, -0.0149,  ..., -0.0184, -0.0120, -0.0224],\n                      ...,\n                      [ 0.0008,  0.0041, -0.0034,  ...,  0.0127,  0.0245,  0.0241],\n                      [ 0.0155,  0.0088,  0.0115,  ..., -0.0158,  0.0133, -0.0083],\n                      [-0.0102, -0.0313, -0.0271,  ..., -0.0180, -0.0073,  0.0044]])),\n             ('model.model.layers.17.self_attn.o_proj.weight',\n              tensor([[ 0.0256,  0.0174, -0.0379,  ..., -0.0282, -0.0459, -0.0422],\n                      [ 0.0053, -0.0336,  0.0240,  ..., -0.0212, -0.0396,  0.0276],\n                      [ 0.0050, -0.0077,  0.0088,  ..., -0.0107, -0.0010,  0.0070],\n                      ...,\n                      [ 0.0023,  0.0038,  0.0040,  ...,  0.0077, -0.0270,  0.0061],\n                      [ 0.0022,  0.0003,  0.0034,  ..., -0.0257,  0.0217,  0.0076],\n                      [ 0.0017, -0.0191, -0.0183,  ...,  0.0279,  0.0095, -0.0405]])),\n             ('model.model.layers.17.mlp.gate_proj.weight',\n              tensor([[ 0.0215, -0.0089, -0.0040,  ...,  0.0129, -0.0021,  0.0344],\n                      [ 0.0104, -0.0026,  0.0058,  ...,  0.0029, -0.0084,  0.0193],\n                      [ 0.0142, -0.0335, -0.0114,  ...,  0.0043, -0.0512,  0.0104],\n                      ...,\n                      [ 0.0218, -0.0017, -0.0071,  ..., -0.0027, -0.0095,  0.0077],\n                      [-0.0083, -0.0095,  0.0037,  ...,  0.0322,  0.0262, -0.0129],\n                      [-0.0015,  0.0159, -0.0168,  ..., -0.0040, -0.0110,  0.0211]])),\n             ('model.model.layers.17.mlp.up_proj.weight',\n              tensor([[ 2.0922e-02,  6.8756e-03,  5.9327e-02,  ...,  4.4769e-02,\n                       -2.1206e-04, -4.3613e-04],\n                      [-2.6228e-02,  1.6926e-02, -6.4626e-03,  ..., -3.1669e-02,\n                       -1.6463e-02, -2.1425e-02],\n                      [ 5.8987e-03,  2.7822e-02,  1.5307e-02,  ..., -3.2149e-03,\n                       -6.1048e-03, -3.6167e-03],\n                      ...,\n                      [ 3.3269e-02,  2.1210e-05,  7.5702e-03,  ...,  2.7629e-02,\n                       -1.2369e-02, -1.6517e-02],\n                      [ 1.4964e-03, -5.3407e-03, -3.2984e-02,  ..., -3.5626e-02,\n                       -2.9537e-03, -3.1346e-03],\n                      [-4.0268e-03,  1.5628e-04,  5.0428e-03,  ..., -2.9817e-02,\n                        9.3193e-03,  1.5174e-02]])),\n             ('model.model.layers.17.mlp.down_proj.weight',\n              tensor([[ 1.2881e-02,  5.9551e-03,  1.0116e-02,  ..., -3.1634e-02,\n                        1.8677e-02, -5.9323e-03],\n                      [ 2.3823e-02,  2.8731e-03,  2.4573e-02,  ..., -7.9917e-04,\n                       -3.8312e-02,  3.0045e-02],\n                      [ 9.0776e-03,  1.4748e-02, -1.4076e-02,  ..., -7.6672e-03,\n                       -8.5587e-05,  2.0584e-02],\n                      ...,\n                      [ 4.8086e-03,  3.1342e-03, -7.7240e-03,  ...,  2.4077e-02,\n                        3.2881e-02, -4.7098e-03],\n                      [-3.9971e-03, -3.4267e-03, -2.4715e-02,  ...,  1.5070e-02,\n                       -2.6726e-02,  3.4626e-02],\n                      [ 1.9702e-02,  1.9424e-02,  3.9135e-03,  ..., -1.5710e-02,\n                       -2.3733e-02,  7.1122e-03]])),\n             ('model.model.layers.17.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.17.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.18.self_attn.q_proj.weight',\n              tensor([[ 0.0055,  0.0009,  0.0233,  ..., -0.0149,  0.0053,  0.0107],\n                      [-0.0149,  0.0141,  0.0262,  ...,  0.0259,  0.0088, -0.0126],\n                      [-0.0356,  0.0007,  0.0214,  ...,  0.0095,  0.0158, -0.0052],\n                      ...,\n                      [ 0.0227, -0.0045,  0.0267,  ..., -0.0198, -0.0289, -0.0182],\n                      [ 0.0149, -0.0383,  0.0198,  ...,  0.0278,  0.0074, -0.0027],\n                      [ 0.0112, -0.0050, -0.0035,  ..., -0.0141, -0.0236,  0.0118]])),\n             ('model.model.layers.18.self_attn.k_proj.weight',\n              tensor([[ 0.0238,  0.0098,  0.0351,  ..., -0.0095, -0.0158,  0.0083],\n                      [-0.0224, -0.0011, -0.0086,  ..., -0.0161, -0.0198, -0.0422],\n                      [-0.0080, -0.0095, -0.0025,  ..., -0.0069,  0.0275,  0.0016],\n                      ...,\n                      [-0.0108,  0.0215, -0.0503,  ...,  0.0259, -0.0027, -0.0184],\n                      [-0.0185,  0.0146, -0.0293,  ...,  0.0116,  0.0303,  0.0149],\n                      [ 0.0037, -0.0056, -0.0046,  ...,  0.0253, -0.0135, -0.0259]])),\n             ('model.model.layers.18.self_attn.v_proj.weight',\n              tensor([[-0.0152,  0.0306, -0.0120,  ..., -0.0268,  0.0156, -0.0021],\n                      [-0.0274,  0.0118, -0.0049,  ...,  0.0168, -0.0070,  0.0012],\n                      [ 0.0185,  0.0086,  0.0146,  ...,  0.0081,  0.0320, -0.0198],\n                      ...,\n                      [-0.0019, -0.0003, -0.0058,  ..., -0.0107, -0.0102, -0.0278],\n                      [ 0.0118, -0.0076, -0.0075,  ..., -0.0205,  0.0134,  0.0116],\n                      [-0.0002,  0.0478,  0.0004,  ..., -0.0182, -0.0108, -0.0161]])),\n             ('model.model.layers.18.self_attn.o_proj.weight',\n              tensor([[ 1.9887e-02,  1.8257e-03, -4.3952e-03,  ..., -5.8591e-03,\n                        2.9418e-02, -6.3369e-03],\n                      [-8.5059e-03,  2.0294e-03,  1.2620e-02,  ...,  1.2412e-02,\n                        1.1090e-02,  2.5751e-05],\n                      [-3.1269e-04,  1.2657e-02,  2.2277e-05,  ..., -1.1434e-02,\n                       -2.4102e-02,  1.4107e-02],\n                      ...,\n                      [-1.4612e-02,  9.5135e-03, -2.3477e-02,  ...,  1.6155e-02,\n                       -4.5874e-03,  9.5484e-03],\n                      [ 2.0704e-02, -1.0562e-02, -2.2082e-02,  ...,  3.4844e-02,\n                        5.7744e-03,  1.4814e-02],\n                      [ 6.1816e-03,  3.6575e-02, -6.4860e-03,  ...,  9.9613e-03,\n                        4.6007e-04,  3.6208e-02]])),\n             ('model.model.layers.18.mlp.gate_proj.weight',\n              tensor([[ 1.1805e-02, -3.3969e-02,  1.1710e-02,  ..., -3.5554e-02,\n                        3.6268e-02, -2.4913e-03],\n                      [ 1.3134e-02,  2.5225e-02,  1.9790e-02,  ...,  9.2380e-03,\n                       -5.7917e-03, -5.5101e-02],\n                      [-2.2686e-02, -2.1803e-02,  1.0842e-02,  ...,  2.0276e-02,\n                       -1.6769e-02, -3.5487e-02],\n                      ...,\n                      [ 2.5598e-02, -7.5809e-03,  8.4396e-04,  ...,  1.6120e-02,\n                        1.0336e-03, -4.4618e-04],\n                      [-2.0089e-02, -1.6448e-02, -2.1579e-02,  ...,  1.6099e-02,\n                       -2.4133e-02, -1.4817e-02],\n                      [-4.8855e-03, -3.2863e-03, -1.2798e-02,  ...,  1.5382e-02,\n                        1.1651e-02,  6.8215e-05]])),\n             ('model.model.layers.18.mlp.up_proj.weight',\n              tensor([[-0.0255, -0.0207,  0.0218,  ...,  0.0020, -0.0110,  0.0307],\n                      [-0.0197,  0.0287, -0.0356,  ..., -0.0183, -0.0426,  0.0287],\n                      [ 0.0201, -0.0098,  0.0335,  ..., -0.0067,  0.0039,  0.0150],\n                      ...,\n                      [ 0.0119,  0.0062,  0.0288,  ..., -0.0173,  0.0191, -0.0191],\n                      [-0.0076, -0.0091, -0.0155,  ..., -0.0065, -0.0061,  0.0281],\n                      [ 0.0054, -0.0035, -0.0228,  ..., -0.0017,  0.0073,  0.0068]])),\n             ('model.model.layers.18.mlp.down_proj.weight',\n              tensor([[ 0.0067,  0.0204,  0.0003,  ..., -0.0068, -0.0279,  0.0251],\n                      [-0.0206, -0.0068, -0.0470,  ..., -0.0109,  0.0006, -0.0186],\n                      [-0.0004, -0.0169, -0.0455,  ...,  0.0370, -0.0008,  0.0158],\n                      ...,\n                      [-0.0141, -0.0193, -0.0029,  ..., -0.0208, -0.0180, -0.0111],\n                      [ 0.0163,  0.0289, -0.0348,  ..., -0.0047, -0.0067, -0.0055],\n                      [-0.0013, -0.0228, -0.0019,  ...,  0.0116, -0.0095, -0.0056]])),\n             ('model.model.layers.18.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.18.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.19.self_attn.q_proj.weight',\n              tensor([[ 0.0365,  0.0143, -0.0036,  ...,  0.0099, -0.0103, -0.0153],\n                      [ 0.0065, -0.0089, -0.0226,  ...,  0.0296,  0.0152, -0.0157],\n                      [-0.0066,  0.0215,  0.0380,  ...,  0.0110, -0.0123,  0.0110],\n                      ...,\n                      [-0.0054,  0.0398, -0.0160,  ..., -0.0179, -0.0228,  0.0161],\n                      [ 0.0207, -0.0192,  0.0276,  ..., -0.0125, -0.0216, -0.0010],\n                      [ 0.0410, -0.0101,  0.0112,  ...,  0.0221, -0.0285, -0.0109]])),\n             ('model.model.layers.19.self_attn.k_proj.weight',\n              tensor([[-0.0127, -0.0126, -0.0170,  ..., -0.0033,  0.0186,  0.0240],\n                      [-0.0382,  0.0202, -0.0094,  ...,  0.0023, -0.0042, -0.0293],\n                      [-0.0216,  0.0085, -0.0117,  ...,  0.0363,  0.0042, -0.0011],\n                      ...,\n                      [-0.0170, -0.0050,  0.0184,  ..., -0.0143, -0.0124,  0.0520],\n                      [-0.0032, -0.0076,  0.0195,  ...,  0.0267, -0.0016,  0.0280],\n                      [-0.0116, -0.0004, -0.0133,  ...,  0.0306,  0.0271, -0.0075]])),\n             ('model.model.layers.19.self_attn.v_proj.weight',\n              tensor([[-0.0055, -0.0085,  0.0102,  ..., -0.0047, -0.0069, -0.0005],\n                      [-0.0252, -0.0384, -0.0071,  ..., -0.0292,  0.0001, -0.0170],\n                      [ 0.0059, -0.0118,  0.0167,  ..., -0.0404, -0.0089, -0.0086],\n                      ...,\n                      [-0.0436,  0.0080,  0.0331,  ...,  0.0023, -0.0246, -0.0149],\n                      [-0.0411, -0.0052, -0.0121,  ...,  0.0135,  0.0270, -0.0037],\n                      [ 0.0208,  0.0220,  0.0063,  ..., -0.0226, -0.0145, -0.0084]])),\n             ('model.model.layers.19.self_attn.o_proj.weight',\n              tensor([[-9.8527e-04,  1.8866e-03,  1.9622e-02,  ..., -2.0414e-02,\n                       -4.1172e-02,  5.8527e-04],\n                      [ 7.4591e-03, -1.3234e-02,  1.4457e-02,  ...,  5.1098e-02,\n                       -4.3355e-05,  1.4126e-02],\n                      [ 2.1305e-02,  3.0187e-02,  3.7113e-03,  ..., -2.3726e-02,\n                       -1.0332e-02,  7.6621e-03],\n                      ...,\n                      [ 1.6403e-02,  1.5911e-02, -9.7043e-03,  ...,  2.3186e-02,\n                        1.4771e-02,  4.0230e-02],\n                      [ 9.3076e-03,  1.7457e-02,  1.5071e-02,  ..., -7.7796e-03,\n                       -2.2475e-02,  3.6888e-02],\n                      [-3.8831e-03, -1.4043e-02, -1.8198e-02,  ...,  8.5504e-03,\n                       -1.7972e-02, -5.7523e-02]])),\n             ('model.model.layers.19.mlp.gate_proj.weight',\n              tensor([[-0.0292,  0.0168, -0.0127,  ..., -0.0071, -0.0247,  0.0045],\n                      [-0.0260, -0.0203, -0.0084,  ...,  0.0095,  0.0063,  0.0174],\n                      [ 0.0049,  0.0507,  0.0163,  ..., -0.0174, -0.0053, -0.0154],\n                      ...,\n                      [ 0.0208,  0.0259, -0.0082,  ...,  0.0172,  0.0030, -0.0112],\n                      [ 0.0131, -0.0066, -0.0012,  ..., -0.0057, -0.0064, -0.0241],\n                      [-0.0041, -0.0107, -0.0124,  ..., -0.0042, -0.0016,  0.0088]])),\n             ('model.model.layers.19.mlp.up_proj.weight',\n              tensor([[-0.0053,  0.0090, -0.0176,  ...,  0.0241, -0.0057, -0.0067],\n                      [-0.0092,  0.0361, -0.0208,  ..., -0.0269,  0.0001, -0.0051],\n                      [ 0.0121, -0.0161, -0.0311,  ..., -0.0208, -0.0208,  0.0022],\n                      ...,\n                      [ 0.0136, -0.0107, -0.0034,  ...,  0.0367, -0.0122, -0.0129],\n                      [-0.0027, -0.0125,  0.0070,  ...,  0.0082,  0.0159,  0.0070],\n                      [ 0.0140, -0.0151, -0.0158,  ..., -0.0317, -0.0253,  0.0785]])),\n             ('model.model.layers.19.mlp.down_proj.weight',\n              tensor([[-0.0080,  0.0211,  0.0255,  ..., -0.0019,  0.0137, -0.0302],\n                      [ 0.0026,  0.0283,  0.0012,  ...,  0.0019, -0.0286, -0.0272],\n                      [ 0.0006, -0.0399, -0.0354,  ...,  0.0024,  0.0057,  0.0332],\n                      ...,\n                      [-0.0268,  0.0142,  0.0377,  ...,  0.0111, -0.0239,  0.0018],\n                      [-0.0023,  0.0452,  0.0163,  ...,  0.0109, -0.0061,  0.0166],\n                      [ 0.0145, -0.0176,  0.0070,  ..., -0.0003,  0.0027,  0.0230]])),\n             ('model.model.layers.19.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.19.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.20.self_attn.q_proj.weight',\n              tensor([[ 0.0014,  0.0268, -0.0410,  ..., -0.0072, -0.0181, -0.0216],\n                      [-0.0128,  0.0076, -0.0009,  ..., -0.0025, -0.0370, -0.0002],\n                      [ 0.0032,  0.0010,  0.0006,  ..., -0.0217, -0.0110, -0.0060],\n                      ...,\n                      [-0.0041, -0.0029,  0.0023,  ..., -0.0137, -0.0331, -0.0073],\n                      [-0.0057, -0.0313,  0.0007,  ...,  0.0288, -0.0024,  0.0087],\n                      [-0.0026,  0.0127, -0.0369,  ..., -0.0148,  0.0127,  0.0117]])),\n             ('model.model.layers.20.self_attn.k_proj.weight',\n              tensor([[-0.0151,  0.0141, -0.0123,  ..., -0.0047,  0.0161, -0.0008],\n                      [ 0.0349,  0.0091, -0.0159,  ..., -0.0077, -0.0334,  0.0015],\n                      [-0.0222, -0.0249, -0.0258,  ..., -0.0143, -0.0152, -0.0004],\n                      ...,\n                      [ 0.0008, -0.0096,  0.0085,  ...,  0.0503, -0.0129,  0.0204],\n                      [ 0.0032,  0.0120,  0.0127,  ..., -0.0298, -0.0204, -0.0150],\n                      [ 0.0330, -0.0125, -0.0202,  ..., -0.0134,  0.0279, -0.0247]])),\n             ('model.model.layers.20.self_attn.v_proj.weight',\n              tensor([[ 0.0049, -0.0278,  0.0210,  ...,  0.0254,  0.0118, -0.0131],\n                      [ 0.0165,  0.0183,  0.0155,  ..., -0.0122, -0.0112, -0.0193],\n                      [ 0.0037, -0.0147, -0.0062,  ..., -0.0366, -0.0351, -0.0187],\n                      ...,\n                      [-0.0199,  0.0017,  0.0044,  ...,  0.0231, -0.0123,  0.0375],\n                      [ 0.0016,  0.0402,  0.0084,  ...,  0.0053,  0.0071,  0.0024],\n                      [ 0.0338, -0.0027, -0.0133,  ...,  0.0229, -0.0283,  0.0339]])),\n             ('model.model.layers.20.self_attn.o_proj.weight',\n              tensor([[-0.0077,  0.0108, -0.0052,  ...,  0.0374, -0.0130, -0.0123],\n                      [-0.0065,  0.0397,  0.0181,  ..., -0.0061,  0.0068, -0.0321],\n                      [ 0.0092,  0.0117, -0.0045,  ..., -0.0046, -0.0224, -0.0106],\n                      ...,\n                      [-0.0018,  0.0432, -0.0055,  ...,  0.0403, -0.0091, -0.0239],\n                      [ 0.0135, -0.0080,  0.0170,  ..., -0.0057,  0.0219, -0.0054],\n                      [ 0.0088, -0.0170,  0.0021,  ...,  0.0015,  0.0213,  0.0296]])),\n             ('model.model.layers.20.mlp.gate_proj.weight',\n              tensor([[ 0.0048, -0.0245, -0.0258,  ...,  0.0073, -0.0078, -0.0167],\n                      [ 0.0087,  0.0469,  0.0045,  ...,  0.0128, -0.0109,  0.0194],\n                      [ 0.0204, -0.0249,  0.0330,  ..., -0.0058, -0.0141, -0.0150],\n                      ...,\n                      [-0.0298, -0.0419, -0.0168,  ...,  0.0233, -0.0198,  0.0456],\n                      [-0.0133, -0.0110,  0.0059,  ...,  0.0146,  0.0266, -0.0053],\n                      [ 0.0065,  0.0170,  0.0273,  ...,  0.0423,  0.0159, -0.0081]])),\n             ('model.model.layers.20.mlp.up_proj.weight',\n              tensor([[ 9.0603e-03, -2.1747e-02,  1.1314e-02,  ..., -1.0355e-02,\n                       -1.0953e-02,  4.8625e-03],\n                      [ 3.3468e-02, -1.8383e-02,  8.9913e-03,  ...,  3.6535e-02,\n                        1.1056e-02, -1.4056e-03],\n                      [ 6.5493e-03,  2.1927e-03,  9.8194e-03,  ..., -1.0467e-03,\n                        1.8313e-02,  9.8103e-03],\n                      ...,\n                      [ 1.1932e-02,  2.7883e-03, -1.6627e-02,  ...,  1.7028e-05,\n                       -4.5545e-03,  1.5665e-02],\n                      [ 1.3068e-02, -3.6066e-02,  3.1290e-02,  ...,  2.4015e-02,\n                       -2.8093e-03,  7.6560e-03],\n                      [ 1.6115e-02,  2.3966e-03, -2.3955e-02,  ..., -1.6290e-03,\n                        4.6381e-03, -1.5966e-02]])),\n             ('model.model.layers.20.mlp.down_proj.weight',\n              tensor([[ 0.0365,  0.0054, -0.0363,  ..., -0.0110,  0.0079,  0.0030],\n                      [-0.0079,  0.0056,  0.0119,  ...,  0.0110, -0.0143, -0.0094],\n                      [ 0.0280,  0.0023,  0.0220,  ..., -0.0124, -0.0170, -0.0032],\n                      ...,\n                      [ 0.0212, -0.0321,  0.0390,  ..., -0.0134,  0.0436,  0.0143],\n                      [ 0.0085, -0.0430,  0.0445,  ..., -0.0166,  0.0171, -0.0186],\n                      [-0.0199,  0.0106,  0.0593,  ...,  0.0082,  0.0265, -0.0019]])),\n             ('model.model.layers.20.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.20.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.21.self_attn.q_proj.weight',\n              tensor([[-0.0189,  0.0124,  0.0032,  ..., -0.0111, -0.0356,  0.0190],\n                      [ 0.0033, -0.0321,  0.0079,  ...,  0.0062,  0.0117, -0.0004],\n                      [ 0.0225, -0.0170,  0.0097,  ...,  0.0038,  0.0196,  0.0078],\n                      ...,\n                      [ 0.0167,  0.0088, -0.0253,  ...,  0.0095, -0.0283, -0.0042],\n                      [-0.0101,  0.0016,  0.0305,  ...,  0.0292, -0.0220,  0.0043],\n                      [ 0.0039, -0.0271, -0.0032,  ...,  0.0232,  0.0195,  0.0084]])),\n             ('model.model.layers.21.self_attn.k_proj.weight',\n              tensor([[ 3.0171e-02, -1.2960e-02,  2.7729e-03,  ..., -2.1439e-02,\n                       -3.0153e-02, -3.7811e-02],\n                      [-2.8944e-04, -4.4850e-02,  6.1449e-02,  ..., -1.9236e-02,\n                        1.2647e-02, -2.6570e-02],\n                      [-2.8081e-02, -1.7242e-02, -2.5270e-02,  ...,  2.1768e-02,\n                       -1.6189e-02,  1.2873e-02],\n                      ...,\n                      [ 6.2939e-03,  3.5847e-02, -5.8062e-03,  ..., -6.7417e-03,\n                        2.6100e-02, -5.9845e-02],\n                      [-1.0919e-02, -3.5219e-02,  1.4870e-02,  ...,  2.0599e-02,\n                        7.2663e-05,  1.9601e-02],\n                      [ 2.9071e-02, -8.7513e-03, -5.1663e-03,  ...,  2.6255e-02,\n                        1.6809e-02, -4.5514e-02]])),\n             ('model.model.layers.21.self_attn.v_proj.weight',\n              tensor([[-2.9801e-03,  1.1492e-02,  3.7409e-02,  ...,  1.7683e-02,\n                       -2.4949e-02,  1.9185e-02],\n                      [ 6.7415e-03,  2.1540e-02,  1.5908e-04,  ...,  2.3597e-02,\n                       -3.6120e-02, -7.9674e-03],\n                      [ 1.0445e-02, -3.4582e-02, -1.9940e-02,  ..., -2.4563e-02,\n                        2.0897e-02,  4.3546e-02],\n                      ...,\n                      [-6.6128e-03, -1.7563e-02, -1.1132e-02,  ...,  1.7838e-02,\n                       -1.8689e-03, -7.4596e-03],\n                      [-3.2859e-04, -1.0916e-02, -1.7114e-02,  ...,  1.4416e-02,\n                        4.0963e-02,  1.0142e-02],\n                      [-7.9486e-05,  7.5267e-03,  1.3812e-02,  ..., -1.4674e-02,\n                       -1.6944e-02,  2.5542e-02]])),\n             ('model.model.layers.21.self_attn.o_proj.weight',\n              tensor([[-0.0206, -0.0108,  0.0088,  ...,  0.0018, -0.0347,  0.0210],\n                      [-0.0211, -0.0143,  0.0146,  ..., -0.0270, -0.0094,  0.0054],\n                      [ 0.0163, -0.0007,  0.0116,  ..., -0.0232,  0.0230,  0.0088],\n                      ...,\n                      [ 0.0410, -0.0070,  0.0354,  ...,  0.0041, -0.0097,  0.0166],\n                      [-0.0293, -0.0229,  0.0262,  ...,  0.0025, -0.0330, -0.0299],\n                      [-0.0207,  0.0037, -0.0006,  ...,  0.0274,  0.0162,  0.0052]])),\n             ('model.model.layers.21.mlp.gate_proj.weight',\n              tensor([[ 0.0416, -0.0134,  0.0419,  ..., -0.0022, -0.0297, -0.0448],\n                      [-0.0287, -0.0122,  0.0017,  ..., -0.0278,  0.0250, -0.0032],\n                      [ 0.0065,  0.0202,  0.0177,  ...,  0.0085, -0.0059, -0.0160],\n                      ...,\n                      [ 0.0327,  0.0050,  0.0083,  ...,  0.0198,  0.0052, -0.0046],\n                      [-0.0346,  0.0067, -0.0057,  ...,  0.0048, -0.0042,  0.0144],\n                      [-0.0249, -0.0229, -0.0006,  ...,  0.0178, -0.0362,  0.0216]])),\n             ('model.model.layers.21.mlp.up_proj.weight',\n              tensor([[ 0.0441, -0.0261, -0.0104,  ...,  0.0016, -0.0325,  0.0054],\n                      [-0.0058,  0.0034, -0.0237,  ..., -0.0203,  0.0091, -0.0204],\n                      [-0.0157, -0.0010,  0.0138,  ...,  0.0034,  0.0250,  0.0070],\n                      ...,\n                      [ 0.0338, -0.0141, -0.0407,  ...,  0.0200,  0.0076,  0.0044],\n                      [-0.0010,  0.0053,  0.0193,  ..., -0.0353, -0.0055, -0.0552],\n                      [-0.0204,  0.0107, -0.0421,  ..., -0.0112, -0.0056, -0.0028]])),\n             ('model.model.layers.21.mlp.down_proj.weight',\n              tensor([[-0.0059, -0.0056, -0.0337,  ...,  0.0143,  0.0140, -0.0340],\n                      [-0.0095,  0.0082, -0.0057,  ..., -0.0040,  0.0104,  0.0247],\n                      [ 0.0095, -0.0046,  0.0027,  ...,  0.0274, -0.0145, -0.0002],\n                      ...,\n                      [ 0.0015,  0.0245,  0.0066,  ..., -0.0136,  0.0080,  0.0199],\n                      [-0.0241, -0.0184, -0.0090,  ..., -0.0037,  0.0143,  0.0319],\n                      [ 0.0188,  0.0347, -0.0120,  ..., -0.0281, -0.0020,  0.0198]])),\n             ('model.model.layers.21.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.21.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.22.self_attn.q_proj.weight',\n              tensor([[ 0.0179, -0.0281, -0.0003,  ...,  0.0169, -0.0128, -0.0250],\n                      [-0.0086, -0.0070, -0.0154,  ..., -0.0623, -0.0277, -0.0144],\n                      [ 0.0059,  0.0227, -0.0217,  ..., -0.0104, -0.0074,  0.0304],\n                      ...,\n                      [ 0.0138,  0.0512, -0.0284,  ...,  0.0276, -0.0031,  0.0031],\n                      [-0.0471,  0.0291, -0.0224,  ...,  0.0184, -0.0106,  0.0076],\n                      [-0.0066,  0.0015,  0.0119,  ...,  0.0067, -0.0254, -0.0283]])),\n             ('model.model.layers.22.self_attn.k_proj.weight',\n              tensor([[-0.0295, -0.0163,  0.0501,  ...,  0.0277,  0.0303, -0.0070],\n                      [-0.0429, -0.0054,  0.0089,  ...,  0.0005,  0.0296,  0.0211],\n                      [-0.0250, -0.0198,  0.0248,  ...,  0.0185, -0.0192,  0.0203],\n                      ...,\n                      [ 0.0010, -0.0134,  0.0015,  ...,  0.0280,  0.0457, -0.0030],\n                      [-0.0151,  0.0389,  0.0123,  ...,  0.0181,  0.0074,  0.0212],\n                      [-0.0230, -0.0076,  0.0166,  ...,  0.0021,  0.0202, -0.0335]])),\n             ('model.model.layers.22.self_attn.v_proj.weight',\n              tensor([[ 0.0269,  0.0115, -0.0110,  ..., -0.0005, -0.0200,  0.0165],\n                      [-0.0008,  0.0232,  0.0115,  ..., -0.0044,  0.0157,  0.0047],\n                      [ 0.0088, -0.0199, -0.0336,  ..., -0.0221,  0.0046, -0.0058],\n                      ...,\n                      [-0.0164,  0.0170, -0.0152,  ..., -0.0209, -0.0055, -0.0066],\n                      [ 0.0025,  0.0072, -0.0302,  ...,  0.0546, -0.0099,  0.0025],\n                      [-0.0159,  0.0037, -0.0006,  ..., -0.0107, -0.0005,  0.0065]])),\n             ('model.model.layers.22.self_attn.o_proj.weight',\n              tensor([[-0.0440,  0.0116,  0.0116,  ..., -0.0508, -0.0201,  0.0192],\n                      [ 0.0026, -0.0050,  0.0152,  ..., -0.0113,  0.0195, -0.0008],\n                      [-0.0127,  0.0113,  0.0363,  ..., -0.0140, -0.0316, -0.0029],\n                      ...,\n                      [-0.0272,  0.0156, -0.0413,  ...,  0.0209, -0.0193, -0.0200],\n                      [ 0.0052,  0.0084,  0.0152,  ..., -0.0171, -0.0138,  0.0277],\n                      [ 0.0068,  0.0071,  0.0247,  ...,  0.0359, -0.0118, -0.0055]])),\n             ('model.model.layers.22.mlp.gate_proj.weight',\n              tensor([[-5.1442e-03, -4.6509e-02,  1.1725e-04,  ..., -1.4752e-02,\n                       -2.2426e-02,  4.3284e-03],\n                      [ 3.6819e-04, -7.6474e-04,  8.3738e-03,  ...,  3.8761e-02,\n                       -1.7868e-02,  6.7040e-03],\n                      [-3.5232e-02, -2.3245e-03,  7.7156e-05,  ..., -1.4166e-02,\n                       -7.0929e-03,  6.1805e-03],\n                      ...,\n                      [-3.7806e-02, -2.5944e-03,  6.2795e-03,  ..., -1.2309e-02,\n                        1.8679e-02,  6.6621e-03],\n                      [ 8.2387e-03,  1.2353e-02,  6.8794e-03,  ...,  3.4939e-02,\n                        1.6232e-02,  7.3236e-03],\n                      [-1.2000e-02,  4.4941e-02, -1.1718e-03,  ...,  8.5656e-03,\n                       -3.1830e-02, -5.2502e-04]])),\n             ('model.model.layers.22.mlp.up_proj.weight',\n              tensor([[-0.0320,  0.0511, -0.0212,  ..., -0.0125, -0.0187, -0.0139],\n                      [-0.0007,  0.0205,  0.0091,  ...,  0.0221,  0.0197,  0.0109],\n                      [-0.0037, -0.0077, -0.0521,  ..., -0.0010,  0.0063,  0.0367],\n                      ...,\n                      [-0.0054, -0.0163, -0.0204,  ..., -0.0134, -0.0170, -0.0194],\n                      [-0.0200, -0.0258, -0.0045,  ..., -0.0094, -0.0302,  0.0014],\n                      [-0.0330, -0.0068, -0.0228,  ..., -0.0040,  0.0334,  0.0231]])),\n             ('model.model.layers.22.mlp.down_proj.weight',\n              tensor([[-1.0725e-02, -3.0167e-02, -1.4361e-02,  ...,  3.8371e-02,\n                       -6.6930e-03, -5.0232e-02],\n                      [-2.3434e-03, -1.0937e-02,  2.0823e-02,  ...,  1.3920e-02,\n                        2.7493e-02,  6.3887e-03],\n                      [-1.0107e-02, -1.7196e-02,  3.5706e-02,  ..., -2.3314e-02,\n                        1.3283e-02, -1.6478e-02],\n                      ...,\n                      [ 2.5271e-02, -6.8379e-03,  1.5231e-02,  ..., -1.2593e-02,\n                        6.0494e-05,  5.7912e-03],\n                      [ 3.1158e-02,  1.2023e-02, -8.9246e-03,  ...,  8.7839e-03,\n                        6.1669e-03, -1.6545e-02],\n                      [-2.1399e-02, -3.7002e-03, -1.2231e-02,  ..., -2.7754e-02,\n                        3.6536e-02,  3.0622e-02]])),\n             ('model.model.layers.22.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.22.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.23.self_attn.q_proj.weight',\n              tensor([[-0.0254,  0.0291, -0.0211,  ...,  0.0112, -0.0304, -0.0052],\n                      [-0.0220, -0.0041, -0.0102,  ..., -0.0202,  0.0104, -0.0272],\n                      [ 0.0143, -0.0330, -0.0025,  ...,  0.0145,  0.0108,  0.0230],\n                      ...,\n                      [ 0.0194,  0.0438, -0.0235,  ...,  0.0103, -0.0074, -0.0049],\n                      [ 0.0350, -0.0073, -0.0184,  ...,  0.0086,  0.0308,  0.0193],\n                      [ 0.0192, -0.0058,  0.0051,  ...,  0.0313,  0.0152,  0.0085]])),\n             ('model.model.layers.23.self_attn.k_proj.weight',\n              tensor([[-0.0007, -0.0164,  0.0505,  ...,  0.0041,  0.0162,  0.0062],\n                      [-0.0068,  0.0200, -0.0061,  ..., -0.0096,  0.0161, -0.0192],\n                      [-0.0100,  0.0007, -0.0036,  ...,  0.0081, -0.0101, -0.0294],\n                      ...,\n                      [-0.0101, -0.0306,  0.0040,  ..., -0.0246, -0.0138, -0.0012],\n                      [-0.0238, -0.0185,  0.0090,  ..., -0.0125, -0.0164,  0.0166],\n                      [-0.0110,  0.0024, -0.0247,  ...,  0.0023, -0.0232, -0.0040]])),\n             ('model.model.layers.23.self_attn.v_proj.weight',\n              tensor([[ 0.0125, -0.0005, -0.0024,  ...,  0.0103, -0.0030,  0.0275],\n                      [-0.0009, -0.0327, -0.0109,  ...,  0.0149, -0.0420,  0.0123],\n                      [-0.0330,  0.0013,  0.0155,  ...,  0.0185, -0.0267,  0.0238],\n                      ...,\n                      [ 0.0290, -0.0040, -0.0184,  ..., -0.0024,  0.0016, -0.0078],\n                      [ 0.0149,  0.0059, -0.0258,  ...,  0.0188, -0.0096,  0.0090],\n                      [-0.0245,  0.0136, -0.0121,  ..., -0.0017, -0.0059, -0.0250]])),\n             ('model.model.layers.23.self_attn.o_proj.weight',\n              tensor([[-2.9724e-02, -1.9686e-02, -7.2216e-04,  ...,  5.4132e-03,\n                       -2.3700e-02, -2.8632e-03],\n                      [-1.4477e-02, -3.8487e-02, -7.6603e-03,  ...,  3.5934e-02,\n                       -4.8519e-02, -4.1973e-02],\n                      [-3.0362e-02, -1.2450e-03,  3.1827e-03,  ..., -1.6626e-02,\n                       -5.5279e-03,  2.0955e-02],\n                      ...,\n                      [-1.0565e-02, -1.1798e-02,  7.3204e-05,  ..., -1.2580e-03,\n                        1.6282e-02,  1.5915e-02],\n                      [-1.3032e-02, -2.3261e-02,  3.0865e-02,  ...,  1.3529e-02,\n                       -1.0100e-02, -1.2984e-02],\n                      [ 1.2290e-02,  3.6200e-02,  2.0926e-02,  ..., -3.0610e-03,\n                       -5.6956e-03,  4.7990e-03]])),\n             ('model.model.layers.23.mlp.gate_proj.weight',\n              tensor([[ 2.0792e-02, -9.2366e-03, -8.1150e-03,  ..., -1.3367e-02,\n                        4.1330e-02, -3.7294e-02],\n                      [-1.5568e-02, -8.4205e-03,  9.8709e-03,  ...,  5.1385e-02,\n                        1.1578e-03, -4.8478e-03],\n                      [-8.4078e-03, -8.4394e-03,  4.4726e-03,  ..., -2.9320e-03,\n                       -1.2727e-02, -8.3262e-05],\n                      ...,\n                      [-1.3530e-02,  1.1205e-04,  6.9847e-03,  ..., -2.3852e-02,\n                        4.3063e-02,  1.0497e-02],\n                      [ 2.3927e-02,  6.6920e-03, -3.0091e-03,  ...,  5.0490e-03,\n                       -5.9734e-03,  1.1899e-02],\n                      [-1.3631e-02,  1.5478e-02, -2.9703e-02,  ..., -1.1689e-02,\n                        7.1565e-03,  9.9683e-03]])),\n             ('model.model.layers.23.mlp.up_proj.weight',\n              tensor([[ 0.0554,  0.0045, -0.0060,  ..., -0.0183,  0.0267,  0.0053],\n                      [-0.0017, -0.0093,  0.0118,  ...,  0.0055, -0.0319,  0.0265],\n                      [-0.0300,  0.0212, -0.0172,  ..., -0.0155,  0.0018,  0.0089],\n                      ...,\n                      [ 0.0165,  0.0153,  0.0151,  ..., -0.0244,  0.0148, -0.0227],\n                      [-0.0214,  0.0003,  0.0082,  ..., -0.0010,  0.0340, -0.0071],\n                      [ 0.0266,  0.0298, -0.0129,  ...,  0.0050,  0.0140,  0.0112]])),\n             ('model.model.layers.23.mlp.down_proj.weight',\n              tensor([[-0.0043,  0.0089, -0.0275,  ...,  0.0013, -0.0005,  0.0011],\n                      [-0.0051,  0.0131,  0.0158,  ..., -0.0242, -0.0235, -0.0061],\n                      [ 0.0125,  0.0017,  0.0062,  ...,  0.0249,  0.0105, -0.0052],\n                      ...,\n                      [-0.0199, -0.0050, -0.0173,  ..., -0.0240,  0.0153,  0.0269],\n                      [ 0.0148, -0.0120,  0.0049,  ...,  0.0252, -0.0013,  0.0157],\n                      [ 0.0073,  0.0146, -0.0163,  ...,  0.0255, -0.0173,  0.0297]])),\n             ('model.model.layers.23.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.23.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.24.self_attn.q_proj.weight',\n              tensor([[ 2.0453e-02,  2.2948e-03, -7.6199e-04,  ...,  2.8491e-02,\n                       -2.7341e-02,  1.2033e-02],\n                      [-2.9037e-03, -9.6891e-03,  1.8963e-02,  ..., -1.9571e-02,\n                       -2.4542e-02, -3.7313e-04],\n                      [-1.3763e-02, -9.3849e-03,  8.1097e-03,  ...,  8.4955e-05,\n                        4.9532e-03, -2.9676e-02],\n                      ...,\n                      [-3.8587e-02, -1.9870e-02, -7.0207e-03,  ...,  2.0066e-02,\n                       -1.7759e-02,  7.4680e-03],\n                      [ 1.8242e-02,  5.1483e-02,  2.7857e-02,  ...,  8.4614e-03,\n                       -4.2819e-02, -3.3459e-03],\n                      [ 5.9143e-03,  6.1318e-03, -1.2474e-02,  ..., -6.0536e-03,\n                       -9.8974e-03, -1.3057e-02]])),\n             ('model.model.layers.24.self_attn.k_proj.weight',\n              tensor([[ 9.0547e-03,  8.5448e-03,  3.0570e-02,  ...,  1.7497e-02,\n                       -1.3008e-02, -4.8016e-02],\n                      [ 1.4426e-02, -9.1044e-03, -1.8570e-02,  ...,  1.2217e-02,\n                        1.2895e-02, -2.9505e-02],\n                      [ 1.2624e-03,  1.3943e-02,  2.6074e-02,  ..., -1.6087e-02,\n                       -5.6432e-03, -1.5274e-02],\n                      ...,\n                      [-3.1574e-02,  6.1396e-03, -2.4829e-02,  ..., -3.4485e-03,\n                        1.7246e-02, -3.4194e-03],\n                      [ 1.0956e-02, -3.6196e-03, -1.1062e-02,  ..., -1.6693e-03,\n                        1.8626e-03,  6.3062e-03],\n                      [-1.2172e-02, -7.7319e-03, -2.8324e-02,  ...,  9.7727e-05,\n                       -2.0068e-02,  1.6603e-04]])),\n             ('model.model.layers.24.self_attn.v_proj.weight',\n              tensor([[-0.0264,  0.0121, -0.0073,  ...,  0.0557,  0.0242, -0.0325],\n                      [-0.0275,  0.0096,  0.0153,  ...,  0.0159,  0.0272, -0.0352],\n                      [-0.0035,  0.0190, -0.0165,  ..., -0.0094,  0.0075,  0.0262],\n                      ...,\n                      [ 0.0076, -0.0226, -0.0032,  ..., -0.0046, -0.0013,  0.0150],\n                      [-0.0176,  0.0334, -0.0237,  ..., -0.0234,  0.0201, -0.0026],\n                      [-0.0048,  0.0358, -0.0056,  ..., -0.0472,  0.0086,  0.0161]])),\n             ('model.model.layers.24.self_attn.o_proj.weight',\n              tensor([[-0.0193,  0.0268, -0.0035,  ..., -0.0070, -0.0341, -0.0038],\n                      [ 0.0083,  0.0247,  0.0014,  ..., -0.0293, -0.0102,  0.0210],\n                      [-0.0206, -0.0215,  0.0473,  ...,  0.0376,  0.0280, -0.0472],\n                      ...,\n                      [-0.0135, -0.0177,  0.0034,  ...,  0.0009, -0.0017, -0.0168],\n                      [-0.0226, -0.0406,  0.0021,  ...,  0.0110,  0.0048,  0.0153],\n                      [ 0.0003,  0.0009,  0.0255,  ...,  0.0048, -0.0271,  0.0012]])),\n             ('model.model.layers.24.mlp.gate_proj.weight',\n              tensor([[ 1.1788e-02, -2.0661e-02,  9.7819e-03,  ...,  1.8799e-02,\n                        1.1716e-02,  1.6843e-02],\n                      [-2.7447e-02, -3.3485e-02,  2.1576e-02,  ...,  1.3188e-02,\n                       -2.4168e-02, -2.9746e-02],\n                      [ 1.4144e-02, -9.5457e-05, -1.3367e-02,  ..., -3.2821e-03,\n                       -1.8862e-02,  1.4462e-02],\n                      ...,\n                      [ 5.9216e-02, -9.8836e-03,  2.2074e-03,  ..., -1.0448e-02,\n                        1.4623e-02,  8.9618e-05],\n                      [ 5.3824e-03,  1.2975e-03,  2.8846e-02,  ..., -1.0554e-03,\n                        3.0815e-02,  2.6008e-02],\n                      [-4.3151e-02,  1.4906e-02,  1.7072e-02,  ..., -2.1000e-02,\n                        1.8395e-02, -7.3212e-03]])),\n             ('model.model.layers.24.mlp.up_proj.weight',\n              tensor([[ 0.0188, -0.0141, -0.0058,  ..., -0.0299, -0.0069,  0.0036],\n                      [-0.0076,  0.0016, -0.0247,  ..., -0.0054,  0.0088, -0.0030],\n                      [-0.0032, -0.0317,  0.0226,  ..., -0.0097,  0.0010,  0.0111],\n                      ...,\n                      [-0.0246, -0.0328,  0.0142,  ..., -0.0222,  0.0073, -0.0146],\n                      [-0.0031,  0.0014, -0.0016,  ...,  0.0194,  0.0388,  0.0255],\n                      [-0.0166,  0.0172, -0.0089,  ...,  0.0261, -0.0203, -0.0104]])),\n             ('model.model.layers.24.mlp.down_proj.weight',\n              tensor([[ 1.8017e-02,  7.5366e-03, -1.0163e-02,  ...,  1.9962e-02,\n                        1.6107e-02, -1.2911e-02],\n                      [ 8.3261e-03, -3.7513e-02, -9.4220e-04,  ...,  3.4863e-03,\n                        1.4041e-02, -3.5716e-02],\n                      [ 2.3441e-03, -2.7340e-06,  1.0478e-02,  ...,  5.9582e-05,\n                       -2.1521e-02,  4.7798e-03],\n                      ...,\n                      [-3.3556e-03, -2.4993e-02,  2.7208e-02,  ..., -5.8480e-02,\n                        9.9572e-03,  2.6488e-03],\n                      [-1.3112e-02, -1.5789e-02, -1.1950e-02,  ...,  1.7985e-03,\n                        2.6819e-03, -1.2092e-02],\n                      [-3.2191e-02,  1.2048e-02, -1.4215e-02,  ...,  6.5083e-03,\n                       -1.5345e-02, -1.8549e-02]])),\n             ('model.model.layers.24.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.24.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.25.self_attn.q_proj.weight',\n              tensor([[-0.0228,  0.0064, -0.0064,  ..., -0.0304,  0.0225, -0.0033],\n                      [ 0.0193, -0.0415,  0.0002,  ..., -0.0128,  0.0153, -0.0109],\n                      [-0.0021,  0.0168, -0.0097,  ...,  0.0294, -0.0173,  0.0056],\n                      ...,\n                      [ 0.0114, -0.0008, -0.0098,  ...,  0.0193, -0.0423, -0.0093],\n                      [-0.0099, -0.0232,  0.0417,  ..., -0.0087,  0.0118,  0.0046],\n                      [-0.0256, -0.0090, -0.0075,  ..., -0.0074, -0.0151, -0.0003]])),\n             ('model.model.layers.25.self_attn.k_proj.weight',\n              tensor([[-0.0088, -0.0039, -0.0047,  ...,  0.0215,  0.0189, -0.0256],\n                      [-0.0105,  0.0126,  0.0162,  ...,  0.0063,  0.0130,  0.0268],\n                      [ 0.0059, -0.0424,  0.0022,  ..., -0.0127, -0.0438, -0.0027],\n                      ...,\n                      [ 0.0233, -0.0161, -0.0011,  ...,  0.0072, -0.0165, -0.0204],\n                      [-0.0244, -0.0089,  0.0198,  ..., -0.0193, -0.0300, -0.0074],\n                      [ 0.0245, -0.0038, -0.0040,  ..., -0.0067, -0.0184,  0.0281]])),\n             ('model.model.layers.25.self_attn.v_proj.weight',\n              tensor([[-0.0189,  0.0185, -0.0196,  ...,  0.0207, -0.0044,  0.0126],\n                      [-0.0174, -0.0070,  0.0315,  ..., -0.0095,  0.0213,  0.0055],\n                      [ 0.0059, -0.0031,  0.0019,  ...,  0.0008, -0.0125,  0.0014],\n                      ...,\n                      [ 0.0253,  0.0088,  0.0066,  ..., -0.0374, -0.0264,  0.0020],\n                      [-0.0159,  0.0065, -0.0028,  ..., -0.0273,  0.0136,  0.0039],\n                      [-0.0099, -0.0091, -0.0024,  ...,  0.0132,  0.0227, -0.0172]])),\n             ('model.model.layers.25.self_attn.o_proj.weight',\n              tensor([[ 1.0986e-02,  1.9088e-02, -2.1901e-02,  ..., -7.1116e-03,\n                        1.8692e-02,  2.1021e-02],\n                      [ 1.4387e-02, -2.3060e-02, -4.4036e-02,  ...,  8.0584e-03,\n                       -2.3140e-02, -2.1726e-03],\n                      [ 3.1622e-02,  2.3547e-02, -1.0779e-02,  ...,  7.3155e-03,\n                        4.9328e-03,  2.7969e-02],\n                      ...,\n                      [-6.5729e-03,  2.5625e-02,  1.5346e-05,  ...,  4.6757e-03,\n                       -1.6567e-02, -9.7882e-04],\n                      [-1.7038e-02,  4.0263e-02, -7.5526e-03,  ...,  1.2351e-02,\n                        3.5012e-02,  4.4621e-02],\n                      [-4.6458e-03, -8.1869e-03,  1.8399e-03,  ...,  3.1148e-02,\n                        1.6195e-02, -3.6264e-03]])),\n             ('model.model.layers.25.mlp.gate_proj.weight',\n              tensor([[-0.0078, -0.0168,  0.0010,  ...,  0.0204, -0.0293,  0.0072],\n                      [-0.0052, -0.0185,  0.0079,  ...,  0.0067,  0.0085, -0.0104],\n                      [-0.0090, -0.0221, -0.0087,  ...,  0.0152,  0.0009, -0.0181],\n                      ...,\n                      [-0.0130,  0.0028, -0.0206,  ...,  0.0180, -0.0077,  0.0076],\n                      [-0.0162, -0.0387, -0.0226,  ...,  0.0078,  0.0227,  0.0033],\n                      [ 0.0049,  0.0105, -0.0035,  ...,  0.0088,  0.0023,  0.0191]])),\n             ('model.model.layers.25.mlp.up_proj.weight',\n              tensor([[-0.0336, -0.0321,  0.0266,  ..., -0.0240, -0.0061, -0.0152],\n                      [-0.0017,  0.0228,  0.0040,  ...,  0.0115,  0.0019,  0.0172],\n                      [ 0.0084, -0.0136,  0.0116,  ..., -0.0116,  0.0211,  0.0072],\n                      ...,\n                      [-0.0009,  0.0054, -0.0052,  ...,  0.0094, -0.0495,  0.0250],\n                      [ 0.0132,  0.0211, -0.0027,  ...,  0.0087,  0.0159,  0.0038],\n                      [ 0.0072, -0.0082,  0.0020,  ..., -0.0248, -0.0078, -0.0092]])),\n             ('model.model.layers.25.mlp.down_proj.weight',\n              tensor([[ 0.0036,  0.0125, -0.0018,  ...,  0.0074, -0.0216,  0.0147],\n                      [-0.0017, -0.0166,  0.0268,  ..., -0.0175, -0.0301, -0.0045],\n                      [-0.0103,  0.0028, -0.0361,  ..., -0.0133, -0.0162,  0.0156],\n                      ...,\n                      [ 0.0419,  0.0285,  0.0348,  ..., -0.0148, -0.0128, -0.0242],\n                      [ 0.0160, -0.0045,  0.0123,  ..., -0.0313,  0.0312,  0.0085],\n                      [-0.0124,  0.0083,  0.0061,  ...,  0.0087, -0.0339,  0.0007]])),\n             ('model.model.layers.25.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.25.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.26.self_attn.q_proj.weight',\n              tensor([[ 0.0284, -0.0312, -0.0482,  ..., -0.0122,  0.0051, -0.0097],\n                      [-0.0070, -0.0047,  0.0054,  ...,  0.0387,  0.0113,  0.0292],\n                      [-0.0124, -0.0079, -0.0255,  ...,  0.0168,  0.0237, -0.0202],\n                      ...,\n                      [ 0.0177,  0.0096, -0.0553,  ..., -0.0275,  0.0081,  0.0229],\n                      [ 0.0297, -0.0187, -0.0158,  ..., -0.0088, -0.0529,  0.0170],\n                      [ 0.0291, -0.0234,  0.0054,  ..., -0.0243,  0.0132,  0.0180]])),\n             ('model.model.layers.26.self_attn.k_proj.weight',\n              tensor([[ 0.0249, -0.0141,  0.0198,  ...,  0.0053,  0.0116,  0.0144],\n                      [-0.0115, -0.0201, -0.0067,  ..., -0.0342, -0.0019, -0.0208],\n                      [ 0.0224,  0.0132, -0.0069,  ..., -0.0210,  0.0078, -0.0185],\n                      ...,\n                      [-0.0292,  0.0079, -0.0131,  ...,  0.0179,  0.0163, -0.0307],\n                      [ 0.0268,  0.0013,  0.0186,  ..., -0.0281, -0.0117,  0.0063],\n                      [-0.0165, -0.0541, -0.0062,  ...,  0.0079,  0.0060, -0.0066]])),\n             ('model.model.layers.26.self_attn.v_proj.weight',\n              tensor([[ 0.0002, -0.0006,  0.0258,  ..., -0.0126,  0.0110, -0.0232],\n                      [ 0.0080, -0.0456, -0.0097,  ..., -0.0133, -0.0197, -0.0335],\n                      [ 0.0272, -0.0106, -0.0063,  ...,  0.0253, -0.0009,  0.0264],\n                      ...,\n                      [ 0.0390, -0.0045, -0.0041,  ...,  0.0038, -0.0090, -0.0325],\n                      [-0.0150, -0.0126,  0.0293,  ..., -0.0174, -0.0332,  0.0046],\n                      [ 0.0108,  0.0437,  0.0061,  ..., -0.0104,  0.0231, -0.0137]])),\n             ('model.model.layers.26.self_attn.o_proj.weight',\n              tensor([[ 0.0057, -0.0047, -0.0006,  ...,  0.0180,  0.0079, -0.0251],\n                      [-0.0049, -0.0243, -0.0185,  ..., -0.0346, -0.0127, -0.0163],\n                      [-0.0016,  0.0126,  0.0105,  ..., -0.0191, -0.0204,  0.0115],\n                      ...,\n                      [ 0.0232, -0.0021,  0.0060,  ...,  0.0254, -0.0328,  0.0149],\n                      [ 0.0099,  0.0319,  0.0054,  ..., -0.0164,  0.0440,  0.0236],\n                      [-0.0076,  0.0036,  0.0191,  ..., -0.0324,  0.0268, -0.0158]])),\n             ('model.model.layers.26.mlp.gate_proj.weight',\n              tensor([[-4.3100e-03, -3.9802e-02,  5.1611e-03,  ...,  1.5449e-02,\n                       -2.0483e-02,  1.8845e-02],\n                      [ 3.9605e-02,  1.0910e-02, -3.0195e-03,  ...,  8.3272e-03,\n                       -5.1248e-02,  3.4019e-02],\n                      [-9.9475e-03,  2.0971e-02, -2.5829e-05,  ...,  4.0233e-02,\n                       -1.8514e-02, -3.0200e-03],\n                      ...,\n                      [ 3.9939e-02,  6.8741e-02, -2.3860e-02,  ...,  1.4668e-02,\n                        2.0938e-02, -5.2462e-03],\n                      [ 2.8058e-04, -3.9012e-02,  2.3880e-02,  ...,  1.8457e-03,\n                        2.6524e-03,  4.0293e-03],\n                      [ 1.2112e-02, -1.4777e-02, -1.8097e-02,  ...,  3.7762e-03,\n                       -1.2561e-02, -1.3172e-02]])),\n             ('model.model.layers.26.mlp.up_proj.weight',\n              tensor([[ 0.0050,  0.0137, -0.0336,  ...,  0.0043,  0.0077, -0.0073],\n                      [ 0.0315, -0.0176,  0.0236,  ...,  0.0026,  0.0046,  0.0243],\n                      [-0.0058,  0.0005,  0.0154,  ..., -0.0009,  0.0086,  0.0172],\n                      ...,\n                      [ 0.0147,  0.0152,  0.0094,  ...,  0.0066,  0.0293, -0.0059],\n                      [-0.0055,  0.0022, -0.0016,  ...,  0.0064, -0.0245,  0.0205],\n                      [-0.0128, -0.0120, -0.0160,  ..., -0.0196, -0.0084,  0.0047]])),\n             ('model.model.layers.26.mlp.down_proj.weight',\n              tensor([[ 0.0032, -0.0140, -0.0109,  ...,  0.0215, -0.0253, -0.0114],\n                      [ 0.0058,  0.0302,  0.0395,  ..., -0.0228,  0.0143, -0.0009],\n                      [ 0.0117, -0.0063, -0.0182,  ...,  0.0254,  0.0252, -0.0125],\n                      ...,\n                      [ 0.0006,  0.0039, -0.0305,  ..., -0.0124, -0.0339, -0.0168],\n                      [ 0.0028, -0.0164,  0.0074,  ..., -0.0029, -0.0258,  0.0114],\n                      [ 0.0041,  0.0114,  0.0144,  ..., -0.0091, -0.0382,  0.0041]])),\n             ('model.model.layers.26.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.26.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.27.self_attn.q_proj.weight',\n              tensor([[-0.0019, -0.0357, -0.0340,  ..., -0.0098,  0.0040, -0.0114],\n                      [-0.0034, -0.0097,  0.0007,  ...,  0.0124,  0.0460,  0.0303],\n                      [ 0.0109, -0.0179,  0.0302,  ..., -0.0323,  0.0109, -0.0320],\n                      ...,\n                      [-0.0039,  0.0165, -0.0307,  ...,  0.0133, -0.0225, -0.0224],\n                      [-0.0298, -0.0032, -0.0005,  ...,  0.0109, -0.0196, -0.0086],\n                      [-0.0207,  0.0055, -0.0155,  ..., -0.0151, -0.0068,  0.0151]])),\n             ('model.model.layers.27.self_attn.k_proj.weight',\n              tensor([[-0.0010,  0.0310, -0.0037,  ...,  0.0057,  0.0113,  0.0055],\n                      [ 0.0019,  0.0076,  0.0073,  ..., -0.0011,  0.0080,  0.0023],\n                      [ 0.0083, -0.0030, -0.0368,  ..., -0.0332, -0.0188,  0.0299],\n                      ...,\n                      [-0.0310, -0.0157, -0.0077,  ..., -0.0052,  0.0182, -0.0163],\n                      [-0.0058,  0.0264, -0.0133,  ...,  0.0399,  0.0492,  0.0226],\n                      [-0.0030, -0.0333,  0.0076,  ...,  0.0026, -0.0180,  0.0042]])),\n             ('model.model.layers.27.self_attn.v_proj.weight',\n              tensor([[ 0.0078, -0.0011,  0.0576,  ...,  0.0155,  0.0105,  0.0003],\n                      [-0.0109,  0.0173,  0.0487,  ...,  0.0020,  0.0246, -0.0190],\n                      [ 0.0359,  0.0244, -0.0131,  ...,  0.0026, -0.0118,  0.0368],\n                      ...,\n                      [-0.0073, -0.0015, -0.0141,  ...,  0.0011, -0.0147, -0.0041],\n                      [ 0.0145, -0.0326,  0.0200,  ...,  0.0043, -0.0295,  0.0153],\n                      [-0.0455, -0.0238, -0.0281,  ...,  0.0299,  0.0491,  0.0090]])),\n             ('model.model.layers.27.self_attn.o_proj.weight',\n              tensor([[ 0.0025, -0.0013, -0.0403,  ...,  0.0121,  0.0223, -0.0034],\n                      [-0.0160,  0.0072, -0.0100,  ..., -0.0047, -0.0297,  0.0459],\n                      [ 0.0375, -0.0075, -0.0098,  ...,  0.0020,  0.0090, -0.0361],\n                      ...,\n                      [-0.0023, -0.0100,  0.0005,  ..., -0.0059,  0.0069,  0.0162],\n                      [ 0.0074, -0.0139, -0.0043,  ...,  0.0101, -0.0026,  0.0181],\n                      [-0.0205,  0.0316,  0.0096,  ...,  0.0301,  0.0078,  0.0054]])),\n             ('model.model.layers.27.mlp.gate_proj.weight',\n              tensor([[-0.0300, -0.0019,  0.0054,  ...,  0.0290, -0.0196,  0.0032],\n                      [-0.0044, -0.0178, -0.0019,  ...,  0.0122, -0.0356, -0.0046],\n                      [ 0.0120,  0.0399,  0.0091,  ..., -0.0342,  0.0214,  0.0090],\n                      ...,\n                      [ 0.0114,  0.0006, -0.0343,  ..., -0.0020, -0.0108, -0.0170],\n                      [ 0.0173, -0.0182, -0.0471,  ...,  0.0093,  0.0091,  0.0211],\n                      [-0.0056,  0.0100,  0.0270,  ..., -0.0273, -0.0073,  0.0123]])),\n             ('model.model.layers.27.mlp.up_proj.weight',\n              tensor([[ 0.0322, -0.0229, -0.0360,  ...,  0.0124, -0.0146,  0.0153],\n                      [-0.0159, -0.0171, -0.0040,  ...,  0.0152,  0.0090,  0.0154],\n                      [ 0.0125, -0.0019, -0.0089,  ...,  0.0013,  0.0132, -0.0241],\n                      ...,\n                      [ 0.0124,  0.0188,  0.0111,  ...,  0.0210, -0.0574, -0.0211],\n                      [ 0.0040,  0.0003,  0.0205,  ...,  0.0017, -0.0026,  0.0278],\n                      [-0.0032, -0.0056,  0.0085,  ..., -0.0003,  0.0234, -0.0299]])),\n             ('model.model.layers.27.mlp.down_proj.weight',\n              tensor([[-0.0149, -0.0229, -0.0480,  ..., -0.0027,  0.0313,  0.0111],\n                      [ 0.0109, -0.0099,  0.0007,  ...,  0.0193,  0.0125,  0.0113],\n                      [ 0.0094, -0.0139, -0.0165,  ..., -0.0130, -0.0399,  0.0154],\n                      ...,\n                      [-0.0028, -0.0251,  0.0396,  ...,  0.0009,  0.0075, -0.0104],\n                      [-0.0069, -0.0051,  0.0143,  ...,  0.0213, -0.0015, -0.0187],\n                      [ 0.0283,  0.0067,  0.0152,  ...,  0.0016, -0.0061, -0.0043]])),\n             ('model.model.layers.27.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.27.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.28.self_attn.q_proj.weight',\n              tensor([[ 0.0097, -0.0037, -0.0434,  ...,  0.0224,  0.0109,  0.0222],\n                      [ 0.0197,  0.0284, -0.0087,  ..., -0.0118,  0.0552, -0.0123],\n                      [ 0.0086,  0.0250,  0.0207,  ...,  0.0243,  0.0077,  0.0099],\n                      ...,\n                      [-0.0343,  0.0303,  0.0017,  ...,  0.0228, -0.0195,  0.0424],\n                      [-0.0231,  0.0059,  0.0013,  ..., -0.0159, -0.0247, -0.0054],\n                      [ 0.0221, -0.0023, -0.0247,  ..., -0.0308,  0.0038,  0.0008]])),\n             ('model.model.layers.28.self_attn.k_proj.weight',\n              tensor([[ 0.0033,  0.0038, -0.0050,  ..., -0.0143, -0.0287, -0.0062],\n                      [ 0.0701,  0.0173,  0.0273,  ..., -0.0134,  0.0225,  0.0195],\n                      [ 0.0049, -0.0099,  0.0206,  ...,  0.0337,  0.0013,  0.0002],\n                      ...,\n                      [-0.0031, -0.0394, -0.0014,  ..., -0.0190, -0.0384, -0.0091],\n                      [ 0.0615, -0.0265,  0.0257,  ...,  0.0248,  0.0019, -0.0353],\n                      [-0.0025,  0.0134,  0.0027,  ..., -0.0143,  0.0077,  0.0103]])),\n             ('model.model.layers.28.self_attn.v_proj.weight',\n              tensor([[-0.0135, -0.0029, -0.0337,  ...,  0.0189,  0.0347, -0.0477],\n                      [ 0.0152, -0.0457, -0.0046,  ..., -0.0100,  0.0149, -0.0209],\n                      [ 0.0310,  0.0299,  0.0013,  ..., -0.0150, -0.0560, -0.0012],\n                      ...,\n                      [ 0.0207,  0.0426,  0.0192,  ...,  0.0030, -0.0097,  0.0021],\n                      [-0.0184, -0.0315, -0.0089,  ..., -0.0166,  0.0013, -0.0163],\n                      [-0.0043, -0.0322, -0.0051,  ..., -0.0238,  0.0053,  0.0029]])),\n             ('model.model.layers.28.self_attn.o_proj.weight',\n              tensor([[-1.1604e-02,  6.3677e-03,  6.4962e-03,  ..., -1.1031e-02,\n                       -1.8209e-02, -1.3537e-02],\n                      [ 1.7865e-02,  3.4250e-02,  4.8039e-03,  ...,  3.3537e-02,\n                       -2.8442e-02,  1.8358e-03],\n                      [ 1.6455e-02,  1.1922e-02, -3.8803e-02,  ..., -1.9302e-02,\n                       -2.4784e-03, -6.2503e-05],\n                      ...,\n                      [ 3.8141e-04,  1.8065e-02,  1.2791e-02,  ...,  4.5163e-03,\n                       -3.2796e-02,  1.7983e-02],\n                      [-2.1311e-02,  1.1713e-02, -2.4740e-03,  ...,  6.6592e-03,\n                       -2.9972e-02,  5.5674e-03],\n                      [-2.2994e-03, -2.0411e-02,  1.7127e-02,  ..., -2.3661e-02,\n                       -3.9357e-02, -2.0127e-03]])),\n             ('model.model.layers.28.mlp.gate_proj.weight',\n              tensor([[ 0.0483, -0.0258, -0.0053,  ..., -0.0262,  0.0072, -0.0437],\n                      [ 0.0193, -0.0174,  0.0045,  ..., -0.0028, -0.0239,  0.0175],\n                      [ 0.0124, -0.0141, -0.0216,  ..., -0.0040,  0.0032, -0.0193],\n                      ...,\n                      [ 0.0005, -0.0221, -0.0278,  ...,  0.0135,  0.0266, -0.0099],\n                      [-0.0005, -0.0138,  0.0232,  ...,  0.0084, -0.0144, -0.0241],\n                      [-0.0389, -0.0053,  0.0308,  ..., -0.0027, -0.0095, -0.0428]])),\n             ('model.model.layers.28.mlp.up_proj.weight',\n              tensor([[-4.6339e-02,  2.3414e-02,  2.2375e-02,  ..., -1.7034e-02,\n                       -2.7200e-02, -4.6909e-03],\n                      [-3.5681e-03,  9.6151e-03,  1.4385e-02,  ...,  1.5888e-02,\n                        1.2544e-02, -2.4074e-03],\n                      [-2.2700e-02,  3.8143e-02,  5.9983e-02,  ...,  9.3857e-03,\n                        4.2907e-02,  1.0680e-02],\n                      ...,\n                      [ 7.8711e-03, -8.4183e-03,  2.4646e-02,  ..., -1.1915e-02,\n                       -1.5342e-02,  6.2679e-04],\n                      [-1.2571e-02,  4.3535e-02,  9.4155e-03,  ..., -2.6528e-02,\n                        1.9297e-02,  6.7367e-06],\n                      [ 1.5952e-02, -2.0885e-02, -1.1301e-02,  ..., -4.3017e-03,\n                       -4.3497e-03,  1.7796e-03]])),\n             ('model.model.layers.28.mlp.down_proj.weight',\n              tensor([[ 0.0278,  0.0125, -0.0122,  ..., -0.0263,  0.0020, -0.0218],\n                      [ 0.0146,  0.0051, -0.0428,  ...,  0.0152, -0.0154,  0.0063],\n                      [-0.0095,  0.0197, -0.0033,  ..., -0.0180, -0.0095,  0.0127],\n                      ...,\n                      [-0.0004, -0.0027,  0.0191,  ...,  0.0336,  0.0157,  0.0031],\n                      [-0.0156,  0.0144,  0.0049,  ..., -0.0012,  0.0351,  0.0089],\n                      [-0.0294, -0.0314, -0.0022,  ..., -0.0127, -0.0208, -0.0345]])),\n             ('model.model.layers.28.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.28.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.29.self_attn.q_proj.weight',\n              tensor([[ 0.0065, -0.0402,  0.0216,  ..., -0.0059, -0.0073, -0.0276],\n                      [ 0.0287, -0.0005,  0.0180,  ...,  0.0062,  0.0259, -0.0139],\n                      [-0.0116,  0.0169,  0.0082,  ...,  0.0085, -0.0115,  0.0014],\n                      ...,\n                      [ 0.0209,  0.0160, -0.0027,  ...,  0.0475,  0.0057, -0.0056],\n                      [ 0.0290,  0.0058,  0.0097,  ...,  0.0135,  0.0109, -0.0127],\n                      [-0.0067, -0.0254, -0.0092,  ..., -0.0297, -0.0232,  0.0504]])),\n             ('model.model.layers.29.self_attn.k_proj.weight',\n              tensor([[-0.0139, -0.0007, -0.0160,  ...,  0.0188,  0.0072, -0.0105],\n                      [ 0.0213, -0.0020,  0.0158,  ..., -0.0265, -0.0277, -0.0058],\n                      [ 0.0169, -0.0025,  0.0003,  ...,  0.0092, -0.0277,  0.0077],\n                      ...,\n                      [-0.0196, -0.0010, -0.0059,  ...,  0.0047,  0.0374, -0.0278],\n                      [-0.0357, -0.0081,  0.0217,  ...,  0.0036, -0.0157,  0.0229],\n                      [ 0.0009,  0.0047,  0.0193,  ..., -0.0057, -0.0076, -0.0031]])),\n             ('model.model.layers.29.self_attn.v_proj.weight',\n              tensor([[-0.0238,  0.0026,  0.0276,  ...,  0.0084, -0.0058,  0.0314],\n                      [ 0.0029,  0.0065, -0.0057,  ..., -0.0296,  0.0144,  0.0502],\n                      [-0.0201, -0.0006,  0.0067,  ...,  0.0118,  0.0072, -0.0249],\n                      ...,\n                      [ 0.0145, -0.0090,  0.0137,  ...,  0.0259,  0.0234, -0.0016],\n                      [-0.0279, -0.0002, -0.0301,  ...,  0.0006, -0.0011, -0.0163],\n                      [ 0.0160, -0.0132,  0.0061,  ...,  0.0205,  0.0107, -0.0130]])),\n             ('model.model.layers.29.self_attn.o_proj.weight',\n              tensor([[-0.0088,  0.0035,  0.0424,  ..., -0.0275,  0.0251, -0.0121],\n                      [ 0.0095, -0.0191,  0.0194,  ...,  0.0064,  0.0271,  0.0143],\n                      [ 0.0083,  0.0101, -0.0196,  ..., -0.0257,  0.0146, -0.0478],\n                      ...,\n                      [ 0.0066,  0.0189, -0.0377,  ...,  0.0128,  0.0013, -0.0104],\n                      [-0.0094,  0.0428,  0.0012,  ..., -0.0298,  0.0111,  0.0210],\n                      [ 0.0125, -0.0011,  0.0043,  ..., -0.0070, -0.0331, -0.0152]])),\n             ('model.model.layers.29.mlp.gate_proj.weight',\n              tensor([[-0.0099,  0.0212,  0.0021,  ...,  0.0056, -0.0016, -0.0236],\n                      [ 0.0168, -0.0103,  0.0004,  ...,  0.0066,  0.0204,  0.0088],\n                      [ 0.0279, -0.0107,  0.0215,  ..., -0.0310,  0.0082, -0.0088],\n                      ...,\n                      [-0.0287,  0.0250, -0.0182,  ..., -0.0181, -0.0007, -0.0186],\n                      [-0.0359,  0.0097,  0.0041,  ...,  0.0095, -0.0147,  0.0128],\n                      [ 0.0023, -0.0274,  0.0034,  ..., -0.0197,  0.0205, -0.0053]])),\n             ('model.model.layers.29.mlp.up_proj.weight',\n              tensor([[-0.0277, -0.0128, -0.0208,  ...,  0.0128,  0.0147,  0.0008],\n                      [ 0.0133,  0.0316,  0.0281,  ..., -0.0186, -0.0009,  0.0028],\n                      [ 0.0111, -0.0084,  0.0055,  ..., -0.0052, -0.0037, -0.0061],\n                      ...,\n                      [ 0.0032,  0.0248, -0.0021,  ..., -0.0177,  0.0072,  0.0135],\n                      [ 0.0099, -0.0116, -0.0185,  ..., -0.0024, -0.0043, -0.0158],\n                      [-0.0167, -0.0012,  0.0053,  ...,  0.0052,  0.0064,  0.0167]])),\n             ('model.model.layers.29.mlp.down_proj.weight',\n              tensor([[ 0.0350, -0.0523, -0.0525,  ..., -0.0372, -0.0166,  0.0206],\n                      [-0.0233, -0.0083, -0.0207,  ...,  0.0164, -0.0125,  0.0083],\n                      [ 0.0400, -0.0179,  0.0040,  ..., -0.0055, -0.0090, -0.0092],\n                      ...,\n                      [-0.0167,  0.0185,  0.0272,  ...,  0.0229, -0.0055,  0.0083],\n                      [ 0.0086, -0.0228,  0.0089,  ...,  0.0482, -0.0019, -0.0143],\n                      [ 0.0141, -0.0050,  0.0064,  ..., -0.0338, -0.0074,  0.0112]])),\n             ('model.model.layers.29.input_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.layers.29.post_attention_layernorm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.model.norm.weight',\n              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n             ('model.lm_head.weight',\n              tensor([[-2.2329e-03,  1.3196e-06, -2.0449e-02,  ..., -1.7959e-02,\n                       -2.9771e-02,  1.3859e-03],\n                      [-2.6125e-02,  1.7284e-02,  1.3007e-02,  ...,  1.1061e-02,\n                        8.5981e-03,  8.1434e-03],\n                      [ 6.7227e-03,  7.7633e-03, -4.7326e-03,  ...,  4.3760e-02,\n                        2.1710e-02, -2.0803e-03],\n                      ...,\n                      [ 2.1178e-02,  3.4066e-02, -9.4033e-03,  ...,  8.6353e-03,\n                       -1.4162e-02,  2.5917e-03],\n                      [-6.3215e-03, -1.6224e-02, -6.4771e-03,  ...,  7.8672e-03,\n                        2.0783e-03,  3.7539e-03],\n                      [-3.0314e-02, -2.1687e-03,  4.9420e-03,  ..., -1.2234e-02,\n                       -8.4047e-03, -7.1266e-03]]))])"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('model.model.embed_tokens.weight',\n              tensor([[ 1.9089e-02, -8.5212e-03, -5.0806e-03,  ...,  1.2034e-02,\n                       -1.0628e-02,  2.4331e-02],\n                      [-4.8080e-05,  6.4076e-03,  1.0679e-02,  ..., -7.2892e-04,\n                        5.9905e-03,  7.0514e-04],\n                      [ 7.6711e-03, -1.7071e-03,  1.0072e-02,  ...,  8.5789e-04,\n                       -6.0404e-03, -1.1977e-02],\n                      ...,\n                      [-1.7551e-03,  1.6520e-02, -2.5553e-03,  ...,  3.5249e-02,\n                        1.2919e-02,  2.7721e-02],\n                      [ 2.7915e-02,  3.4685e-03,  2.0460e-03,  ..., -1.9489e-02,\n                        2.2904e-02,  5.2769e-03],\n                      [ 5.0748e-03, -2.6082e-04,  1.7759e-02,  ...,  5.5835e-02,\n                        1.1389e-03, -6.4412e-03]], device='cuda:0')),\n             ('model.model.layers.0.self_attn.q_proj.weight',\n              tensor([[ 0.0102,  0.0255,  0.0661,  ..., -0.0534, -0.0031, -0.0277],\n                      [-0.0111,  0.0210, -0.0065,  ...,  0.0445, -0.0074, -0.0132],\n                      [-0.0181,  0.0152,  0.0134,  ..., -0.0269, -0.0101, -0.0451],\n                      ...,\n                      [ 0.0028, -0.0050, -0.0256,  ..., -0.0122,  0.0106,  0.0045],\n                      [ 0.0571, -0.0301,  0.0086,  ..., -0.0420, -0.0092,  0.0085],\n                      [-0.0195, -0.0629, -0.0389,  ...,  0.0192, -0.0271,  0.0074]],\n                     device='cuda:0')),\n             ('model.model.layers.0.self_attn.k_proj.weight',\n              tensor([[-0.0179,  0.0401, -0.0062,  ..., -0.0076,  0.0215,  0.0157],\n                      [-0.0134, -0.0429, -0.0306,  ..., -0.0331, -0.0350,  0.0708],\n                      [-0.0104, -0.0204, -0.0007,  ...,  0.0282, -0.0273, -0.0753],\n                      ...,\n                      [ 0.0060,  0.0027, -0.0060,  ..., -0.0254,  0.0481, -0.0149],\n                      [-0.0409,  0.0163,  0.0567,  ...,  0.0286,  0.0466, -0.0725],\n                      [-0.0161,  0.0142, -0.0293,  ...,  0.0592, -0.0682, -0.0073]],\n                     device='cuda:0')),\n             ('model.model.layers.0.self_attn.v_proj.weight',\n              tensor([[ 0.0137,  0.0124, -0.0032,  ...,  0.0176,  0.0163, -0.0120],\n                      [ 0.0095,  0.0203, -0.0140,  ..., -0.0063,  0.0120,  0.0385],\n                      [ 0.0191,  0.0311, -0.0029,  ...,  0.0135,  0.0224,  0.0179],\n                      ...,\n                      [-0.0327,  0.0034,  0.0041,  ..., -0.0035,  0.0244,  0.0111],\n                      [ 0.0138,  0.0301,  0.0106,  ..., -0.0120, -0.0004, -0.0102],\n                      [ 0.0174,  0.0251,  0.0052,  ..., -0.0041, -0.0220,  0.0053]],\n                     device='cuda:0')),\n             ('model.model.layers.0.self_attn.o_proj.weight',\n              tensor([[ 0.0074, -0.0093, -0.0027,  ...,  0.0119, -0.0166,  0.0055],\n                      [-0.0080, -0.0024, -0.0164,  ..., -0.0009,  0.0126, -0.0147],\n                      [-0.0130, -0.0042,  0.0143,  ...,  0.0075,  0.0112, -0.0062],\n                      ...,\n                      [-0.0286,  0.0083, -0.0279,  ...,  0.0205,  0.0052,  0.0090],\n                      [-0.0045, -0.0211, -0.0161,  ...,  0.0134,  0.0186, -0.0132],\n                      [ 0.0117,  0.0243, -0.0311,  ...,  0.0030, -0.0399,  0.0284]],\n                     device='cuda:0')),\n             ('model.model.layers.0.mlp.gate_proj.weight',\n              tensor([[-0.0030,  0.0055,  0.0171,  ..., -0.0047,  0.0129,  0.0102],\n                      [-0.0039, -0.0032,  0.0037,  ...,  0.0443, -0.0208,  0.0499],\n                      [-0.0170,  0.0180, -0.0068,  ..., -0.0275, -0.0082,  0.0233],\n                      ...,\n                      [-0.0460,  0.0112,  0.0179,  ..., -0.0016,  0.0013,  0.0012],\n                      [ 0.0200,  0.0287, -0.0018,  ...,  0.0219,  0.0020, -0.0307],\n                      [ 0.0048,  0.0186,  0.0049,  ..., -0.0416, -0.0408,  0.0083]],\n                     device='cuda:0')),\n             ('model.model.layers.0.mlp.up_proj.weight',\n              tensor([[ 0.0036,  0.0110,  0.0280,  ...,  0.0020, -0.0073, -0.0163],\n                      [-0.0316,  0.0118, -0.0396,  ...,  0.0033,  0.0334, -0.0191],\n                      [-0.0120, -0.0038, -0.0258,  ...,  0.0077,  0.0154, -0.0318],\n                      ...,\n                      [ 0.0250,  0.0211,  0.0304,  ..., -0.0060,  0.0272, -0.0161],\n                      [-0.0332,  0.0447,  0.0056,  ..., -0.0219,  0.0216, -0.0161],\n                      [ 0.0096,  0.0305,  0.0190,  ..., -0.0060, -0.0134,  0.0049]],\n                     device='cuda:0')),\n             ('model.model.layers.0.mlp.down_proj.weight',\n              tensor([[-0.0040,  0.0059,  0.0017,  ..., -0.0029, -0.0381,  0.0119],\n                      [-0.0085, -0.0017, -0.0133,  ...,  0.0394,  0.0131,  0.0196],\n                      [ 0.0112,  0.0081, -0.0113,  ..., -0.0168, -0.0285,  0.0035],\n                      ...,\n                      [ 0.0385,  0.0127, -0.0371,  ..., -0.0047,  0.0118,  0.0250],\n                      [ 0.0015,  0.0038,  0.0072,  ...,  0.0256,  0.0099,  0.0040],\n                      [ 0.0395,  0.0012,  0.0213,  ..., -0.0491,  0.0189,  0.0335]],\n                     device='cuda:0')),\n             ('model.model.layers.0.input_layernorm.weight',\n              tensor([0.4001, 0.4124, 0.3961,  ..., 0.4017, 0.4143, 0.4053], device='cuda:0')),\n             ('model.model.layers.0.post_attention_layernorm.weight',\n              tensor([0.3388, 0.3432, 0.3529,  ..., 0.3506, 0.3502, 0.3505], device='cuda:0')),\n             ('model.model.layers.1.self_attn.q_proj.weight',\n              tensor([[-0.0793, -0.0174,  0.0078,  ...,  0.0042,  0.0083, -0.0007],\n                      [-0.0353, -0.0402, -0.0094,  ..., -0.0581, -0.0089, -0.0209],\n                      [-0.0197, -0.0403, -0.0580,  ...,  0.0135, -0.0420, -0.0131],\n                      ...,\n                      [ 0.0828,  0.0152, -0.0193,  ..., -0.0205,  0.0242,  0.0392],\n                      [ 0.0769,  0.0426, -0.0186,  ...,  0.0143, -0.0342,  0.0248],\n                      [-0.0772, -0.0172, -0.0145,  ..., -0.0168,  0.0368, -0.0108]],\n                     device='cuda:0')),\n             ('model.model.layers.1.self_attn.k_proj.weight',\n              tensor([[ 0.0593,  0.0064,  0.0023,  ..., -0.0578, -0.0320,  0.0680],\n                      [ 0.1784, -0.1923, -0.0893,  ..., -0.0447,  0.0438, -0.0486],\n                      [-0.0975,  0.0795,  0.1980,  ...,  0.0409, -0.1306,  0.0145],\n                      ...,\n                      [-0.0831, -0.0044,  0.0506,  ...,  0.0157, -0.1013, -0.0567],\n                      [-0.0391, -0.0146,  0.0285,  ..., -0.0010, -0.0336, -0.0552],\n                      [ 0.0316, -0.0262,  0.0228,  ...,  0.0225,  0.0238, -0.0041]],\n                     device='cuda:0')),\n             ('model.model.layers.1.self_attn.v_proj.weight',\n              tensor([[-0.0034,  0.0422,  0.0123,  ...,  0.0130, -0.0113,  0.0033],\n                      [ 0.0026,  0.0266,  0.0173,  ...,  0.0011, -0.0127, -0.0265],\n                      [-0.0155,  0.0140,  0.0108,  ...,  0.0115, -0.0243,  0.0026],\n                      ...,\n                      [-0.0076,  0.0174, -0.0106,  ..., -0.0034,  0.0228, -0.0185],\n                      [-0.0074,  0.0022,  0.0128,  ..., -0.0412,  0.0104,  0.0274],\n                      [ 0.0153, -0.0106, -0.0030,  ...,  0.0086, -0.0286,  0.0044]],\n                     device='cuda:0')),\n             ('model.model.layers.1.self_attn.o_proj.weight',\n              tensor([[-1.5010e-03,  2.1908e-02,  7.0745e-03,  ..., -2.3754e-02,\n                        6.1394e-03,  4.4744e-02],\n                      [ 1.4386e-02,  2.6940e-03,  8.5363e-04,  ..., -3.9530e-03,\n                        1.1491e-02,  1.8442e-02],\n                      [ 7.8995e-03,  3.1829e-02,  4.0792e-03,  ..., -2.4915e-03,\n                        1.5022e-03,  4.6093e-03],\n                      ...,\n                      [ 1.9243e-02,  9.9195e-03,  3.4026e-03,  ...,  2.7789e-02,\n                       -6.8344e-03,  4.2757e-03],\n                      [-4.2561e-05, -1.3344e-02,  1.5615e-02,  ..., -1.2566e-02,\n                        8.5262e-03, -2.1790e-02],\n                      [ 3.6845e-03, -3.5727e-02, -4.9830e-03,  ...,  7.2717e-03,\n                        1.9496e-03, -3.7194e-03]], device='cuda:0')),\n             ('model.model.layers.1.mlp.gate_proj.weight',\n              tensor([[-0.0187, -0.0495, -0.0056,  ..., -0.0551, -0.0318,  0.0152],\n                      [-0.0124, -0.0128,  0.0241,  ..., -0.0162,  0.0318,  0.0077],\n                      [ 0.0026, -0.0128, -0.0191,  ..., -0.0013, -0.0207, -0.0199],\n                      ...,\n                      [-0.0222,  0.0385,  0.0058,  ..., -0.0196,  0.0201,  0.0101],\n                      [-0.0276, -0.0083,  0.0037,  ..., -0.0024,  0.0183, -0.0092],\n                      [ 0.0263, -0.0046, -0.0131,  ...,  0.0173,  0.0028, -0.0185]],\n                     device='cuda:0')),\n             ('model.model.layers.1.mlp.up_proj.weight',\n              tensor([[ 0.0030, -0.0086,  0.0233,  ...,  0.0225,  0.0108, -0.0164],\n                      [ 0.0029, -0.0240, -0.0177,  ..., -0.0004,  0.0083, -0.0269],\n                      [-0.0022,  0.0203,  0.0208,  ...,  0.0154, -0.0225,  0.0118],\n                      ...,\n                      [-0.0033, -0.0093,  0.0192,  ...,  0.0247, -0.0024, -0.0224],\n                      [-0.0042,  0.0038, -0.0028,  ..., -0.0369,  0.0069, -0.0190],\n                      [-0.0189, -0.0282,  0.0146,  ..., -0.0058,  0.0074, -0.0077]],\n                     device='cuda:0')),\n             ('model.model.layers.1.mlp.down_proj.weight',\n              tensor([[ 0.0261, -0.0075,  0.0126,  ...,  0.0255,  0.0160,  0.0275],\n                      [ 0.0215,  0.0320,  0.0022,  ...,  0.0298, -0.0286, -0.0004],\n                      [-0.0007, -0.0189,  0.0126,  ..., -0.0298,  0.0316,  0.0128],\n                      ...,\n                      [-0.0082, -0.0170,  0.0195,  ...,  0.0213, -0.0244, -0.0236],\n                      [ 0.0259, -0.0328,  0.0002,  ...,  0.0242, -0.0230,  0.0144],\n                      [ 0.0024,  0.0254,  0.0260,  ...,  0.0221,  0.0144,  0.0158]],\n                     device='cuda:0')),\n             ('model.model.layers.1.input_layernorm.weight',\n              tensor([0.0817, 0.0809, 0.0818,  ..., 0.0762, 0.0870, 0.0786], device='cuda:0')),\n             ('model.model.layers.1.post_attention_layernorm.weight',\n              tensor([0.2602, 0.2649, 0.2712,  ..., 0.2466, 0.2577, 0.2604], device='cuda:0')),\n             ('model.model.layers.2.self_attn.q_proj.weight',\n              tensor([[-0.0119,  0.0288,  0.0414,  ...,  0.0207, -0.0008, -0.0111],\n                      [-0.0121,  0.0159, -0.0007,  ...,  0.0213, -0.0207,  0.0294],\n                      [ 0.0179, -0.0002, -0.0268,  ...,  0.0325,  0.0276,  0.0272],\n                      ...,\n                      [ 0.0402, -0.0226,  0.0182,  ..., -0.0064,  0.0360, -0.0101],\n                      [ 0.0020, -0.0330, -0.0084,  ...,  0.0058,  0.0158, -0.0216],\n                      [-0.0068, -0.0161, -0.0057,  ...,  0.0222, -0.0308, -0.0108]],\n                     device='cuda:0')),\n             ('model.model.layers.2.self_attn.k_proj.weight',\n              tensor([[-0.0100, -0.0654,  0.1272,  ...,  0.1378, -0.0642, -0.0673],\n                      [ 0.0249,  0.0362, -0.0423,  ..., -0.1538, -0.0074,  0.0790],\n                      [ 0.0487,  0.0233, -0.0062,  ...,  0.0102, -0.0340, -0.0237],\n                      ...,\n                      [ 0.0155,  0.0811,  0.0167,  ...,  0.0035,  0.0419, -0.0153],\n                      [ 0.0023,  0.0138, -0.0869,  ..., -0.0234, -0.0707,  0.0574],\n                      [-0.0699, -0.0774, -0.0065,  ..., -0.0707, -0.1671,  0.1018]],\n                     device='cuda:0')),\n             ('model.model.layers.2.self_attn.v_proj.weight',\n              tensor([[ 0.0030,  0.0193, -0.0018,  ..., -0.0003,  0.0141, -0.0170],\n                      [ 0.0026,  0.0203, -0.0280,  ..., -0.0013,  0.0204, -0.0006],\n                      [-0.0345,  0.0068,  0.0017,  ..., -0.0065, -0.0059, -0.0221],\n                      ...,\n                      [-0.0031,  0.0413,  0.0011,  ..., -0.0247,  0.0018,  0.0224],\n                      [-0.0027, -0.0181, -0.0213,  ...,  0.0084,  0.0050, -0.0323],\n                      [ 0.0004, -0.0106, -0.0318,  ...,  0.0110,  0.0158,  0.0199]],\n                     device='cuda:0')),\n             ('model.model.layers.2.self_attn.o_proj.weight',\n              tensor([[-4.7514e-02,  3.3845e-02, -2.3485e-02,  ...,  2.4974e-02,\n                        2.1151e-02,  1.5409e-03],\n                      [ 3.1038e-02,  2.3628e-02, -2.6511e-02,  ...,  5.9966e-03,\n                        3.9136e-04, -1.5154e-02],\n                      [-3.9132e-03,  1.7310e-02,  1.1348e-03,  ...,  1.2535e-02,\n                        7.5465e-04, -1.8446e-02],\n                      ...,\n                      [-3.8221e-02,  9.2221e-03, -2.0590e-02,  ...,  4.2679e-02,\n                        1.4825e-02,  3.3700e-04],\n                      [-5.6507e-03, -9.6748e-03,  7.8577e-03,  ...,  2.1021e-02,\n                        1.5956e-02,  2.8018e-02],\n                      [ 9.8336e-05,  4.3899e-03,  1.6820e-02,  ..., -1.6000e-02,\n                        9.7485e-04, -1.9953e-02]], device='cuda:0')),\n             ('model.model.layers.2.mlp.gate_proj.weight',\n              tensor([[ 0.0242,  0.0095,  0.0243,  ..., -0.0123,  0.0190, -0.0027],\n                      [-0.0167,  0.0051,  0.0107,  ..., -0.0019,  0.0135, -0.0174],\n                      [-0.0356, -0.0238, -0.0171,  ..., -0.0054, -0.0174, -0.0150],\n                      ...,\n                      [-0.0180, -0.0073, -0.0077,  ...,  0.0067,  0.0041,  0.0498],\n                      [ 0.0014,  0.0099,  0.0396,  ...,  0.0084, -0.0097,  0.0242],\n                      [ 0.0107, -0.0074, -0.0412,  ..., -0.0063,  0.0114, -0.0010]],\n                     device='cuda:0')),\n             ('model.model.layers.2.mlp.up_proj.weight',\n              tensor([[ 0.0301,  0.0101, -0.0125,  ..., -0.0082,  0.0016, -0.0183],\n                      [-0.0113, -0.0086,  0.0326,  ..., -0.0415,  0.0244,  0.0036],\n                      [-0.0094,  0.0064, -0.0400,  ...,  0.0050,  0.0006, -0.0124],\n                      ...,\n                      [-0.0267, -0.0146,  0.0491,  ...,  0.0095,  0.0062,  0.0075],\n                      [ 0.0044,  0.0037,  0.0016,  ...,  0.0307,  0.0081,  0.0447],\n                      [-0.0074,  0.0203,  0.0021,  ..., -0.0012, -0.0083, -0.0205]],\n                     device='cuda:0')),\n             ('model.model.layers.2.mlp.down_proj.weight',\n              tensor([[ 0.0113, -0.0358,  0.0252,  ..., -0.0094, -0.0401, -0.0386],\n                      [ 0.0344, -0.0014, -0.0076,  ..., -0.0234,  0.0299,  0.0029],\n                      [ 0.0515,  0.0237,  0.0088,  ...,  0.0176,  0.0009,  0.0139],\n                      ...,\n                      [-0.0027, -0.0014,  0.0127,  ..., -0.0064,  0.0182,  0.0226],\n                      [-0.0294, -0.0157,  0.0108,  ...,  0.0198, -0.0412,  0.0282],\n                      [-0.0028, -0.0034,  0.0253,  ...,  0.0229,  0.0053,  0.0171]],\n                     device='cuda:0')),\n             ('model.model.layers.2.input_layernorm.weight',\n              tensor([0.0994, 0.0973, 0.1065,  ..., 0.1060, 0.1125, 0.0962], device='cuda:0')),\n             ('model.model.layers.2.post_attention_layernorm.weight',\n              tensor([0.2444, 0.2485, 0.2580,  ..., 0.2425, 0.2559, 0.2552], device='cuda:0')),\n             ('model.model.layers.3.self_attn.q_proj.weight',\n              tensor([[-0.1086, -0.0955,  0.0471,  ..., -0.0110, -0.0387, -0.0519],\n                      [-0.0316, -0.0596,  0.0513,  ..., -0.0049,  0.0058, -0.0155],\n                      [ 0.0156,  0.0231,  0.0145,  ..., -0.0205, -0.0275,  0.0240],\n                      ...,\n                      [ 0.0063, -0.0497,  0.0042,  ..., -0.0269,  0.0477,  0.0030],\n                      [ 0.0148, -0.0052, -0.0370,  ...,  0.0307, -0.0762, -0.0201],\n                      [ 0.0175, -0.0166,  0.0150,  ...,  0.0109,  0.0133, -0.0032]],\n                     device='cuda:0')),\n             ('model.model.layers.3.self_attn.k_proj.weight',\n              tensor([[ 0.0485,  0.1032, -0.0405,  ..., -0.1253, -0.0089,  0.0816],\n                      [ 0.0299,  0.0889, -0.0308,  ..., -0.1220, -0.0430,  0.0430],\n                      [-0.0911, -0.0970, -0.0747,  ...,  0.1213, -0.0188, -0.0751],\n                      ...,\n                      [ 0.0183,  0.0224,  0.0098,  ...,  0.0325,  0.0140, -0.0340],\n                      [-0.0520, -0.1106,  0.0617,  ..., -0.0680, -0.0167,  0.0098],\n                      [ 0.0012, -0.0137, -0.0008,  ..., -0.0327, -0.0180,  0.0059]],\n                     device='cuda:0')),\n             ('model.model.layers.3.self_attn.v_proj.weight',\n              tensor([[ 3.5170e-03, -3.4141e-02, -3.7863e-03,  ..., -6.9957e-03,\n                        5.3074e-03, -3.3809e-03],\n                      [-5.9984e-03, -8.0746e-03,  3.7939e-02,  ...,  3.3494e-02,\n                       -1.6942e-02,  3.5160e-03],\n                      [-2.0369e-03, -1.3337e-02, -1.5241e-03,  ..., -5.5426e-04,\n                        6.2054e-03,  5.1773e-03],\n                      ...,\n                      [ 6.7502e-03, -5.0117e-04, -5.1680e-03,  ..., -2.2010e-02,\n                       -7.1338e-03, -1.9853e-02],\n                      [ 1.8761e-03,  1.0886e-02, -5.2241e-03,  ...,  1.4794e-02,\n                        6.2010e-04,  1.7550e-03],\n                      [-2.8226e-02,  2.1561e-02,  9.6913e-03,  ...,  4.0517e-03,\n                        2.1332e-02, -3.0074e-05]], device='cuda:0')),\n             ('model.model.layers.3.self_attn.o_proj.weight',\n              tensor([[-0.0389,  0.0132, -0.0058,  ..., -0.0084,  0.0121,  0.0007],\n                      [-0.0063,  0.0072,  0.0146,  ..., -0.0135,  0.0142, -0.0168],\n                      [-0.0117, -0.0079,  0.0043,  ..., -0.0111,  0.0191, -0.0126],\n                      ...,\n                      [ 0.0011, -0.0196,  0.0231,  ...,  0.0015, -0.0111,  0.0181],\n                      [ 0.0087,  0.0178, -0.0011,  ..., -0.0284,  0.0076, -0.0127],\n                      [ 0.0151,  0.0359, -0.0242,  ..., -0.0099,  0.0181, -0.0007]],\n                     device='cuda:0')),\n             ('model.model.layers.3.mlp.gate_proj.weight',\n              tensor([[-0.0382, -0.0358, -0.0235,  ..., -0.0173,  0.0121, -0.0404],\n                      [ 0.0031,  0.0047,  0.0317,  ...,  0.0092,  0.0183, -0.0083],\n                      [-0.0045, -0.0304, -0.0357,  ...,  0.0089,  0.0201, -0.0321],\n                      ...,\n                      [-0.0058,  0.0464, -0.0049,  ...,  0.0031,  0.0030, -0.0306],\n                      [ 0.0003, -0.0103, -0.0282,  ..., -0.0179,  0.0319,  0.0250],\n                      [-0.0061,  0.0077,  0.0190,  ..., -0.0516,  0.0224, -0.0247]],\n                     device='cuda:0')),\n             ('model.model.layers.3.mlp.up_proj.weight',\n              tensor([[ 0.0354,  0.0151, -0.0010,  ...,  0.0143,  0.0055, -0.0245],\n                      [-0.0225, -0.0477, -0.0205,  ..., -0.0273, -0.0103,  0.0332],\n                      [ 0.0228,  0.0055,  0.0487,  ...,  0.0022, -0.0075, -0.0009],\n                      ...,\n                      [ 0.0047,  0.0009,  0.0004,  ..., -0.0249,  0.0046, -0.0178],\n                      [ 0.0211, -0.0414, -0.0187,  ...,  0.0078,  0.0003, -0.0072],\n                      [-0.0119, -0.0416,  0.0076,  ...,  0.0365, -0.0047,  0.0026]],\n                     device='cuda:0')),\n             ('model.model.layers.3.mlp.down_proj.weight',\n              tensor([[ 0.0585, -0.0162,  0.0192,  ..., -0.0037, -0.0120, -0.0181],\n                      [ 0.0134, -0.0248, -0.0329,  ..., -0.0132,  0.0229, -0.0077],\n                      [-0.0080,  0.0009, -0.0186,  ...,  0.0170, -0.0229,  0.0099],\n                      ...,\n                      [-0.0139,  0.0226, -0.0016,  ...,  0.0012,  0.0262,  0.0011],\n                      [ 0.0053,  0.0445, -0.0310,  ..., -0.0441,  0.0033,  0.0132],\n                      [-0.0021, -0.0303, -0.0357,  ..., -0.0076,  0.0197, -0.0157]],\n                     device='cuda:0')),\n             ('model.model.layers.3.input_layernorm.weight',\n              tensor([0.1545, 0.1618, 0.1496,  ..., 0.1440, 0.1595, 0.1430], device='cuda:0')),\n             ('model.model.layers.3.post_attention_layernorm.weight',\n              tensor([0.2577, 0.2438, 0.2627,  ..., 0.2436, 0.2626, 0.2595], device='cuda:0')),\n             ('model.model.layers.4.self_attn.q_proj.weight',\n              tensor([[ 0.0320, -0.0062,  0.0088,  ...,  0.0147, -0.0045,  0.0071],\n                      [ 0.0408, -0.0173,  0.0028,  ...,  0.0583, -0.0464, -0.0113],\n                      [ 0.0007,  0.0115,  0.0095,  ..., -0.0030,  0.0159,  0.0019],\n                      ...,\n                      [ 0.0450, -0.0131, -0.0600,  ..., -0.0123, -0.0072, -0.0188],\n                      [ 0.0148, -0.0366, -0.0186,  ..., -0.0002,  0.0045, -0.0320],\n                      [-0.0383, -0.0083,  0.0145,  ..., -0.0079,  0.0291,  0.0182]],\n                     device='cuda:0')),\n             ('model.model.layers.4.self_attn.k_proj.weight',\n              tensor([[-0.0319, -0.0634,  0.0384,  ...,  0.0036, -0.0523, -0.0252],\n                      [-0.0202,  0.0076,  0.0203,  ...,  0.0184,  0.0234, -0.0518],\n                      [ 0.0315, -0.0242, -0.0120,  ..., -0.0311, -0.0263, -0.0486],\n                      ...,\n                      [-0.0182, -0.0154,  0.0240,  ...,  0.0477, -0.0249, -0.0173],\n                      [-0.0128, -0.0126,  0.0428,  ...,  0.0504,  0.0407, -0.0181],\n                      [-0.0239,  0.0197, -0.0696,  ..., -0.0420,  0.0059,  0.0339]],\n                     device='cuda:0')),\n             ('model.model.layers.4.self_attn.v_proj.weight',\n              tensor([[-0.0057,  0.0014, -0.0003,  ..., -0.0037,  0.0148, -0.0212],\n                      [-0.0147, -0.0035,  0.0002,  ..., -0.0071,  0.0041, -0.0006],\n                      [-0.0229, -0.0292,  0.0201,  ...,  0.0031, -0.0290,  0.0519],\n                      ...,\n                      [-0.0072, -0.0030, -0.0110,  ..., -0.0245,  0.0185, -0.0175],\n                      [-0.0040, -0.0048,  0.0042,  ...,  0.0230,  0.0032, -0.0149],\n                      [-0.0169,  0.0443,  0.0017,  ...,  0.0102,  0.0291, -0.0052]],\n                     device='cuda:0')),\n             ('model.model.layers.4.self_attn.o_proj.weight',\n              tensor([[-0.0188,  0.0059, -0.0016,  ..., -0.0175, -0.0359,  0.0133],\n                      [-0.0250,  0.0092, -0.0254,  ...,  0.0082, -0.0152, -0.0063],\n                      [ 0.0070, -0.0015, -0.0306,  ...,  0.0104,  0.0049,  0.0089],\n                      ...,\n                      [ 0.0299, -0.0015,  0.0034,  ...,  0.0192, -0.0260, -0.0105],\n                      [ 0.0104,  0.0337, -0.0039,  ...,  0.0140, -0.0021,  0.0035],\n                      [ 0.0086,  0.0079, -0.0472,  ..., -0.0049,  0.0085,  0.0153]],\n                     device='cuda:0')),\n             ('model.model.layers.4.mlp.gate_proj.weight',\n              tensor([[-0.0207, -0.0217,  0.0194,  ...,  0.0209,  0.0219, -0.0361],\n                      [ 0.0181, -0.0127,  0.0014,  ..., -0.0032,  0.0161,  0.0092],\n                      [-0.0072,  0.0140,  0.0110,  ...,  0.0053, -0.0032, -0.0070],\n                      ...,\n                      [-0.0041,  0.0133, -0.0059,  ...,  0.0173, -0.0091,  0.0070],\n                      [ 0.0515,  0.0135, -0.0066,  ..., -0.0025, -0.0216,  0.0095],\n                      [-0.0228,  0.0033, -0.0027,  ..., -0.0495, -0.0335, -0.0281]],\n                     device='cuda:0')),\n             ('model.model.layers.4.mlp.up_proj.weight',\n              tensor([[-0.0079, -0.0128,  0.0037,  ...,  0.0140,  0.0068, -0.0060],\n                      [-0.0284, -0.0038,  0.0128,  ...,  0.0019, -0.0009, -0.0325],\n                      [-0.0292,  0.0218,  0.0105,  ...,  0.0015, -0.0265,  0.0340],\n                      ...,\n                      [-0.0293, -0.0256,  0.0198,  ...,  0.0308,  0.0250,  0.0297],\n                      [-0.0064, -0.0349, -0.0187,  ..., -0.0095, -0.0087,  0.0080],\n                      [ 0.0250, -0.0062, -0.0138,  ..., -0.0058, -0.0011,  0.0113]],\n                     device='cuda:0')),\n             ('model.model.layers.4.mlp.down_proj.weight',\n              tensor([[ 0.0357, -0.0458, -0.0127,  ...,  0.0366, -0.0033,  0.0131],\n                      [-0.0076, -0.0043, -0.0261,  ..., -0.0089, -0.0025,  0.0443],\n                      [-0.0011, -0.0171, -0.0164,  ..., -0.0222, -0.0018,  0.0303],\n                      ...,\n                      [-0.0091,  0.0388, -0.0174,  ...,  0.0159,  0.0002, -0.0278],\n                      [ 0.0064,  0.0020,  0.0011,  ..., -0.0301, -0.0037,  0.0086],\n                      [-0.0108, -0.0063,  0.0200,  ..., -0.0034, -0.0118,  0.0009]],\n                     device='cuda:0')),\n             ('model.model.layers.4.input_layernorm.weight',\n              tensor([0.1824, 0.1739, 0.1566,  ..., 0.1471, 0.1647, 0.1574], device='cuda:0')),\n             ('model.model.layers.4.post_attention_layernorm.weight',\n              tensor([0.2569, 0.2592, 0.2693,  ..., 0.2597, 0.2521, 0.2674], device='cuda:0')),\n             ('model.model.layers.5.self_attn.q_proj.weight',\n              tensor([[ 0.0241, -0.0178,  0.0135,  ..., -0.0066,  0.0212, -0.0444],\n                      [ 0.0904, -0.0295,  0.0246,  ...,  0.0030,  0.0207, -0.0126],\n                      [ 0.0675,  0.0307,  0.0112,  ...,  0.0249,  0.0217, -0.0160],\n                      ...,\n                      [-0.0460, -0.0174,  0.0273,  ...,  0.0181,  0.0371, -0.0041],\n                      [ 0.0362,  0.0292, -0.0335,  ..., -0.0338,  0.0118,  0.0048],\n                      [ 0.0452,  0.0281, -0.0149,  ..., -0.0169, -0.0204, -0.0048]],\n                     device='cuda:0')),\n             ('model.model.layers.5.self_attn.k_proj.weight',\n              tensor([[ 0.0452,  0.0503, -0.0241,  ..., -0.0178,  0.0327,  0.0136],\n                      [-0.0685, -0.0364, -0.0133,  ..., -0.0069, -0.0198, -0.0340],\n                      [-0.0268, -0.0341, -0.0131,  ...,  0.0156, -0.0184,  0.0024],\n                      ...,\n                      [-0.0407,  0.0202,  0.0182,  ...,  0.0112, -0.0493,  0.0424],\n                      [ 0.0007,  0.0002,  0.0089,  ...,  0.0004, -0.0282, -0.0425],\n                      [-0.0242,  0.0168,  0.0146,  ..., -0.0009, -0.0172, -0.0231]],\n                     device='cuda:0')),\n             ('model.model.layers.5.self_attn.v_proj.weight',\n              tensor([[-0.0006,  0.0036,  0.0006,  ..., -0.0004, -0.0024, -0.0160],\n                      [ 0.0131, -0.0305,  0.0148,  ..., -0.0072, -0.0003, -0.0009],\n                      [-0.0031, -0.0164, -0.0118,  ...,  0.0108, -0.0148, -0.0092],\n                      ...,\n                      [ 0.0242,  0.0293, -0.0225,  ...,  0.0491, -0.0152, -0.0053],\n                      [ 0.0015, -0.0094, -0.0165,  ..., -0.0017,  0.0220,  0.0111],\n                      [-0.0239, -0.0061,  0.0065,  ...,  0.0153, -0.0081, -0.0035]],\n                     device='cuda:0')),\n             ('model.model.layers.5.self_attn.o_proj.weight',\n              tensor([[-0.0003, -0.0137, -0.0128,  ...,  0.0008, -0.0177,  0.0093],\n                      [-0.0342,  0.0301,  0.0087,  ...,  0.0048,  0.0189,  0.0066],\n                      [ 0.0078, -0.0054,  0.0039,  ...,  0.0208,  0.0092, -0.0107],\n                      ...,\n                      [-0.0076, -0.0104, -0.0023,  ..., -0.0178, -0.0162,  0.0105],\n                      [ 0.0065,  0.0145, -0.0243,  ...,  0.0205,  0.0044, -0.0270],\n                      [ 0.0120,  0.0066, -0.0138,  ..., -0.0341, -0.0093,  0.0061]],\n                     device='cuda:0')),\n             ('model.model.layers.5.mlp.gate_proj.weight',\n              tensor([[-0.0447,  0.0338,  0.0021,  ...,  0.0011,  0.0494,  0.0116],\n                      [ 0.0382,  0.0028, -0.0147,  ..., -0.0037, -0.0005, -0.0165],\n                      [ 0.0122, -0.0256,  0.0167,  ...,  0.0393,  0.0050, -0.0125],\n                      ...,\n                      [-0.0024, -0.0191,  0.0213,  ..., -0.0159,  0.0143,  0.0073],\n                      [-0.0288,  0.0019, -0.0292,  ..., -0.0178, -0.0392, -0.0033],\n                      [-0.0006,  0.0456,  0.0212,  ...,  0.0076, -0.0288, -0.0293]],\n                     device='cuda:0')),\n             ('model.model.layers.5.mlp.up_proj.weight',\n              tensor([[ 0.0008,  0.0192, -0.0018,  ..., -0.0221, -0.0114, -0.0388],\n                      [ 0.0065, -0.0281, -0.0002,  ..., -0.0039, -0.0382,  0.0025],\n                      [ 0.0096, -0.0352, -0.0468,  ..., -0.0083, -0.0069,  0.0200],\n                      ...,\n                      [-0.0415, -0.0324, -0.0236,  ...,  0.0251,  0.0095,  0.0015],\n                      [ 0.0322, -0.0286, -0.0204,  ...,  0.0032,  0.0056,  0.0244],\n                      [-0.0320, -0.0203,  0.0178,  ...,  0.0270,  0.0165, -0.0390]],\n                     device='cuda:0')),\n             ('model.model.layers.5.mlp.down_proj.weight',\n              tensor([[-0.0289, -0.0089,  0.0133,  ..., -0.0302,  0.0157,  0.0121],\n                      [-0.0405, -0.0285, -0.0040,  ..., -0.0119, -0.0191,  0.0197],\n                      [ 0.0043, -0.0410,  0.0283,  ..., -0.0369,  0.0059,  0.0168],\n                      ...,\n                      [ 0.0103, -0.0249,  0.0208,  ..., -0.0224, -0.0434, -0.0092],\n                      [ 0.0448,  0.0076,  0.0327,  ...,  0.0395, -0.0252, -0.0119],\n                      [ 0.0312,  0.0281,  0.0204,  ..., -0.0034, -0.0173, -0.0069]],\n                     device='cuda:0')),\n             ('model.model.layers.5.input_layernorm.weight',\n              tensor([0.1966, 0.1997, 0.1818,  ..., 0.1790, 0.1837, 0.1865], device='cuda:0')),\n             ('model.model.layers.5.post_attention_layernorm.weight',\n              tensor([0.2521, 0.2459, 0.2593,  ..., 0.2598, 0.2614, 0.2639], device='cuda:0')),\n             ('model.model.layers.6.self_attn.q_proj.weight',\n              tensor([[ 6.6858e-03, -6.0909e-02, -4.4495e-03,  ..., -2.9911e-02,\n                       -5.9786e-02,  1.8724e-02],\n                      [ 2.1891e-04,  2.5840e-02, -7.0325e-05,  ..., -2.9039e-02,\n                       -6.8550e-02, -3.5107e-03],\n                      [ 1.8238e-02,  1.6678e-02,  1.0774e-02,  ...,  1.4792e-02,\n                       -2.5729e-02,  5.0131e-02],\n                      ...,\n                      [-3.7916e-02, -2.4549e-02,  1.1517e-04,  ..., -3.2388e-03,\n                       -1.1010e-02, -1.9422e-02],\n                      [-2.0623e-02, -3.0773e-02, -4.4396e-03,  ..., -1.1670e-02,\n                       -2.7476e-02, -1.0022e-02],\n                      [ 2.8761e-02,  2.5321e-02,  1.3962e-03,  ..., -9.3823e-04,\n                        2.0024e-02,  1.6427e-02]], device='cuda:0')),\n             ('model.model.layers.6.self_attn.k_proj.weight',\n              tensor([[-0.0705, -0.0774,  0.0422,  ..., -0.0305,  0.0215, -0.0108],\n                      [-0.0265, -0.1028,  0.0215,  ...,  0.0456, -0.0070, -0.0428],\n                      [-0.0721, -0.0425,  0.0051,  ...,  0.0308,  0.0093, -0.0432],\n                      ...,\n                      [ 0.0445,  0.0037, -0.0082,  ..., -0.0301,  0.0123,  0.0151],\n                      [ 0.1004,  0.0785,  0.0269,  ...,  0.0476,  0.0393, -0.0473],\n                      [-0.0562, -0.0076,  0.0051,  ...,  0.0258, -0.0054, -0.0201]],\n                     device='cuda:0')),\n             ('model.model.layers.6.self_attn.v_proj.weight',\n              tensor([[-0.0088, -0.0006,  0.0079,  ...,  0.0001,  0.0424,  0.0084],\n                      [-0.0073,  0.0349, -0.0209,  ...,  0.0242, -0.0019, -0.0017],\n                      [-0.0486,  0.0172, -0.0109,  ..., -0.0140, -0.0092, -0.0335],\n                      ...,\n                      [ 0.0075,  0.0158,  0.0047,  ...,  0.0052, -0.0189,  0.0294],\n                      [-0.0164,  0.0018,  0.0130,  ...,  0.0243, -0.0138,  0.0178],\n                      [ 0.0019, -0.0382, -0.0328,  ..., -0.0181, -0.0166,  0.0062]],\n                     device='cuda:0')),\n             ('model.model.layers.6.self_attn.o_proj.weight',\n              tensor([[-0.0122,  0.0073,  0.0389,  ..., -0.0039, -0.0060, -0.0020],\n                      [-0.0039,  0.0002, -0.0089,  ...,  0.0329, -0.0061,  0.0387],\n                      [ 0.0055, -0.0037,  0.0018,  ..., -0.0097, -0.0038,  0.0448],\n                      ...,\n                      [ 0.0023, -0.0375,  0.0104,  ...,  0.0141, -0.0215,  0.0025],\n                      [-0.0100, -0.0154,  0.0020,  ...,  0.0070,  0.0124,  0.0137],\n                      [-0.0243, -0.0001,  0.0190,  ..., -0.0047,  0.0107,  0.0160]],\n                     device='cuda:0')),\n             ('model.model.layers.6.mlp.gate_proj.weight',\n              tensor([[-0.0138,  0.0048,  0.0055,  ..., -0.0265, -0.0053,  0.0283],\n                      [ 0.0113, -0.0094, -0.0418,  ..., -0.0137, -0.0088, -0.0089],\n                      [ 0.0235,  0.0421, -0.0242,  ...,  0.0116,  0.0081, -0.0154],\n                      ...,\n                      [ 0.0094,  0.0003,  0.0096,  ...,  0.0015, -0.0042, -0.0003],\n                      [-0.0317,  0.0176,  0.0165,  ..., -0.0042, -0.0011,  0.0162],\n                      [ 0.0080, -0.0397, -0.0245,  ...,  0.0042,  0.0054, -0.0076]],\n                     device='cuda:0')),\n             ('model.model.layers.6.mlp.up_proj.weight',\n              tensor([[ 0.0294,  0.0239,  0.0259,  ...,  0.0135,  0.0323,  0.0047],\n                      [ 0.0065,  0.0186,  0.0005,  ...,  0.0073,  0.0167,  0.0075],\n                      [ 0.0135,  0.0070,  0.0326,  ...,  0.0251, -0.0071, -0.0051],\n                      ...,\n                      [-0.0274, -0.0093,  0.0274,  ..., -0.0215, -0.0366,  0.0027],\n                      [ 0.0122, -0.0216,  0.0198,  ...,  0.0046,  0.0110, -0.0191],\n                      [-0.0208,  0.0101, -0.0499,  ..., -0.0332, -0.0011,  0.0023]],\n                     device='cuda:0')),\n             ('model.model.layers.6.mlp.down_proj.weight',\n              tensor([[ 0.0398, -0.0195, -0.0060,  ...,  0.0219,  0.0181,  0.0384],\n                      [ 0.0016, -0.0081,  0.0203,  ...,  0.0082, -0.0245,  0.0008],\n                      [ 0.0107, -0.0027,  0.0043,  ..., -0.0114,  0.0439,  0.0270],\n                      ...,\n                      [ 0.0436, -0.0145,  0.0218,  ..., -0.0158, -0.0064, -0.0038],\n                      [ 0.0104,  0.0096, -0.0296,  ..., -0.0098,  0.0111, -0.0247],\n                      [ 0.0017,  0.0255, -0.0149,  ...,  0.0101, -0.0084,  0.0291]],\n                     device='cuda:0')),\n             ('model.model.layers.6.input_layernorm.weight',\n              tensor([0.2190, 0.1986, 0.1887,  ..., 0.1958, 0.1885, 0.1895], device='cuda:0')),\n             ('model.model.layers.6.post_attention_layernorm.weight',\n              tensor([0.2621, 0.2548, 0.2709,  ..., 0.2619, 0.2623, 0.2533], device='cuda:0')),\n             ('model.model.layers.7.self_attn.q_proj.weight',\n              tensor([[-0.0167, -0.0240, -0.0236,  ..., -0.0402,  0.0010,  0.0150],\n                      [-0.0158, -0.0027,  0.0289,  ...,  0.0242,  0.0057,  0.0178],\n                      [ 0.0154,  0.0141,  0.0095,  ...,  0.0263, -0.0003, -0.0131],\n                      ...,\n                      [ 0.0534,  0.0041, -0.0120,  ...,  0.0708, -0.0064,  0.0423],\n                      [-0.0305,  0.0022,  0.0066,  ..., -0.0082,  0.0063,  0.0018],\n                      [-0.0179, -0.0386, -0.0152,  ...,  0.0698, -0.0347, -0.0563]],\n                     device='cuda:0')),\n             ('model.model.layers.7.self_attn.k_proj.weight',\n              tensor([[-0.0340, -0.0604,  0.0152,  ..., -0.0145, -0.0503, -0.0804],\n                      [-0.0158,  0.0078, -0.0486,  ...,  0.0250, -0.0013, -0.0084],\n                      [ 0.0243, -0.0114, -0.0032,  ..., -0.0223,  0.0229, -0.0159],\n                      ...,\n                      [ 0.0131, -0.0327, -0.0064,  ..., -0.0390,  0.0051,  0.0310],\n                      [ 0.0187,  0.0054, -0.0276,  ..., -0.0018,  0.0080, -0.0047],\n                      [ 0.0052, -0.0199,  0.0438,  ..., -0.0147,  0.0030, -0.0181]],\n                     device='cuda:0')),\n             ('model.model.layers.7.self_attn.v_proj.weight',\n              tensor([[-0.0002, -0.0304,  0.0106,  ..., -0.0060, -0.0082, -0.0095],\n                      [ 0.0192, -0.0204, -0.0040,  ..., -0.0022, -0.0081, -0.0103],\n                      [ 0.0145,  0.0174, -0.0161,  ..., -0.0128,  0.0042, -0.0285],\n                      ...,\n                      [ 0.0109, -0.0128,  0.0111,  ...,  0.0276,  0.0050,  0.0151],\n                      [ 0.0308, -0.0089,  0.0015,  ..., -0.0180,  0.0173,  0.0289],\n                      [-0.0094,  0.0061,  0.0073,  ...,  0.0194,  0.0168, -0.0423]],\n                     device='cuda:0')),\n             ('model.model.layers.7.self_attn.o_proj.weight',\n              tensor([[ 1.4437e-02, -7.7571e-03, -1.0384e-02,  ...,  8.2843e-03,\n                       -1.9598e-03,  3.0399e-02],\n                      [ 3.1447e-02,  1.3716e-02,  5.8926e-04,  ...,  1.4045e-03,\n                        1.4637e-02,  8.0435e-03],\n                      [-4.8909e-04,  8.8483e-03, -1.0281e-03,  ..., -1.9480e-02,\n                       -3.6177e-02,  2.1764e-02],\n                      ...,\n                      [ 1.0775e-03,  4.2429e-03,  1.2564e-02,  ..., -1.0590e-02,\n                        1.5059e-02, -4.5038e-03],\n                      [-1.3636e-02, -4.5011e-03,  5.2546e-03,  ..., -2.6592e-02,\n                       -1.7437e-02, -5.0281e-06],\n                      [ 1.0182e-02,  1.6520e-02,  1.6105e-02,  ...,  3.8491e-02,\n                       -2.0010e-02,  2.8924e-02]], device='cuda:0')),\n             ('model.model.layers.7.mlp.gate_proj.weight',\n              tensor([[ 0.0079, -0.0131,  0.0209,  ...,  0.0222, -0.0160,  0.0076],\n                      [ 0.0098,  0.0203, -0.0355,  ..., -0.0112, -0.0140, -0.0023],\n                      [-0.0112,  0.0238,  0.0006,  ..., -0.0495, -0.0076,  0.0528],\n                      ...,\n                      [-0.0085, -0.0228,  0.0148,  ..., -0.0051, -0.0096, -0.0204],\n                      [ 0.0037,  0.0329,  0.0125,  ...,  0.0164, -0.0064, -0.0056],\n                      [-0.0146, -0.0407, -0.0085,  ...,  0.0500,  0.0210, -0.0260]],\n                     device='cuda:0')),\n             ('model.model.layers.7.mlp.up_proj.weight',\n              tensor([[ 0.0209,  0.0231,  0.0065,  ..., -0.0224,  0.0213, -0.0072],\n                      [-0.0170,  0.0068,  0.0434,  ...,  0.0139,  0.0100,  0.0033],\n                      [ 0.0052,  0.0201,  0.0376,  ...,  0.0128,  0.0200,  0.0273],\n                      ...,\n                      [ 0.0075, -0.0297,  0.0341,  ..., -0.0122,  0.0145, -0.0076],\n                      [ 0.0079, -0.0268,  0.0255,  ..., -0.0163,  0.0273,  0.0183],\n                      [-0.0174,  0.0064,  0.0143,  ..., -0.0032, -0.0029,  0.0170]],\n                     device='cuda:0')),\n             ('model.model.layers.7.mlp.down_proj.weight',\n              tensor([[-2.4747e-05,  1.8290e-05, -1.6105e-02,  ..., -7.9883e-03,\n                       -1.5343e-02,  5.8337e-03],\n                      [ 1.0273e-02, -1.0302e-03,  1.5401e-02,  ...,  1.3688e-02,\n                       -6.6996e-04,  3.7799e-02],\n                      [ 2.2215e-02, -4.8951e-03, -3.7809e-02,  ...,  5.3514e-03,\n                       -3.2285e-02,  2.6763e-02],\n                      ...,\n                      [-1.8074e-02, -1.5146e-02, -1.8481e-02,  ...,  8.7823e-03,\n                       -1.3132e-02, -2.0427e-02],\n                      [-2.3807e-02, -1.9362e-04, -4.2436e-02,  ...,  1.3350e-02,\n                        1.2232e-02, -3.5591e-02],\n                      [ 2.2371e-02, -1.7956e-02,  7.7359e-04,  ..., -2.0723e-02,\n                        3.5862e-02,  2.7047e-02]], device='cuda:0')),\n             ('model.model.layers.7.input_layernorm.weight',\n              tensor([0.2300, 0.2251, 0.2200,  ..., 0.2268, 0.2015, 0.2066], device='cuda:0')),\n             ('model.model.layers.7.post_attention_layernorm.weight',\n              tensor([0.2638, 0.2702, 0.2772,  ..., 0.2606, 0.2711, 0.2739], device='cuda:0')),\n             ('model.model.layers.8.self_attn.q_proj.weight',\n              tensor([[ 0.0006, -0.0071, -0.0610,  ...,  0.0031, -0.0233,  0.0383],\n                      [-0.0334,  0.0382,  0.0093,  ...,  0.0284, -0.0188,  0.0501],\n                      [-0.0069, -0.0514,  0.0459,  ..., -0.0113,  0.0391, -0.0046],\n                      ...,\n                      [ 0.0261,  0.0074, -0.0345,  ...,  0.0208, -0.0372, -0.0039],\n                      [ 0.0184,  0.0416,  0.0325,  ...,  0.0439,  0.0058,  0.0364],\n                      [ 0.0203,  0.0156,  0.0064,  ...,  0.0228,  0.0066,  0.0237]],\n                     device='cuda:0')),\n             ('model.model.layers.8.self_attn.k_proj.weight',\n              tensor([[-7.5317e-02, -1.0131e-01,  5.5690e-02,  ...,  2.5758e-02,\n                        4.1221e-03, -3.5364e-02],\n                      [-2.8243e-04, -5.2343e-02,  6.3566e-02,  ...,  8.5131e-02,\n                       -2.2992e-02,  8.1129e-03],\n                      [-6.6418e-02, -1.0704e-01,  1.7028e-06,  ...,  2.8073e-02,\n                        4.4070e-04, -3.4934e-02],\n                      ...,\n                      [ 3.4622e-03, -4.0972e-02,  2.1831e-02,  ..., -4.4049e-02,\n                        4.5953e-03,  2.5808e-02],\n                      [ 2.9081e-02,  2.0681e-04, -3.9297e-03,  ..., -9.2574e-03,\n                       -3.0116e-02, -4.8148e-02],\n                      [-2.2547e-02, -1.2553e-02,  3.7636e-02,  ..., -1.8360e-02,\n                       -3.5991e-03, -2.6611e-02]], device='cuda:0')),\n             ('model.model.layers.8.self_attn.v_proj.weight',\n              tensor([[ 0.0008,  0.0261,  0.0140,  ...,  0.0046, -0.0182, -0.0312],\n                      [-0.0246, -0.0077, -0.0303,  ...,  0.0261,  0.0070, -0.0250],\n                      [ 0.0088, -0.0075, -0.0074,  ...,  0.0233, -0.0024, -0.0152],\n                      ...,\n                      [-0.0051, -0.0129, -0.0197,  ..., -0.0343, -0.0060, -0.0045],\n                      [ 0.0238,  0.0190,  0.0042,  ...,  0.0125, -0.0198,  0.0131],\n                      [ 0.0015,  0.0033, -0.0028,  ...,  0.0004, -0.0198,  0.0056]],\n                     device='cuda:0')),\n             ('model.model.layers.8.self_attn.o_proj.weight',\n              tensor([[ 0.0124, -0.0120, -0.0051,  ...,  0.0266, -0.0305,  0.0006],\n                      [-0.0150,  0.0252, -0.0225,  ..., -0.0193, -0.0181, -0.0033],\n                      [ 0.0105, -0.0080,  0.0186,  ...,  0.0091,  0.0106,  0.0260],\n                      ...,\n                      [-0.0284, -0.0261, -0.0431,  ...,  0.0043, -0.0100,  0.0036],\n                      [-0.0004, -0.0167, -0.0083,  ...,  0.0138,  0.0196,  0.0035],\n                      [ 0.0213,  0.0193,  0.0238,  ..., -0.0069,  0.0036, -0.0230]],\n                     device='cuda:0')),\n             ('model.model.layers.8.mlp.gate_proj.weight',\n              tensor([[ 0.0274, -0.0052,  0.0216,  ..., -0.0265, -0.0067,  0.0227],\n                      [-0.0032, -0.0056,  0.0056,  ...,  0.0032, -0.0237,  0.0226],\n                      [-0.0171, -0.0079, -0.0188,  ...,  0.0049,  0.0306, -0.0248],\n                      ...,\n                      [ 0.0179,  0.0165, -0.0436,  ...,  0.0066, -0.0029, -0.0291],\n                      [-0.0222,  0.0193,  0.0256,  ...,  0.0280,  0.0396, -0.0209],\n                      [-0.0043, -0.0153,  0.0048,  ...,  0.0202,  0.0322,  0.0025]],\n                     device='cuda:0')),\n             ('model.model.layers.8.mlp.up_proj.weight',\n              tensor([[ 0.0133,  0.0096,  0.0210,  ..., -0.0035,  0.0346,  0.0061],\n                      [-0.0048, -0.0007, -0.0088,  ..., -0.0159, -0.0147, -0.0266],\n                      [ 0.0101, -0.0012,  0.0185,  ...,  0.0057,  0.0178,  0.0361],\n                      ...,\n                      [ 0.0300,  0.0302,  0.0146,  ...,  0.0044,  0.0439,  0.0062],\n                      [ 0.0170, -0.0024,  0.0081,  ..., -0.0177, -0.0154,  0.0117],\n                      [-0.0074, -0.0360,  0.0200,  ..., -0.0244,  0.0255, -0.0340]],\n                     device='cuda:0')),\n             ('model.model.layers.8.mlp.down_proj.weight',\n              tensor([[ 0.0558,  0.0044,  0.0065,  ..., -0.0127, -0.0437,  0.0023],\n                      [ 0.0062,  0.0162, -0.0018,  ..., -0.0169, -0.0232,  0.0140],\n                      [ 0.0074, -0.0181, -0.0162,  ...,  0.0261,  0.0081, -0.0033],\n                      ...,\n                      [-0.0057,  0.0101, -0.0181,  ...,  0.0271,  0.0352, -0.0144],\n                      [-0.0064, -0.0064,  0.0135,  ...,  0.0084,  0.0076, -0.0167],\n                      [ 0.0098,  0.0249, -0.0015,  ..., -0.0149,  0.0378, -0.0037]],\n                     device='cuda:0')),\n             ('model.model.layers.8.input_layernorm.weight',\n              tensor([0.2317, 0.2181, 0.2246,  ..., 0.2358, 0.2108, 0.2364], device='cuda:0')),\n             ('model.model.layers.8.post_attention_layernorm.weight',\n              tensor([0.2653, 0.2585, 0.2793,  ..., 0.2740, 0.2736, 0.2733], device='cuda:0')),\n             ('model.model.layers.9.self_attn.q_proj.weight',\n              tensor([[-0.0032, -0.0285,  0.0297,  ...,  0.0046, -0.0107,  0.0208],\n                      [-0.0343,  0.0524, -0.0062,  ...,  0.0632, -0.0347,  0.0135],\n                      [ 0.0480, -0.0155,  0.0639,  ...,  0.0206, -0.0691,  0.0468],\n                      ...,\n                      [ 0.0203,  0.0295,  0.0367,  ..., -0.0062,  0.0057,  0.0087],\n                      [-0.0205, -0.0294, -0.0190,  ..., -0.0060, -0.0048, -0.0183],\n                      [ 0.0017, -0.0235, -0.0350,  ..., -0.0148, -0.0414,  0.0043]],\n                     device='cuda:0')),\n             ('model.model.layers.9.self_attn.k_proj.weight',\n              tensor([[ 0.0563,  0.0657,  0.0410,  ..., -0.0675,  0.0744,  0.1111],\n                      [-0.0438, -0.0855,  0.0681,  ...,  0.0363, -0.0324, -0.0798],\n                      [ 0.0627,  0.0453, -0.0122,  ...,  0.0025,  0.0108,  0.0710],\n                      ...,\n                      [-0.0313, -0.0251,  0.0056,  ...,  0.0004,  0.0004, -0.0030],\n                      [ 0.0318,  0.0197,  0.0014,  ..., -0.0094,  0.0048,  0.0028],\n                      [ 0.0035,  0.0056, -0.0254,  ...,  0.0050,  0.0712,  0.0076]],\n                     device='cuda:0')),\n             ('model.model.layers.9.self_attn.v_proj.weight',\n              tensor([[-0.0440, -0.0081, -0.0102,  ..., -0.0172, -0.0120, -0.0366],\n                      [-0.0164, -0.0187,  0.0146,  ...,  0.0045, -0.0071, -0.0123],\n                      [ 0.0031, -0.0104,  0.0166,  ...,  0.0157, -0.0056, -0.0201],\n                      ...,\n                      [ 0.0154, -0.0299,  0.0254,  ..., -0.0098, -0.0129, -0.0157],\n                      [-0.0499, -0.0080,  0.0251,  ..., -0.0006,  0.0085, -0.0088],\n                      [-0.0389, -0.0143,  0.0073,  ...,  0.0045,  0.0062, -0.0232]],\n                     device='cuda:0')),\n             ('model.model.layers.9.self_attn.o_proj.weight',\n              tensor([[ 0.0198, -0.0142, -0.0357,  ..., -0.0110,  0.0248,  0.0085],\n                      [-0.0130, -0.0003,  0.0144,  ...,  0.0262, -0.0154,  0.0136],\n                      [ 0.0148, -0.0036,  0.0181,  ..., -0.0006,  0.0031,  0.0029],\n                      ...,\n                      [ 0.0112, -0.0081, -0.0447,  ..., -0.0279,  0.0067, -0.0009],\n                      [ 0.0128, -0.0145, -0.0321,  ..., -0.0158,  0.0119,  0.0083],\n                      [ 0.0127,  0.0310,  0.0227,  ...,  0.0174,  0.0066,  0.0097]],\n                     device='cuda:0')),\n             ('model.model.layers.9.mlp.gate_proj.weight',\n              tensor([[-0.0009,  0.0011,  0.0228,  ...,  0.0066,  0.0221, -0.0182],\n                      [-0.0210,  0.0204, -0.0335,  ...,  0.0198,  0.0079, -0.0348],\n                      [ 0.0001,  0.0103, -0.0314,  ...,  0.0010, -0.0185, -0.0316],\n                      ...,\n                      [-0.0121,  0.0266,  0.0473,  ...,  0.0289, -0.0239,  0.0151],\n                      [ 0.0006,  0.0062, -0.0218,  ..., -0.0329, -0.0311,  0.0050],\n                      [-0.0093,  0.0298, -0.0091,  ..., -0.0050,  0.0093,  0.0408]],\n                     device='cuda:0')),\n             ('model.model.layers.9.mlp.up_proj.weight',\n              tensor([[ 0.0009, -0.0168, -0.0060,  ..., -0.0061,  0.0090,  0.0167],\n                      [-0.0079, -0.0153,  0.0043,  ..., -0.0089, -0.0115, -0.0010],\n                      [-0.0083,  0.0114, -0.0321,  ...,  0.0112,  0.0405, -0.0131],\n                      ...,\n                      [-0.0252,  0.0030, -0.0441,  ...,  0.0035,  0.0146,  0.0050],\n                      [-0.0124, -0.0084,  0.0217,  ..., -0.0245, -0.0012,  0.0136],\n                      [-0.0162, -0.0049, -0.0283,  ...,  0.0071,  0.0340, -0.0267]],\n                     device='cuda:0')),\n             ('model.model.layers.9.mlp.down_proj.weight',\n              tensor([[-0.0136,  0.0050, -0.0380,  ..., -0.0270, -0.0069, -0.0104],\n                      [-0.0035,  0.0326, -0.0094,  ..., -0.0194, -0.0275,  0.0058],\n                      [-0.0314, -0.0558, -0.0147,  ..., -0.0044,  0.0427,  0.0110],\n                      ...,\n                      [-0.0186, -0.0020,  0.0075,  ..., -0.0169,  0.0057, -0.0263],\n                      [ 0.0136,  0.0237,  0.0277,  ..., -0.0349,  0.0053,  0.0265],\n                      [ 0.0310,  0.0461, -0.0517,  ..., -0.0528, -0.0178,  0.0210]],\n                     device='cuda:0')),\n             ('model.model.layers.9.input_layernorm.weight',\n              tensor([0.2307, 0.2261, 0.2135,  ..., 0.2387, 0.2030, 0.2202], device='cuda:0')),\n             ('model.model.layers.9.post_attention_layernorm.weight',\n              tensor([0.2664, 0.2710, 0.2792,  ..., 0.2804, 0.2840, 0.2680], device='cuda:0')),\n             ('model.model.layers.10.self_attn.q_proj.weight',\n              tensor([[ 1.3779e-02, -2.4871e-02, -3.9426e-02,  ...,  3.2259e-02,\n                       -1.3729e-02, -5.9047e-02],\n                      [ 5.4956e-02, -1.8758e-02,  5.5763e-02,  ..., -8.5061e-04,\n                        1.0426e-02,  3.4145e-02],\n                      [ 3.1341e-02,  1.3652e-02, -4.7564e-02,  ...,  6.1904e-02,\n                        1.9406e-02, -5.1666e-02],\n                      ...,\n                      [-1.7368e-02, -8.6595e-03, -1.4792e-02,  ..., -3.5940e-02,\n                       -2.5083e-02,  1.9986e-03],\n                      [-2.9957e-03, -2.0789e-02,  2.7932e-02,  ...,  2.4997e-02,\n                        3.8061e-02, -1.6077e-02],\n                      [-2.3160e-02, -3.6008e-02, -1.0673e-05,  ..., -3.7676e-02,\n                       -1.3722e-02, -2.5837e-02]], device='cuda:0')),\n             ('model.model.layers.10.self_attn.k_proj.weight',\n              tensor([[-0.0257, -0.0222,  0.0133,  ..., -0.0447, -0.0356,  0.0459],\n                      [-0.0015, -0.0145,  0.0361,  ..., -0.0076,  0.0179,  0.0244],\n                      [-0.0145,  0.0139, -0.0208,  ..., -0.0093, -0.0123, -0.0343],\n                      ...,\n                      [ 0.0115,  0.0029,  0.0250,  ..., -0.0117,  0.0083, -0.0155],\n                      [-0.0399,  0.0016, -0.0468,  ...,  0.0249,  0.0222, -0.0246],\n                      [ 0.0214,  0.0117,  0.0295,  ...,  0.0109,  0.0104, -0.0098]],\n                     device='cuda:0')),\n             ('model.model.layers.10.self_attn.v_proj.weight',\n              tensor([[-0.0255,  0.0148, -0.0371,  ...,  0.0049, -0.0043, -0.0315],\n                      [-0.0070, -0.0328,  0.0051,  ..., -0.0252, -0.0052,  0.0060],\n                      [ 0.0179,  0.0037, -0.0011,  ..., -0.0109,  0.0090, -0.0081],\n                      ...,\n                      [ 0.0032, -0.0217,  0.0209,  ...,  0.0107,  0.0033,  0.0058],\n                      [ 0.0084, -0.0031, -0.0076,  ...,  0.0111,  0.0058,  0.0093],\n                      [ 0.0226, -0.0065,  0.0168,  ..., -0.0335, -0.0288,  0.0061]],\n                     device='cuda:0')),\n             ('model.model.layers.10.self_attn.o_proj.weight',\n              tensor([[ 0.0022,  0.0079, -0.0077,  ...,  0.0100,  0.0073, -0.0083],\n                      [ 0.0004,  0.0457, -0.0146,  ...,  0.0071,  0.0018,  0.0054],\n                      [ 0.0120,  0.0229,  0.0084,  ..., -0.0102,  0.0222,  0.0117],\n                      ...,\n                      [ 0.0016, -0.0065,  0.0098,  ..., -0.0128,  0.0045,  0.0245],\n                      [ 0.0207, -0.0147, -0.0283,  ...,  0.0008, -0.0211,  0.0219],\n                      [ 0.0221, -0.0236, -0.0113,  ..., -0.0003,  0.0128,  0.0032]],\n                     device='cuda:0')),\n             ('model.model.layers.10.mlp.gate_proj.weight',\n              tensor([[-0.0113, -0.0077, -0.0160,  ..., -0.0312, -0.0201,  0.0034],\n                      [ 0.0043,  0.0162,  0.0158,  ...,  0.0279,  0.0173,  0.0028],\n                      [-0.0017, -0.0007,  0.0194,  ..., -0.0154,  0.0220, -0.0451],\n                      ...,\n                      [ 0.0017, -0.0088, -0.0071,  ..., -0.0169, -0.0511, -0.0036],\n                      [ 0.0120,  0.0340, -0.0325,  ...,  0.0003,  0.0273,  0.0020],\n                      [-0.0012,  0.0404,  0.0072,  ..., -0.0370,  0.0042, -0.0013]],\n                     device='cuda:0')),\n             ('model.model.layers.10.mlp.up_proj.weight',\n              tensor([[-0.0069, -0.0132, -0.0055,  ...,  0.0078,  0.0073,  0.0086],\n                      [ 0.0018,  0.0008, -0.0187,  ..., -0.0133,  0.0047, -0.0511],\n                      [ 0.0022,  0.0030, -0.0011,  ..., -0.0113,  0.0134, -0.0029],\n                      ...,\n                      [ 0.0118, -0.0045, -0.0017,  ...,  0.0315,  0.0206,  0.0118],\n                      [ 0.0181, -0.0294,  0.0132,  ..., -0.0015, -0.0216, -0.0262],\n                      [-0.0209,  0.0220, -0.0295,  ...,  0.0111,  0.0001, -0.0235]],\n                     device='cuda:0')),\n             ('model.model.layers.10.mlp.down_proj.weight',\n              tensor([[-0.0189,  0.0108,  0.0032,  ...,  0.0002,  0.0297, -0.0069],\n                      [-0.0225, -0.0083, -0.0296,  ...,  0.0301,  0.0014,  0.0107],\n                      [ 0.0201, -0.0344, -0.0469,  ..., -0.0185, -0.0398, -0.0086],\n                      ...,\n                      [ 0.0051,  0.0090,  0.0523,  ..., -0.0102,  0.0096, -0.0328],\n                      [ 0.0109, -0.0051,  0.0171,  ...,  0.0512,  0.0138, -0.0175],\n                      [-0.0393,  0.0325,  0.0071,  ...,  0.0115, -0.0534, -0.0150]],\n                     device='cuda:0')),\n             ('model.model.layers.10.input_layernorm.weight',\n              tensor([0.2391, 0.2364, 0.1925,  ..., 0.2429, 0.2051, 0.2181], device='cuda:0')),\n             ('model.model.layers.10.post_attention_layernorm.weight',\n              tensor([0.2686, 0.2715, 0.2821,  ..., 0.2707, 0.2719, 0.2756], device='cuda:0')),\n             ('model.model.layers.11.self_attn.q_proj.weight',\n              tensor([[ 0.0759, -0.0701,  0.0012,  ...,  0.0786,  0.0205,  0.0693],\n                      [-0.0181, -0.0021,  0.0366,  ...,  0.0025,  0.0030,  0.0244],\n                      [-0.0034,  0.0238,  0.0210,  ..., -0.0089, -0.0484,  0.0368],\n                      ...,\n                      [-0.0503, -0.0276,  0.0121,  ..., -0.0263,  0.0139, -0.0279],\n                      [ 0.0349,  0.0148,  0.0133,  ...,  0.0303,  0.0388,  0.0632],\n                      [ 0.0472, -0.0238,  0.0050,  ..., -0.0249,  0.0174, -0.0168]],\n                     device='cuda:0')),\n             ('model.model.layers.11.self_attn.k_proj.weight',\n              tensor([[-0.0377,  0.0322,  0.0682,  ..., -0.1087,  0.0075, -0.0640],\n                      [-0.0076, -0.0194, -0.0205,  ...,  0.0485,  0.0051,  0.0670],\n                      [-0.0015,  0.0842,  0.0081,  ..., -0.0074,  0.0286,  0.0847],\n                      ...,\n                      [ 0.0124, -0.0090,  0.0422,  ...,  0.0076,  0.0289,  0.0114],\n                      [ 0.0004, -0.0098, -0.0367,  ..., -0.0154, -0.0203, -0.0930],\n                      [-0.0280, -0.0283,  0.0046,  ...,  0.0227,  0.0888,  0.0294]],\n                     device='cuda:0')),\n             ('model.model.layers.11.self_attn.v_proj.weight',\n              tensor([[-0.0032, -0.0042, -0.0401,  ...,  0.0534, -0.0069,  0.0340],\n                      [-0.0343,  0.0076, -0.0010,  ..., -0.0020,  0.0030,  0.0218],\n                      [-0.0052, -0.0113,  0.0152,  ...,  0.0056,  0.0210, -0.0020],\n                      ...,\n                      [-0.0015,  0.0148, -0.0130,  ..., -0.0017,  0.0232,  0.0001],\n                      [-0.0218, -0.0241, -0.0018,  ..., -0.0043,  0.0098,  0.0040],\n                      [ 0.0156, -0.0033, -0.0084,  ...,  0.0172, -0.0231, -0.0339]],\n                     device='cuda:0')),\n             ('model.model.layers.11.self_attn.o_proj.weight',\n              tensor([[ 0.0307,  0.0342, -0.0071,  ...,  0.0036,  0.0215, -0.0244],\n                      [ 0.0204,  0.0056,  0.0338,  ...,  0.0236, -0.0053,  0.0103],\n                      [ 0.0268, -0.0073, -0.0079,  ...,  0.0049,  0.0163,  0.0069],\n                      ...,\n                      [-0.0181, -0.0124,  0.0027,  ...,  0.0185,  0.0185, -0.0031],\n                      [ 0.0225, -0.0268,  0.0141,  ..., -0.0123,  0.0070, -0.0045],\n                      [-0.0193, -0.0172, -0.0189,  ...,  0.0068,  0.0148,  0.0367]],\n                     device='cuda:0')),\n             ('model.model.layers.11.mlp.gate_proj.weight',\n              tensor([[-0.0003,  0.0201,  0.0540,  ...,  0.0165, -0.0077,  0.0612],\n                      [-0.0158,  0.0250,  0.0064,  ..., -0.0332,  0.0103, -0.0135],\n                      [-0.0104, -0.0217, -0.0317,  ...,  0.0133, -0.0072, -0.0084],\n                      ...,\n                      [-0.0039,  0.0400,  0.0129,  ...,  0.0070, -0.0130, -0.0282],\n                      [ 0.0088, -0.0257, -0.0014,  ..., -0.0161,  0.0229,  0.0347],\n                      [-0.0004, -0.0141, -0.0111,  ...,  0.0235, -0.0233,  0.0004]],\n                     device='cuda:0')),\n             ('model.model.layers.11.mlp.up_proj.weight',\n              tensor([[-0.0162,  0.0067,  0.0555,  ..., -0.0008,  0.0253,  0.0185],\n                      [-0.0051, -0.0009,  0.0085,  ..., -0.0415, -0.0028,  0.0071],\n                      [ 0.0151,  0.0237, -0.0105,  ..., -0.0053, -0.0469, -0.0356],\n                      ...,\n                      [-0.0131,  0.0106, -0.0035,  ..., -0.0024,  0.0221, -0.0057],\n                      [ 0.0085, -0.0263,  0.0032,  ..., -0.0130, -0.0410, -0.0084],\n                      [ 0.0293,  0.0198,  0.0115,  ..., -0.0165, -0.0045,  0.0159]],\n                     device='cuda:0')),\n             ('model.model.layers.11.mlp.down_proj.weight',\n              tensor([[ 0.0097,  0.0053,  0.0042,  ..., -0.0351,  0.0292, -0.0141],\n                      [ 0.0224, -0.0172,  0.0136,  ...,  0.0015, -0.0066,  0.0126],\n                      [ 0.0362,  0.0090,  0.0064,  ...,  0.0197,  0.0147,  0.0075],\n                      ...,\n                      [-0.0091,  0.0096,  0.0516,  ...,  0.0041, -0.0008, -0.0303],\n                      [-0.0446,  0.0073,  0.0193,  ..., -0.0183, -0.0300,  0.0098],\n                      [-0.0404, -0.0130, -0.0195,  ...,  0.0130,  0.0024, -0.0179]],\n                     device='cuda:0')),\n             ('model.model.layers.11.input_layernorm.weight',\n              tensor([0.2490, 0.2588, 0.2066,  ..., 0.2492, 0.2087, 0.2192], device='cuda:0')),\n             ('model.model.layers.11.post_attention_layernorm.weight',\n              tensor([0.2746, 0.2780, 0.2812,  ..., 0.2773, 0.2792, 0.2854], device='cuda:0')),\n             ('model.model.layers.12.self_attn.q_proj.weight',\n              tensor([[ 0.0339,  0.0146, -0.0346,  ..., -0.0267,  0.0130, -0.0079],\n                      [ 0.0354, -0.0433, -0.0090,  ..., -0.0483, -0.0088,  0.0193],\n                      [-0.0048, -0.0216, -0.0061,  ...,  0.0093, -0.0410,  0.0016],\n                      ...,\n                      [ 0.0237,  0.0333,  0.0166,  ...,  0.0262, -0.0251,  0.0066],\n                      [ 0.0283,  0.0413,  0.0325,  ...,  0.0095, -0.0572, -0.0104],\n                      [-0.0234,  0.0060, -0.0593,  ..., -0.0538, -0.0127, -0.0214]],\n                     device='cuda:0')),\n             ('model.model.layers.12.self_attn.k_proj.weight',\n              tensor([[-0.0563, -0.0702, -0.0122,  ...,  0.0835, -0.0402, -0.0558],\n                      [-0.0551, -0.0121,  0.0219,  ...,  0.0535,  0.0209, -0.0692],\n                      [-0.1582, -0.0206, -0.0326,  ...,  0.0083,  0.0010, -0.0572],\n                      ...,\n                      [-0.0666, -0.0061, -0.0180,  ...,  0.0131, -0.0312, -0.0372],\n                      [-0.0622,  0.0295, -0.0319,  ...,  0.0375, -0.0109, -0.0142],\n                      [-0.0203, -0.0106, -0.1147,  ...,  0.0236,  0.0390,  0.0385]],\n                     device='cuda:0')),\n             ('model.model.layers.12.self_attn.v_proj.weight',\n              tensor([[ 0.0153, -0.0204, -0.0411,  ...,  0.0176, -0.0320,  0.0119],\n                      [ 0.0184,  0.0035, -0.0294,  ...,  0.0074,  0.0241, -0.0170],\n                      [ 0.0018, -0.0067,  0.0135,  ..., -0.0021,  0.0228, -0.0063],\n                      ...,\n                      [-0.0169, -0.0033, -0.0119,  ...,  0.0052, -0.0152,  0.0050],\n                      [-0.0081, -0.0114, -0.0100,  ...,  0.0179,  0.0044,  0.0146],\n                      [ 0.0355,  0.0030, -0.0177,  ...,  0.0245, -0.0056,  0.0227]],\n                     device='cuda:0')),\n             ('model.model.layers.12.self_attn.o_proj.weight',\n              tensor([[ 0.0076, -0.0303,  0.0078,  ...,  0.0138,  0.0024, -0.0023],\n                      [ 0.0162, -0.0018, -0.0100,  ...,  0.0235,  0.0108, -0.0035],\n                      [ 0.0247,  0.0197, -0.0292,  ...,  0.0307,  0.0016, -0.0255],\n                      ...,\n                      [-0.0203,  0.0015, -0.0048,  ..., -0.0229,  0.0055, -0.0158],\n                      [ 0.0239,  0.0128, -0.0267,  ..., -0.0086,  0.0029,  0.0071],\n                      [-0.0143,  0.0373, -0.0028,  ...,  0.0143, -0.0086, -0.0133]],\n                     device='cuda:0')),\n             ('model.model.layers.12.mlp.gate_proj.weight',\n              tensor([[ 0.0202, -0.0384,  0.0080,  ..., -0.0418,  0.0389,  0.0047],\n                      [-0.0171,  0.0031, -0.0463,  ...,  0.0121, -0.0086,  0.0111],\n                      [-0.0059,  0.0520,  0.0094,  ..., -0.0111, -0.0158,  0.0533],\n                      ...,\n                      [-0.0267, -0.0038,  0.0529,  ...,  0.0092, -0.0102, -0.0408],\n                      [-0.0212, -0.0189,  0.0269,  ..., -0.0059, -0.0223, -0.0216],\n                      [-0.0207, -0.0178,  0.0196,  ...,  0.0061, -0.0530, -0.0122]],\n                     device='cuda:0')),\n             ('model.model.layers.12.mlp.up_proj.weight',\n              tensor([[-2.6065e-02, -7.5305e-03,  2.1050e-03,  ..., -2.1373e-02,\n                        1.1142e-02,  4.0735e-04],\n                      [ 1.6277e-02, -1.4433e-02,  2.4229e-02,  ..., -1.0654e-02,\n                        5.9083e-02,  2.9351e-03],\n                      [ 2.7537e-02,  3.4338e-02, -2.9185e-02,  ...,  1.2668e-02,\n                        8.2534e-03, -8.8228e-03],\n                      ...,\n                      [ 4.3734e-03,  2.2658e-02,  3.8959e-02,  ...,  4.6070e-03,\n                       -6.5731e-03,  3.4199e-02],\n                      [-1.6607e-02, -1.1904e-02, -1.6986e-02,  ...,  1.4906e-02,\n                        1.0338e-02, -2.6101e-02],\n                      [-1.8793e-02, -1.0513e-02,  7.2685e-03,  ...,  1.7416e-05,\n                       -1.8178e-02, -2.1599e-02]], device='cuda:0')),\n             ('model.model.layers.12.mlp.down_proj.weight',\n              tensor([[-0.0219, -0.0038, -0.0178,  ...,  0.0024,  0.0473, -0.0195],\n                      [ 0.0032,  0.0240,  0.0547,  ..., -0.0213,  0.0046,  0.0273],\n                      [-0.0066,  0.0322, -0.0002,  ..., -0.0127,  0.0022,  0.0153],\n                      ...,\n                      [ 0.0103,  0.0098, -0.0067,  ...,  0.0008,  0.0215, -0.0223],\n                      [-0.0249, -0.0172, -0.0276,  ...,  0.0070, -0.0163, -0.0128],\n                      [ 0.0091,  0.0024, -0.0049,  ..., -0.0080, -0.0117, -0.0460]],\n                     device='cuda:0')),\n             ('model.model.layers.12.input_layernorm.weight',\n              tensor([0.2331, 0.2224, 0.1972,  ..., 0.2277, 0.2050, 0.2124], device='cuda:0')),\n             ('model.model.layers.12.post_attention_layernorm.weight',\n              tensor([0.2830, 0.2703, 0.2900,  ..., 0.2832, 0.2810, 0.2867], device='cuda:0')),\n             ('model.model.layers.13.self_attn.q_proj.weight',\n              tensor([[ 0.0144, -0.0027,  0.0153,  ..., -0.0033, -0.0306,  0.0090],\n                      [ 0.0049, -0.0021, -0.0145,  ...,  0.0205, -0.0183,  0.0448],\n                      [ 0.0451, -0.0020,  0.0113,  ..., -0.0007,  0.0022,  0.0354],\n                      ...,\n                      [-0.0097,  0.0407,  0.0340,  ...,  0.0090, -0.0074,  0.0319],\n                      [ 0.0947, -0.0421, -0.0027,  ...,  0.0265, -0.0282,  0.0104],\n                      [-0.0428, -0.0346, -0.0169,  ..., -0.0024, -0.0242, -0.0066]],\n                     device='cuda:0')),\n             ('model.model.layers.13.self_attn.k_proj.weight',\n              tensor([[ 0.0218,  0.0491,  0.0222,  ...,  0.0036,  0.0042,  0.0847],\n                      [-0.0111, -0.0733, -0.0342,  ...,  0.0725, -0.0392, -0.0733],\n                      [ 0.0854,  0.0248, -0.0176,  ..., -0.0698,  0.0531,  0.0480],\n                      ...,\n                      [-0.0150, -0.0924, -0.0706,  ...,  0.0636, -0.0028, -0.0607],\n                      [-0.0238,  0.0296, -0.0450,  ..., -0.0589,  0.0107,  0.0635],\n                      [ 0.0476,  0.1030,  0.0068,  ..., -0.0451,  0.0148,  0.0612]],\n                     device='cuda:0')),\n             ('model.model.layers.13.self_attn.v_proj.weight',\n              tensor([[ 0.0072,  0.0720,  0.0146,  ...,  0.0485,  0.0043, -0.0071],\n                      [-0.0235, -0.0054, -0.0037,  ..., -0.0015, -0.0032, -0.0029],\n                      [ 0.0098,  0.0152, -0.0014,  ..., -0.0137,  0.0356,  0.0015],\n                      ...,\n                      [ 0.0145, -0.0076,  0.0124,  ..., -0.0291, -0.0037, -0.0144],\n                      [ 0.0310,  0.0008, -0.0166,  ..., -0.0121,  0.0213, -0.0183],\n                      [ 0.0138, -0.0133,  0.0196,  ...,  0.0072, -0.0075,  0.0059]],\n                     device='cuda:0')),\n             ('model.model.layers.13.self_attn.o_proj.weight',\n              tensor([[-0.0159, -0.0022,  0.0066,  ..., -0.0016, -0.0376, -0.0149],\n                      [-0.0514, -0.0173, -0.0172,  ..., -0.0128,  0.0086,  0.0301],\n                      [-0.0329,  0.0221,  0.0288,  ...,  0.0035,  0.0079, -0.0036],\n                      ...,\n                      [-0.0388,  0.0062, -0.0062,  ...,  0.0135,  0.0148, -0.0162],\n                      [ 0.0029, -0.0015,  0.0134,  ...,  0.0095,  0.0037,  0.0180],\n                      [-0.0127,  0.0200,  0.0072,  ...,  0.0107,  0.0110, -0.0073]],\n                     device='cuda:0')),\n             ('model.model.layers.13.mlp.gate_proj.weight',\n              tensor([[-5.6029e-03,  1.4540e-02,  1.8924e-02,  ...,  8.1926e-03,\n                       -1.5206e-03, -6.3704e-03],\n                      [ 8.3923e-03, -1.4306e-02, -1.9004e-03,  ...,  5.1996e-05,\n                        1.6475e-02,  4.0802e-02],\n                      [-6.1089e-03,  4.8135e-03,  5.4726e-03,  ...,  8.7781e-03,\n                        5.0732e-02, -3.0608e-04],\n                      ...,\n                      [ 1.2730e-02, -1.9947e-03, -1.2863e-03,  ..., -2.1113e-02,\n                       -3.5363e-02,  9.7556e-03],\n                      [-4.2700e-03,  4.0119e-02, -4.1124e-02,  ..., -5.5851e-03,\n                        2.4403e-02, -5.7400e-03],\n                      [-3.8137e-02, -8.0285e-03,  3.0547e-03,  ..., -2.8420e-02,\n                        2.4365e-02, -8.6492e-03]], device='cuda:0')),\n             ('model.model.layers.13.mlp.up_proj.weight',\n              tensor([[ 0.0071,  0.0332, -0.0183,  ..., -0.0101,  0.0066,  0.0156],\n                      [ 0.0271, -0.0360,  0.0008,  ..., -0.0271,  0.0140, -0.0088],\n                      [ 0.0192,  0.0096,  0.0245,  ..., -0.0295, -0.0075,  0.0449],\n                      ...,\n                      [ 0.0127, -0.0030,  0.0141,  ..., -0.0054,  0.0022,  0.0110],\n                      [-0.0160,  0.0430, -0.0136,  ..., -0.0465, -0.0006,  0.0033],\n                      [-0.0163,  0.0021, -0.0036,  ...,  0.0130,  0.0544,  0.0024]],\n                     device='cuda:0')),\n             ('model.model.layers.13.mlp.down_proj.weight',\n              tensor([[ 0.0026,  0.0589, -0.0047,  ..., -0.0397,  0.0040,  0.0086],\n                      [-0.0236,  0.0043,  0.0073,  ..., -0.0011,  0.0008,  0.0338],\n                      [ 0.0283,  0.0267, -0.0359,  ..., -0.0068,  0.0123,  0.0111],\n                      ...,\n                      [ 0.0412,  0.0135,  0.0154,  ..., -0.0170,  0.0031,  0.0020],\n                      [-0.0178,  0.0157,  0.0001,  ..., -0.0148, -0.0246, -0.0178],\n                      [ 0.0312, -0.0181, -0.0168,  ..., -0.0113,  0.0066, -0.0331]],\n                     device='cuda:0')),\n             ('model.model.layers.13.input_layernorm.weight',\n              tensor([0.2174, 0.2288, 0.1977,  ..., 0.1999, 0.1936, 0.1957], device='cuda:0')),\n             ('model.model.layers.13.post_attention_layernorm.weight',\n              tensor([0.2819, 0.2776, 0.2826,  ..., 0.2816, 0.2721, 0.2837], device='cuda:0')),\n             ('model.model.layers.14.self_attn.q_proj.weight',\n              tensor([[ 0.0031, -0.0231, -0.0102,  ...,  0.0337, -0.0113, -0.0098],\n                      [ 0.0006,  0.0282,  0.0444,  ...,  0.0141, -0.0491, -0.0451],\n                      [ 0.0101,  0.0067, -0.0446,  ...,  0.0162,  0.0289, -0.0037],\n                      ...,\n                      [-0.0546, -0.0322,  0.0218,  ..., -0.0403,  0.0202,  0.0066],\n                      [-0.0167,  0.0115,  0.0048,  ...,  0.0117, -0.0164, -0.0341],\n                      [ 0.0309, -0.0174, -0.0164,  ...,  0.0031,  0.0046,  0.0005]],\n                     device='cuda:0')),\n             ('model.model.layers.14.self_attn.k_proj.weight',\n              tensor([[-0.0281, -0.0338,  0.0114,  ..., -0.0289, -0.0156, -0.0201],\n                      [-0.0207,  0.0679,  0.0222,  ..., -0.0201,  0.0487,  0.0102],\n                      [-0.0090,  0.0153, -0.0724,  ...,  0.1479,  0.0257,  0.0292],\n                      ...,\n                      [-0.0027,  0.0143,  0.0373,  ...,  0.0320,  0.0135,  0.0283],\n                      [-0.0758, -0.0320, -0.0246,  ...,  0.0245,  0.0168, -0.0206],\n                      [ 0.0287,  0.0322, -0.0241,  ..., -0.0128, -0.0187, -0.0405]],\n                     device='cuda:0')),\n             ('model.model.layers.14.self_attn.v_proj.weight',\n              tensor([[ 0.0180, -0.0493, -0.0223,  ..., -0.0083, -0.0044, -0.0039],\n                      [-0.0054,  0.0004,  0.0168,  ...,  0.0103,  0.0040,  0.0105],\n                      [ 0.0208,  0.0065,  0.0099,  ..., -0.0129,  0.0447, -0.0068],\n                      ...,\n                      [ 0.0099,  0.0098,  0.0167,  ...,  0.0032, -0.0221, -0.0075],\n                      [ 0.0020,  0.0208,  0.0155,  ..., -0.0399, -0.0088,  0.0130],\n                      [-0.0160,  0.0186,  0.0247,  ..., -0.0155,  0.0009,  0.0184]],\n                     device='cuda:0')),\n             ('model.model.layers.14.self_attn.o_proj.weight',\n              tensor([[ 0.0016,  0.0253,  0.0020,  ...,  0.0041, -0.0156, -0.0018],\n                      [ 0.0269, -0.0225, -0.0094,  ..., -0.0324,  0.0043, -0.0107],\n                      [ 0.0101, -0.0004, -0.0066,  ..., -0.0065, -0.0023, -0.0165],\n                      ...,\n                      [ 0.0229, -0.0085, -0.0039,  ...,  0.0013,  0.0157,  0.0144],\n                      [ 0.0196,  0.0089, -0.0173,  ...,  0.0248,  0.0052, -0.0177],\n                      [-0.0061, -0.0089, -0.0007,  ...,  0.0196,  0.0017, -0.0134]],\n                     device='cuda:0')),\n             ('model.model.layers.14.mlp.gate_proj.weight',\n              tensor([[ 0.0254, -0.0109,  0.0158,  ..., -0.0131, -0.0234, -0.0471],\n                      [ 0.0231, -0.0415, -0.0140,  ...,  0.0086, -0.0296,  0.0214],\n                      [-0.0437, -0.0070,  0.0230,  ...,  0.0002, -0.0482,  0.0295],\n                      ...,\n                      [-0.0047,  0.0322,  0.0391,  ...,  0.0321, -0.0305, -0.0074],\n                      [ 0.0085,  0.0079,  0.0104,  ...,  0.0222,  0.0011, -0.0228],\n                      [-0.0177,  0.0390,  0.0144,  ..., -0.0163, -0.0100,  0.0226]],\n                     device='cuda:0')),\n             ('model.model.layers.14.mlp.up_proj.weight',\n              tensor([[-0.0043, -0.0113,  0.0109,  ...,  0.0050,  0.0198,  0.0073],\n                      [-0.0145,  0.0109, -0.0091,  ..., -0.0304, -0.0175, -0.0183],\n                      [-0.0276, -0.0141,  0.0208,  ..., -0.0168,  0.0015,  0.0102],\n                      ...,\n                      [-0.0096, -0.0316,  0.0094,  ..., -0.0187, -0.0294,  0.0076],\n                      [-0.0345, -0.0080,  0.0131,  ...,  0.0233, -0.0149,  0.0032],\n                      [ 0.0312,  0.0036,  0.0044,  ...,  0.0015, -0.0085,  0.0025]],\n                     device='cuda:0')),\n             ('model.model.layers.14.mlp.down_proj.weight',\n              tensor([[-0.0216,  0.0155, -0.0140,  ..., -0.0151, -0.0109,  0.0024],\n                      [ 0.0170,  0.0136,  0.0095,  ..., -0.0024,  0.0131, -0.0278],\n                      [-0.0171, -0.0353, -0.0108,  ...,  0.0476, -0.0004,  0.0041],\n                      ...,\n                      [ 0.0103,  0.0325,  0.0173,  ...,  0.0068,  0.0051, -0.0325],\n                      [ 0.0133, -0.0111,  0.0124,  ..., -0.0459,  0.0108,  0.0128],\n                      [ 0.0058,  0.0125, -0.0377,  ...,  0.0223, -0.0023,  0.0092]],\n                     device='cuda:0')),\n             ('model.model.layers.14.input_layernorm.weight',\n              tensor([0.2061, 0.2157, 0.1927,  ..., 0.2097, 0.2008, 0.1895], device='cuda:0')),\n             ('model.model.layers.14.post_attention_layernorm.weight',\n              tensor([0.2922, 0.2801, 0.2782,  ..., 0.2802, 0.2773, 0.2976], device='cuda:0')),\n             ('model.model.layers.15.self_attn.q_proj.weight',\n              tensor([[-0.0112,  0.0194,  0.0293,  ...,  0.0309,  0.0804, -0.0145],\n                      [ 0.0324, -0.0249,  0.0500,  ..., -0.0204, -0.0064,  0.0011],\n                      [-0.0141, -0.0355,  0.0234,  ...,  0.0100, -0.0075, -0.0169],\n                      ...,\n                      [ 0.0340,  0.0165,  0.0069,  ..., -0.0218, -0.0181, -0.0307],\n                      [ 0.0117,  0.0153, -0.0207,  ..., -0.0024, -0.0056, -0.0291],\n                      [ 0.0514, -0.0088, -0.0207,  ...,  0.0436,  0.0069, -0.0135]],\n                     device='cuda:0')),\n             ('model.model.layers.15.self_attn.k_proj.weight',\n              tensor([[ 0.0040,  0.0008,  0.0732,  ...,  0.0309,  0.0320,  0.0306],\n                      [ 0.0622,  0.0159,  0.0666,  ...,  0.0585,  0.0535,  0.0589],\n                      [-0.0326, -0.0429,  0.0163,  ...,  0.0013, -0.0481,  0.0088],\n                      ...,\n                      [ 0.0330,  0.0249, -0.0423,  ..., -0.0180, -0.0037, -0.0352],\n                      [-0.0044, -0.0124,  0.0181,  ...,  0.0154, -0.0065, -0.0046],\n                      [ 0.0041, -0.0220, -0.0660,  ..., -0.0215, -0.0365, -0.0048]],\n                     device='cuda:0')),\n             ('model.model.layers.15.self_attn.v_proj.weight',\n              tensor([[-0.0176, -0.0005,  0.0173,  ..., -0.0037, -0.0190, -0.0150],\n                      [-0.0217,  0.0146, -0.0098,  ..., -0.0091,  0.0173, -0.0035],\n                      [ 0.0090,  0.0097,  0.0216,  ...,  0.0058,  0.0428, -0.0026],\n                      ...,\n                      [-0.0127,  0.0324, -0.0139,  ...,  0.0072,  0.0128,  0.0052],\n                      [-0.0153,  0.0012, -0.0067,  ..., -0.0107,  0.0003,  0.0130],\n                      [-0.0105,  0.0027, -0.0026,  ..., -0.0153,  0.0105,  0.0030]],\n                     device='cuda:0')),\n             ('model.model.layers.15.self_attn.o_proj.weight',\n              tensor([[ 0.0071,  0.0069, -0.0039,  ...,  0.0337,  0.0028,  0.0012],\n                      [ 0.0123,  0.0042, -0.0176,  ..., -0.0375, -0.0050, -0.0201],\n                      [-0.0068, -0.0065,  0.0045,  ..., -0.0034,  0.0157, -0.0258],\n                      ...,\n                      [-0.0058,  0.0048,  0.0158,  ..., -0.0130, -0.0020,  0.0071],\n                      [ 0.0350,  0.0040,  0.0103,  ..., -0.0067,  0.0077,  0.0028],\n                      [-0.0131,  0.0056, -0.0157,  ..., -0.0109, -0.0084, -0.0078]],\n                     device='cuda:0')),\n             ('model.model.layers.15.mlp.gate_proj.weight',\n              tensor([[-0.0201, -0.0380, -0.0313,  ..., -0.0297,  0.0171, -0.0086],\n                      [-0.0120,  0.0155, -0.0096,  ..., -0.0198,  0.0092, -0.0234],\n                      [ 0.0270, -0.0179, -0.0150,  ...,  0.0083,  0.0144, -0.0076],\n                      ...,\n                      [ 0.0482,  0.0427, -0.0491,  ...,  0.0230,  0.0008, -0.0074],\n                      [-0.0170,  0.0071, -0.0140,  ...,  0.0018, -0.0191, -0.0045],\n                      [-0.0104, -0.0318, -0.0629,  ...,  0.0256,  0.0420, -0.0137]],\n                     device='cuda:0')),\n             ('model.model.layers.15.mlp.up_proj.weight',\n              tensor([[-0.0038, -0.0179, -0.0022,  ..., -0.0067, -0.0114, -0.0159],\n                      [ 0.0508,  0.0066, -0.0040,  ...,  0.0059, -0.0412,  0.0201],\n                      [ 0.0012, -0.0100, -0.0235,  ...,  0.0201,  0.0109,  0.0157],\n                      ...,\n                      [-0.0062, -0.0312,  0.0151,  ..., -0.0035, -0.0221, -0.0089],\n                      [ 0.0110,  0.0076, -0.0143,  ..., -0.0073, -0.0296, -0.0121],\n                      [ 0.0265,  0.0136, -0.0009,  ...,  0.0194,  0.0116, -0.0038]],\n                     device='cuda:0')),\n             ('model.model.layers.15.mlp.down_proj.weight',\n              tensor([[-0.0403,  0.0242,  0.0236,  ...,  0.0227,  0.0343,  0.0045],\n                      [ 0.0322,  0.0309,  0.0151,  ...,  0.0175,  0.0379, -0.0222],\n                      [-0.0509,  0.0111,  0.0153,  ..., -0.0142, -0.0542,  0.0027],\n                      ...,\n                      [ 0.0505, -0.0037,  0.0031,  ...,  0.0110,  0.0217,  0.0124],\n                      [-0.0025, -0.0322,  0.0189,  ..., -0.0195,  0.0129, -0.0291],\n                      [ 0.0061, -0.0002,  0.0021,  ...,  0.0276, -0.0085,  0.0196]],\n                     device='cuda:0')),\n             ('model.model.layers.15.input_layernorm.weight',\n              tensor([0.2185, 0.2180, 0.2058,  ..., 0.2032, 0.1917, 0.1927], device='cuda:0')),\n             ('model.model.layers.15.post_attention_layernorm.weight',\n              tensor([0.2835, 0.2799, 0.2852,  ..., 0.2758, 0.2845, 0.2923], device='cuda:0')),\n             ('model.model.layers.16.self_attn.q_proj.weight',\n              tensor([[ 0.0171,  0.0402, -0.0451,  ..., -0.0042,  0.0030,  0.0216],\n                      [-0.0034, -0.0730, -0.0708,  ..., -0.0027, -0.0496,  0.0152],\n                      [-0.0097, -0.0039,  0.0180,  ...,  0.0055, -0.0069,  0.0178],\n                      ...,\n                      [-0.0106,  0.0189, -0.0138,  ...,  0.0278, -0.0061,  0.0360],\n                      [-0.0652, -0.0209,  0.0326,  ...,  0.0063, -0.0298, -0.0216],\n                      [-0.0308, -0.0308,  0.0121,  ...,  0.0128, -0.0120, -0.0074]],\n                     device='cuda:0')),\n             ('model.model.layers.16.self_attn.k_proj.weight',\n              tensor([[ 0.0377, -0.0019,  0.0745,  ...,  0.0116, -0.0478, -0.0140],\n                      [-0.0706, -0.0276,  0.0934,  ...,  0.0825, -0.0376, -0.0398],\n                      [-0.0465,  0.0798,  0.0517,  ...,  0.0259,  0.0452,  0.0090],\n                      ...,\n                      [-0.0318, -0.0264,  0.0642,  ..., -0.0030,  0.0200, -0.0384],\n                      [-0.0229,  0.0144, -0.0107,  ...,  0.0358, -0.0032, -0.0571],\n                      [ 0.0102,  0.0537, -0.0106,  ..., -0.0035,  0.0382,  0.0022]],\n                     device='cuda:0')),\n             ('model.model.layers.16.self_attn.v_proj.weight',\n              tensor([[ 0.0118,  0.0008, -0.0125,  ..., -0.0127, -0.0033, -0.0020],\n                      [ 0.0081,  0.0231,  0.0218,  ..., -0.0135, -0.0012, -0.0044],\n                      [ 0.0195, -0.0370, -0.0062,  ..., -0.0276, -0.0198,  0.0262],\n                      ...,\n                      [ 0.0139,  0.0246,  0.0198,  ...,  0.0070, -0.0109,  0.0078],\n                      [ 0.0062, -0.0225, -0.0027,  ...,  0.0158, -0.0016, -0.0183],\n                      [ 0.0191, -0.0015, -0.0085,  ..., -0.0042, -0.0139, -0.0152]],\n                     device='cuda:0')),\n             ('model.model.layers.16.self_attn.o_proj.weight',\n              tensor([[ 7.5578e-03, -3.5390e-02,  2.3206e-03,  ..., -1.3047e-02,\n                       -8.1450e-03, -2.0279e-03],\n                      [-7.2945e-03, -2.5036e-02,  8.9824e-03,  ..., -7.5505e-03,\n                        9.4848e-03, -9.4822e-03],\n                      [-1.5010e-02, -1.0827e-02, -1.1018e-03,  ..., -1.5364e-02,\n                        8.6675e-03,  2.1669e-02],\n                      ...,\n                      [ 2.0292e-02, -8.3587e-03, -6.5819e-03,  ..., -2.0645e-02,\n                       -4.3592e-03, -2.0485e-02],\n                      [ 5.5092e-03,  4.9045e-03,  3.0904e-02,  ..., -1.6494e-03,\n                       -5.6494e-06,  7.1515e-04],\n                      [ 1.6463e-02, -1.3680e-02, -1.3231e-02,  ...,  1.9620e-02,\n                        3.1996e-02, -6.6813e-04]], device='cuda:0')),\n             ('model.model.layers.16.mlp.gate_proj.weight',\n              tensor([[-0.0132, -0.0184, -0.0048,  ...,  0.0192,  0.0130,  0.0169],\n                      [-0.0042, -0.0395,  0.0216,  ..., -0.0417,  0.0351, -0.0013],\n                      [ 0.0016, -0.0482,  0.0333,  ...,  0.0415,  0.0179,  0.0025],\n                      ...,\n                      [-0.0156,  0.0167, -0.0523,  ..., -0.0305, -0.0117,  0.0185],\n                      [ 0.0039,  0.0026, -0.0148,  ...,  0.0172,  0.0355,  0.0114],\n                      [ 0.0127, -0.0087, -0.0037,  ...,  0.0301,  0.0100,  0.0078]],\n                     device='cuda:0')),\n             ('model.model.layers.16.mlp.up_proj.weight',\n              tensor([[ 0.0090,  0.0214, -0.0163,  ..., -0.0111, -0.0272, -0.0310],\n                      [-0.0407,  0.0014,  0.0211,  ..., -0.0182,  0.0069, -0.0040],\n                      [-0.0038,  0.0014, -0.0100,  ...,  0.0319,  0.0070,  0.0250],\n                      ...,\n                      [-0.0198,  0.0032, -0.0454,  ..., -0.0009, -0.0016, -0.0004],\n                      [-0.0350, -0.0188, -0.0228,  ...,  0.0055,  0.0131, -0.0218],\n                      [ 0.0321, -0.0195,  0.0100,  ...,  0.0430, -0.0244, -0.0217]],\n                     device='cuda:0')),\n             ('model.model.layers.16.mlp.down_proj.weight',\n              tensor([[ 0.0208, -0.0340, -0.0245,  ...,  0.0171,  0.0274,  0.0228],\n                      [-0.0004, -0.0222, -0.0007,  ..., -0.0116, -0.0033, -0.0276],\n                      [ 0.0178,  0.0127, -0.0269,  ...,  0.0006, -0.0096,  0.0003],\n                      ...,\n                      [-0.0165,  0.0200, -0.0085,  ..., -0.0312,  0.0070,  0.0196],\n                      [ 0.0002, -0.0059, -0.0277,  ..., -0.0196, -0.0060,  0.0243],\n                      [ 0.0221, -0.0027,  0.0261,  ...,  0.0159, -0.0050, -0.0025]],\n                     device='cuda:0')),\n             ('model.model.layers.16.input_layernorm.weight',\n              tensor([0.2093, 0.2023, 0.1963,  ..., 0.2213, 0.1926, 0.1858], device='cuda:0')),\n             ('model.model.layers.16.post_attention_layernorm.weight',\n              tensor([0.2866, 0.2995, 0.2931,  ..., 0.2894, 0.2858, 0.2926], device='cuda:0')),\n             ('model.model.layers.17.self_attn.q_proj.weight',\n              tensor([[ 0.0531, -0.0333, -0.0530,  ..., -0.0393, -0.0086,  0.0080],\n                      [-0.0455,  0.0168,  0.0125,  ...,  0.0051,  0.0315, -0.0046],\n                      [ 0.0863, -0.0359,  0.0130,  ..., -0.0208, -0.0243,  0.0230],\n                      ...,\n                      [-0.0201,  0.0183,  0.0228,  ..., -0.0416, -0.0123,  0.0810],\n                      [ 0.0449,  0.0524, -0.0233,  ..., -0.0873,  0.0051,  0.0211],\n                      [-0.0260, -0.0432, -0.0085,  ...,  0.0209,  0.0056,  0.0144]],\n                     device='cuda:0')),\n             ('model.model.layers.17.self_attn.k_proj.weight',\n              tensor([[-0.0360, -0.0076,  0.0343,  ...,  0.0186, -0.0128,  0.0003],\n                      [ 0.0469,  0.0136, -0.0033,  ..., -0.0121,  0.0155, -0.0065],\n                      [-0.0406, -0.0689, -0.0098,  ..., -0.0273, -0.0338,  0.0281],\n                      ...,\n                      [-0.0219,  0.0024,  0.0393,  ...,  0.0144, -0.0152, -0.0276],\n                      [ 0.0092, -0.0285, -0.0257,  ..., -0.0015,  0.0300, -0.0179],\n                      [ 0.0203,  0.0038,  0.0298,  ...,  0.0048, -0.0077,  0.0445]],\n                     device='cuda:0')),\n             ('model.model.layers.17.self_attn.v_proj.weight',\n              tensor([[-0.0065,  0.0186,  0.0186,  ...,  0.0058, -0.0116,  0.0236],\n                      [-0.0212, -0.0003,  0.0072,  ...,  0.0049, -0.0059,  0.0062],\n                      [-0.0120, -0.0009, -0.0186,  ...,  0.0447,  0.0184,  0.0103],\n                      ...,\n                      [ 0.0005,  0.0075,  0.0162,  ...,  0.0157,  0.0580,  0.0371],\n                      [ 0.0094, -0.0259, -0.0049,  ...,  0.0159,  0.0082, -0.0114],\n                      [-0.0222, -0.0091,  0.0178,  ..., -0.0042,  0.0235, -0.0191]],\n                     device='cuda:0')),\n             ('model.model.layers.17.self_attn.o_proj.weight',\n              tensor([[ 0.0235, -0.0088,  0.0060,  ..., -0.0147, -0.0257, -0.0047],\n                      [-0.0019, -0.0269, -0.0003,  ..., -0.0022,  0.0120, -0.0111],\n                      [-0.0340,  0.0005,  0.0280,  ..., -0.0138, -0.0133, -0.0286],\n                      ...,\n                      [-0.0009, -0.0025, -0.0211,  ...,  0.0009, -0.0144, -0.0310],\n                      [ 0.0166, -0.0003,  0.0067,  ..., -0.0188,  0.0402, -0.0132],\n                      [-0.0384,  0.0119, -0.0071,  ..., -0.0176,  0.0129,  0.0016]],\n                     device='cuda:0')),\n             ('model.model.layers.17.mlp.gate_proj.weight',\n              tensor([[ 0.0010,  0.0024, -0.0056,  ..., -0.0015, -0.0278,  0.0264],\n                      [ 0.0041, -0.0071,  0.0114,  ...,  0.0132,  0.0068,  0.0168],\n                      [ 0.0079,  0.0119, -0.0185,  ..., -0.0151,  0.0282, -0.0174],\n                      ...,\n                      [ 0.0236,  0.0100, -0.0023,  ..., -0.0239,  0.0212,  0.0104],\n                      [ 0.0040,  0.0084, -0.0260,  ..., -0.0036,  0.0046,  0.0468],\n                      [ 0.0176, -0.0024, -0.0130,  ..., -0.0132, -0.0238,  0.0193]],\n                     device='cuda:0')),\n             ('model.model.layers.17.mlp.up_proj.weight',\n              tensor([[-0.0093,  0.0317,  0.0241,  ..., -0.0307,  0.0087,  0.0230],\n                      [-0.0266, -0.0346,  0.0170,  ..., -0.0137, -0.0365, -0.0335],\n                      [ 0.0122,  0.0113,  0.0144,  ...,  0.0189,  0.0138,  0.0011],\n                      ...,\n                      [ 0.0032,  0.0043,  0.0238,  ..., -0.0070,  0.0131, -0.0159],\n                      [-0.0325,  0.0130, -0.0102,  ..., -0.0178, -0.0108, -0.0207],\n                      [-0.0075,  0.0096, -0.0121,  ...,  0.0231, -0.0290, -0.0161]],\n                     device='cuda:0')),\n             ('model.model.layers.17.mlp.down_proj.weight',\n              tensor([[ 0.0131,  0.0078, -0.0414,  ...,  0.0107, -0.0283,  0.0126],\n                      [ 0.0154,  0.0228, -0.0175,  ...,  0.0029,  0.0051,  0.0156],\n                      [-0.0035, -0.0022,  0.0029,  ..., -0.0257, -0.0015, -0.0054],\n                      ...,\n                      [ 0.0139,  0.0353,  0.0375,  ...,  0.0017,  0.0122,  0.0426],\n                      [ 0.0051,  0.0001, -0.0281,  ..., -0.0069, -0.0019,  0.0122],\n                      [ 0.0274, -0.0053,  0.0470,  ...,  0.0101,  0.0425,  0.0247]],\n                     device='cuda:0')),\n             ('model.model.layers.17.input_layernorm.weight',\n              tensor([0.2250, 0.2036, 0.2150,  ..., 0.2304, 0.1933, 0.1892], device='cuda:0')),\n             ('model.model.layers.17.post_attention_layernorm.weight',\n              tensor([0.2766, 0.3024, 0.2890,  ..., 0.3021, 0.2856, 0.2990], device='cuda:0')),\n             ('model.model.layers.18.self_attn.q_proj.weight',\n              tensor([[ 0.0194, -0.0077, -0.0042,  ..., -0.0014,  0.0167, -0.0281],\n                      [-0.0210,  0.0179, -0.0433,  ...,  0.0062, -0.0740, -0.0074],\n                      [-0.0306, -0.0182, -0.0593,  ..., -0.0622,  0.0057, -0.0054],\n                      ...,\n                      [-0.0159, -0.0296, -0.0006,  ..., -0.0262,  0.0134,  0.0008],\n                      [ 0.0139, -0.0415, -0.0298,  ..., -0.0308,  0.0190,  0.0296],\n                      [ 0.0481,  0.0440,  0.0084,  ...,  0.0023, -0.0272, -0.0147]],\n                     device='cuda:0')),\n             ('model.model.layers.18.self_attn.k_proj.weight',\n              tensor([[-0.1005,  0.0649, -0.0196,  ..., -0.0291, -0.0100,  0.0230],\n                      [-0.0401,  0.0179, -0.0185,  ...,  0.0082,  0.0256,  0.0149],\n                      [ 0.0290,  0.0556,  0.0736,  ...,  0.0307,  0.0639, -0.0136],\n                      ...,\n                      [ 0.0506,  0.0373, -0.0039,  ..., -0.0397,  0.0436, -0.0516],\n                      [ 0.0189, -0.0022,  0.0033,  ...,  0.0788,  0.0388,  0.0061],\n                      [ 0.0379,  0.0145,  0.0070,  ..., -0.0491, -0.0281, -0.0278]],\n                     device='cuda:0')),\n             ('model.model.layers.18.self_attn.v_proj.weight',\n              tensor([[ 0.0259, -0.0003,  0.0196,  ...,  0.0110,  0.0043,  0.0249],\n                      [-0.0030, -0.0044,  0.0073,  ...,  0.0051,  0.0266, -0.0172],\n                      [ 0.0131, -0.0103,  0.0020,  ..., -0.0184, -0.0001, -0.0171],\n                      ...,\n                      [-0.0019,  0.0020, -0.0142,  ...,  0.0205,  0.0025,  0.0052],\n                      [-0.0064, -0.0071,  0.0275,  ...,  0.0180,  0.0188,  0.0034],\n                      [ 0.0020,  0.0121, -0.0163,  ...,  0.0067, -0.0053,  0.0145]],\n                     device='cuda:0')),\n             ('model.model.layers.18.self_attn.o_proj.weight',\n              tensor([[ 5.5451e-03, -2.0649e-02, -1.4031e-02,  ..., -1.0845e-02,\n                        1.5018e-03,  1.0539e-02],\n                      [ 1.0183e-02, -2.6409e-02,  1.8064e-02,  ..., -1.3675e-02,\n                        5.6595e-04,  3.9386e-06],\n                      [ 1.5332e-02,  4.9094e-03, -3.7614e-04,  ..., -1.1659e-03,\n                       -1.2435e-02,  1.1779e-02],\n                      ...,\n                      [-3.7980e-03,  2.9583e-03, -2.9732e-03,  ..., -2.4539e-02,\n                       -9.5434e-03,  1.0327e-02],\n                      [ 1.7552e-02, -3.2971e-02,  5.4630e-03,  ...,  1.8200e-02,\n                       -2.2314e-02,  2.0920e-02],\n                      [-1.5190e-02, -2.3431e-02,  2.6359e-02,  ...,  1.2697e-02,\n                       -3.6752e-03, -2.2828e-02]], device='cuda:0')),\n             ('model.model.layers.18.mlp.gate_proj.weight',\n              tensor([[-0.0012, -0.0118, -0.0179,  ...,  0.0066,  0.0032,  0.0062],\n                      [ 0.0505,  0.0034,  0.0164,  ..., -0.0143,  0.0325, -0.0127],\n                      [-0.0182,  0.0240,  0.0459,  ...,  0.0215,  0.0237,  0.0008],\n                      ...,\n                      [ 0.0124, -0.0006,  0.0025,  ...,  0.0177,  0.0031,  0.0124],\n                      [ 0.0033,  0.0343, -0.0124,  ...,  0.0053, -0.0430,  0.0296],\n                      [ 0.0170,  0.0025, -0.0279,  ...,  0.0349,  0.0173,  0.0244]],\n                     device='cuda:0')),\n             ('model.model.layers.18.mlp.up_proj.weight',\n              tensor([[ 0.0274,  0.0141, -0.0506,  ..., -0.0243, -0.0205, -0.0179],\n                      [-0.0619, -0.0163,  0.0059,  ..., -0.0186,  0.0070,  0.0047],\n                      [-0.0241, -0.0280, -0.0359,  ...,  0.0033,  0.0197, -0.0139],\n                      ...,\n                      [ 0.0130,  0.0044, -0.0357,  ..., -0.0236, -0.0050,  0.0149],\n                      [-0.0184,  0.0139, -0.0242,  ...,  0.0243, -0.0340, -0.0115],\n                      [-0.0026,  0.0079, -0.0326,  ...,  0.0068,  0.0004,  0.0077]],\n                     device='cuda:0')),\n             ('model.model.layers.18.mlp.down_proj.weight',\n              tensor([[ 0.0237,  0.0040,  0.0415,  ..., -0.0092, -0.0181, -0.0081],\n                      [-0.0163,  0.0025, -0.0196,  ...,  0.0472,  0.0042, -0.0399],\n                      [-0.0232, -0.0073, -0.0378,  ..., -0.0022, -0.0148,  0.0014],\n                      ...,\n                      [ 0.0175, -0.0053, -0.0145,  ..., -0.0235, -0.0262, -0.0148],\n                      [ 0.0325, -0.0179,  0.0041,  ..., -0.0118,  0.0253,  0.0149],\n                      [-0.0247,  0.0219, -0.0093,  ..., -0.0186,  0.0058, -0.0142]],\n                     device='cuda:0')),\n             ('model.model.layers.18.input_layernorm.weight',\n              tensor([0.2277, 0.2142, 0.2183,  ..., 0.2317, 0.1976, 0.2158], device='cuda:0')),\n             ('model.model.layers.18.post_attention_layernorm.weight',\n              tensor([0.3035, 0.3003, 0.2943,  ..., 0.2930, 0.3062, 0.2998], device='cuda:0')),\n             ('model.model.layers.19.self_attn.q_proj.weight',\n              tensor([[ 0.0199,  0.0106,  0.0299,  ...,  0.0474,  0.0470,  0.0478],\n                      [ 0.0029, -0.0248, -0.0646,  ...,  0.0053, -0.0237,  0.0128],\n                      [-0.0387,  0.0285,  0.0043,  ...,  0.0119,  0.0082, -0.0198],\n                      ...,\n                      [-0.0327,  0.0135,  0.0005,  ...,  0.0057,  0.0035,  0.0490],\n                      [ 0.0203, -0.0419, -0.0037,  ...,  0.0382, -0.0011, -0.0098],\n                      [ 0.0152,  0.0107, -0.0088,  ..., -0.0432,  0.0094, -0.0067]],\n                     device='cuda:0')),\n             ('model.model.layers.19.self_attn.k_proj.weight',\n              tensor([[-0.0470, -0.0250, -0.0070,  ...,  0.0660, -0.0421, -0.0276],\n                      [-0.0363, -0.0166, -0.0213,  ...,  0.0592, -0.0709,  0.0073],\n                      [ 0.0668,  0.0060,  0.0026,  ..., -0.0160, -0.0228, -0.0409],\n                      ...,\n                      [-0.0141, -0.0145, -0.0232,  ...,  0.0586, -0.0506,  0.0633],\n                      [-0.0130, -0.0430,  0.0237,  ..., -0.0655, -0.0248,  0.0023],\n                      [-0.0023,  0.0209,  0.0106,  ..., -0.0365, -0.0450, -0.0201]],\n                     device='cuda:0')),\n             ('model.model.layers.19.self_attn.v_proj.weight',\n              tensor([[-0.0158,  0.0014, -0.0311,  ...,  0.0183,  0.0117,  0.0150],\n                      [-0.0251, -0.0200, -0.0158,  ...,  0.0011,  0.0021, -0.0185],\n                      [ 0.0079,  0.0117,  0.0168,  ..., -0.0118,  0.0087,  0.0017],\n                      ...,\n                      [-0.0142,  0.0074,  0.0111,  ...,  0.0232,  0.0140, -0.0218],\n                      [ 0.0009,  0.0047, -0.0021,  ...,  0.0116,  0.0013,  0.0075],\n                      [ 0.0198,  0.0003, -0.0142,  ...,  0.0115,  0.0080, -0.0349]],\n                     device='cuda:0')),\n             ('model.model.layers.19.self_attn.o_proj.weight',\n              tensor([[ 0.0012,  0.0100, -0.0104,  ..., -0.0100, -0.0067, -0.0029],\n                      [-0.0156,  0.0010, -0.0127,  ..., -0.0046, -0.0013, -0.0079],\n                      [ 0.0345,  0.0193, -0.0231,  ..., -0.0102,  0.0285,  0.0062],\n                      ...,\n                      [-0.0063,  0.0156,  0.0070,  ...,  0.0073,  0.0030, -0.0121],\n                      [-0.0046, -0.0018, -0.0165,  ..., -0.0149, -0.0087, -0.0092],\n                      [-0.0298,  0.0003,  0.0322,  ..., -0.0033,  0.0233,  0.0250]],\n                     device='cuda:0')),\n             ('model.model.layers.19.mlp.gate_proj.weight',\n              tensor([[ 3.7526e-02, -1.0377e-02, -2.4651e-02,  ...,  2.4780e-02,\n                        4.0290e-03, -1.2964e-02],\n                      [ 2.9417e-03, -5.1733e-03, -3.2401e-02,  ...,  6.4966e-03,\n                        5.2769e-03, -4.5321e-02],\n                      [ 2.3995e-02, -2.8939e-02,  1.0364e-02,  ..., -3.0695e-02,\n                       -7.4468e-03,  7.8847e-03],\n                      ...,\n                      [-3.2490e-02, -2.7159e-02, -3.6005e-02,  ...,  3.5356e-03,\n                        2.4067e-02,  1.2780e-02],\n                      [ 5.5005e-03,  1.2532e-02, -4.5858e-03,  ...,  8.1121e-03,\n                        3.8181e-02, -2.5980e-02],\n                      [-1.5980e-02, -2.8298e-02,  2.2521e-02,  ..., -3.0295e-02,\n                        6.7360e-04, -2.2510e-05]], device='cuda:0')),\n             ('model.model.layers.19.mlp.up_proj.weight',\n              tensor([[-0.0108,  0.0062, -0.0068,  ...,  0.0128, -0.0125,  0.0361],\n                      [ 0.0028, -0.0063, -0.0235,  ...,  0.0044, -0.0097, -0.0076],\n                      [-0.0205,  0.0322,  0.0100,  ..., -0.0336, -0.0092, -0.0253],\n                      ...,\n                      [ 0.0063, -0.0442, -0.0139,  ...,  0.0252,  0.0033, -0.0084],\n                      [-0.0003,  0.0402, -0.0099,  ..., -0.0037, -0.0233, -0.0239],\n                      [-0.0058,  0.0037, -0.0058,  ...,  0.0105, -0.0066, -0.0249]],\n                     device='cuda:0')),\n             ('model.model.layers.19.mlp.down_proj.weight',\n              tensor([[ 0.0201,  0.0263, -0.0058,  ...,  0.0259, -0.0372, -0.0096],\n                      [-0.0135, -0.0041, -0.0298,  ...,  0.0177,  0.0062,  0.0017],\n                      [ 0.0076,  0.0027, -0.0369,  ..., -0.0057, -0.0040, -0.0118],\n                      ...,\n                      [ 0.0093, -0.0197,  0.0039,  ...,  0.0176, -0.0095,  0.0416],\n                      [-0.0284, -0.0156,  0.0015,  ...,  0.0220, -0.0048, -0.0239],\n                      [ 0.0252, -0.0097,  0.0100,  ...,  0.0059, -0.0119, -0.0041]],\n                     device='cuda:0')),\n             ('model.model.layers.19.input_layernorm.weight',\n              tensor([0.2354, 0.2120, 0.2276,  ..., 0.2391, 0.2101, 0.2166], device='cuda:0')),\n             ('model.model.layers.19.post_attention_layernorm.weight',\n              tensor([0.2910, 0.2934, 0.2955,  ..., 0.3031, 0.2996, 0.2954], device='cuda:0')),\n             ('model.model.layers.20.self_attn.q_proj.weight',\n              tensor([[ 0.0021,  0.0022,  0.0096,  ..., -0.0115,  0.0645, -0.0112],\n                      [-0.0404,  0.0585, -0.0322,  ..., -0.0298, -0.0217,  0.0197],\n                      [-0.0293,  0.0291, -0.0355,  ..., -0.0101,  0.0201, -0.0062],\n                      ...,\n                      [ 0.0419, -0.0028,  0.0056,  ...,  0.0113, -0.0652, -0.0530],\n                      [-0.0346, -0.0276,  0.0303,  ...,  0.0139, -0.0719,  0.0567],\n                      [-0.0154,  0.0034, -0.0520,  ...,  0.0668, -0.0112, -0.0203]],\n                     device='cuda:0')),\n             ('model.model.layers.20.self_attn.k_proj.weight',\n              tensor([[ 0.0500,  0.0149, -0.0225,  ...,  0.0440,  0.0114,  0.0520],\n                      [ 0.0321,  0.0645,  0.0040,  ...,  0.0459,  0.0108,  0.0394],\n                      [-0.0032, -0.0281,  0.0312,  ...,  0.0534, -0.0068,  0.0231],\n                      ...,\n                      [-0.0190,  0.0208,  0.0032,  ...,  0.0826, -0.0377, -0.0193],\n                      [ 0.0083, -0.0170,  0.0152,  ...,  0.0263, -0.0091,  0.0352],\n                      [ 0.0243, -0.0023, -0.0080,  ..., -0.0165, -0.0373, -0.0054]],\n                     device='cuda:0')),\n             ('model.model.layers.20.self_attn.v_proj.weight',\n              tensor([[-0.0070, -0.0110, -0.0190,  ...,  0.0003, -0.0225, -0.0223],\n                      [-0.0080,  0.0118,  0.0223,  ...,  0.0089, -0.0010, -0.0082],\n                      [ 0.0116, -0.0039,  0.0330,  ..., -0.0125,  0.0201, -0.0020],\n                      ...,\n                      [-0.0065, -0.0150,  0.0141,  ...,  0.0192,  0.0080, -0.0086],\n                      [-0.0202,  0.0197, -0.0319,  ...,  0.0210, -0.0009, -0.0225],\n                      [ 0.0038, -0.0223,  0.0002,  ..., -0.0025,  0.0048,  0.0235]],\n                     device='cuda:0')),\n             ('model.model.layers.20.self_attn.o_proj.weight',\n              tensor([[-0.0187, -0.0089, -0.0216,  ..., -0.0291,  0.0110, -0.0053],\n                      [-0.0025,  0.0150,  0.0143,  ...,  0.0050, -0.0053,  0.0122],\n                      [-0.0236,  0.0084, -0.0357,  ..., -0.0099, -0.0063, -0.0135],\n                      ...,\n                      [-0.0093,  0.0031, -0.0006,  ..., -0.0266, -0.0323, -0.0078],\n                      [ 0.0373, -0.0047, -0.0203,  ..., -0.0321,  0.0060, -0.0153],\n                      [-0.0141, -0.0308, -0.0095,  ...,  0.0117,  0.0156,  0.0128]],\n                     device='cuda:0')),\n             ('model.model.layers.20.mlp.gate_proj.weight',\n              tensor([[ 1.1879e-02, -2.3687e-03, -5.5912e-03,  ..., -2.8422e-02,\n                       -7.9523e-02,  7.3109e-05],\n                      [-5.7217e-04, -4.7762e-02, -2.9603e-02,  ...,  2.3228e-02,\n                       -4.6809e-02,  1.0372e-02],\n                      [-1.6114e-02,  3.4481e-02, -9.2805e-03,  ...,  6.0850e-03,\n                       -1.7185e-03,  1.1632e-02],\n                      ...,\n                      [-1.9425e-03,  1.2025e-02, -2.1023e-02,  ...,  1.9274e-02,\n                        2.6418e-04,  8.1739e-03],\n                      [-2.3922e-02, -2.9068e-02, -3.1872e-03,  ..., -3.7812e-03,\n                       -4.5972e-03, -1.3752e-02],\n                      [ 8.7647e-03, -2.1302e-02,  1.9072e-02,  ..., -2.4066e-02,\n                        1.0801e-02,  4.4174e-04]], device='cuda:0')),\n             ('model.model.layers.20.mlp.up_proj.weight',\n              tensor([[-0.0407,  0.0208, -0.0114,  ..., -0.0144,  0.0552,  0.0061],\n                      [ 0.0020, -0.0125,  0.0104,  ..., -0.0138,  0.0495, -0.0020],\n                      [-0.0150, -0.0309, -0.0038,  ...,  0.0075,  0.0152, -0.0249],\n                      ...,\n                      [-0.0298,  0.0510, -0.0490,  ..., -0.0034, -0.0092, -0.0202],\n                      [ 0.0032, -0.0458,  0.0460,  ..., -0.0282,  0.0080, -0.0267],\n                      [ 0.0381, -0.0127,  0.0034,  ...,  0.0528, -0.0361,  0.0149]],\n                     device='cuda:0')),\n             ('model.model.layers.20.mlp.down_proj.weight',\n              tensor([[ 0.0128,  0.0253,  0.0174,  ..., -0.0379,  0.0163,  0.0199],\n                      [ 0.0371, -0.0289,  0.0085,  ...,  0.0108,  0.0122, -0.0005],\n                      [-0.0068, -0.0117,  0.0009,  ...,  0.0235, -0.0197, -0.0148],\n                      ...,\n                      [ 0.0050,  0.0164,  0.0033,  ...,  0.0012,  0.0020,  0.0198],\n                      [ 0.0130, -0.0100, -0.0210,  ...,  0.0188, -0.0158, -0.0153],\n                      [ 0.0128,  0.0104,  0.0527,  ...,  0.0130,  0.0112, -0.0062]],\n                     device='cuda:0')),\n             ('model.model.layers.20.input_layernorm.weight',\n              tensor([0.2437, 0.2339, 0.2483,  ..., 0.2754, 0.2252, 0.2440], device='cuda:0')),\n             ('model.model.layers.20.post_attention_layernorm.weight',\n              tensor([0.3195, 0.3183, 0.3176,  ..., 0.3087, 0.3190, 0.3141], device='cuda:0')),\n             ('model.model.layers.21.self_attn.q_proj.weight',\n              tensor([[ 0.0163, -0.0583,  0.0192,  ..., -0.0272,  0.0130, -0.0250],\n                      [ 0.0414,  0.0230,  0.0060,  ...,  0.0363, -0.0099,  0.0022],\n                      [ 0.0095, -0.0079, -0.0226,  ...,  0.0326, -0.0133,  0.0257],\n                      ...,\n                      [-0.0078,  0.0186, -0.0390,  ..., -0.0206,  0.0157,  0.0152],\n                      [-0.0237, -0.0448, -0.0506,  ..., -0.0131, -0.0016, -0.0061],\n                      [-0.0041, -0.0290,  0.0027,  ...,  0.0038, -0.0141,  0.0069]],\n                     device='cuda:0')),\n             ('model.model.layers.21.self_attn.k_proj.weight',\n              tensor([[ 0.0384,  0.0190, -0.0115,  ..., -0.0075, -0.0971,  0.0122],\n                      [ 0.0019,  0.0034,  0.0221,  ...,  0.0209,  0.0222, -0.0198],\n                      [-0.0100, -0.0556, -0.0302,  ..., -0.0251,  0.0195,  0.0073],\n                      ...,\n                      [ 0.0385, -0.0048, -0.0650,  ...,  0.0164, -0.0379,  0.0097],\n                      [-0.0198,  0.0408, -0.0040,  ..., -0.0371,  0.0330,  0.0010],\n                      [ 0.0385,  0.0056, -0.0192,  ...,  0.0165, -0.0450, -0.0085]],\n                     device='cuda:0')),\n             ('model.model.layers.21.self_attn.v_proj.weight',\n              tensor([[-0.0075,  0.0056, -0.0030,  ...,  0.0075, -0.0428, -0.0151],\n                      [ 0.0385, -0.0024, -0.0200,  ..., -0.0262,  0.0127, -0.0370],\n                      [ 0.0032,  0.0059,  0.0134,  ..., -0.0089,  0.0102,  0.0099],\n                      ...,\n                      [ 0.0040,  0.0068,  0.0110,  ..., -0.0109,  0.0019, -0.0314],\n                      [ 0.0092,  0.0013,  0.0100,  ..., -0.0105, -0.0272,  0.0289],\n                      [-0.0267, -0.0115, -0.0102,  ..., -0.0146,  0.0009,  0.0162]],\n                     device='cuda:0')),\n             ('model.model.layers.21.self_attn.o_proj.weight',\n              tensor([[ 0.0127, -0.0098,  0.0104,  ...,  0.0001, -0.0307,  0.0095],\n                      [-0.0128,  0.0332,  0.0170,  ..., -0.0052, -0.0009,  0.0121],\n                      [ 0.0120,  0.0114, -0.0472,  ...,  0.0020, -0.0037,  0.0234],\n                      ...,\n                      [ 0.0004,  0.0180, -0.0086,  ..., -0.0007,  0.0036,  0.0104],\n                      [ 0.0311,  0.0175,  0.0074,  ...,  0.0110, -0.0256, -0.0326],\n                      [ 0.0108,  0.0122, -0.0153,  ...,  0.0169,  0.0118, -0.0133]],\n                     device='cuda:0')),\n             ('model.model.layers.21.mlp.gate_proj.weight',\n              tensor([[-0.0221,  0.0562,  0.0560,  ...,  0.0046,  0.0025,  0.0276],\n                      [-0.0031,  0.0223,  0.0017,  ..., -0.0079,  0.0007,  0.0026],\n                      [-0.0307, -0.0252,  0.0046,  ...,  0.0286,  0.0282,  0.0337],\n                      ...,\n                      [ 0.0228, -0.0201,  0.0300,  ..., -0.0350, -0.0271,  0.0018],\n                      [ 0.0171, -0.0131,  0.0304,  ...,  0.0151,  0.0032, -0.0265],\n                      [ 0.0186, -0.0135,  0.0216,  ...,  0.0127,  0.0182,  0.0286]],\n                     device='cuda:0')),\n             ('model.model.layers.21.mlp.up_proj.weight',\n              tensor([[ 2.1494e-03,  2.3238e-02, -4.6114e-03,  ..., -1.2345e-02,\n                        5.7107e-04, -1.4719e-02],\n                      [ 1.3596e-03, -9.7017e-03,  2.6182e-02,  ..., -9.5596e-03,\n                       -8.4040e-03,  9.0752e-03],\n                      [ 3.2941e-02, -1.9150e-03,  2.5652e-02,  ...,  5.9501e-03,\n                       -1.5407e-02,  3.3979e-02],\n                      ...,\n                      [-8.5673e-03, -2.1174e-02,  2.0599e-02,  ...,  3.8433e-02,\n                       -1.7709e-02,  8.1588e-03],\n                      [ 1.8861e-02, -2.1270e-03,  6.5782e-02,  ..., -1.6134e-02,\n                        1.0173e-02, -2.2125e-02],\n                      [ 3.0806e-03,  1.8469e-02, -1.6801e-02,  ..., -6.4892e-03,\n                        3.2482e-05,  5.3016e-03]], device='cuda:0')),\n             ('model.model.layers.21.mlp.down_proj.weight',\n              tensor([[-0.0190, -0.0152, -0.0064,  ..., -0.0335, -0.0121,  0.0005],\n                      [ 0.0026,  0.0004,  0.0112,  ...,  0.0089, -0.0154,  0.0147],\n                      [ 0.0102, -0.0235, -0.0227,  ...,  0.0168,  0.0439,  0.0215],\n                      ...,\n                      [-0.0118, -0.0071,  0.0201,  ..., -0.0137, -0.0013, -0.0004],\n                      [-0.0043,  0.0042,  0.0132,  ...,  0.0275, -0.0228,  0.0061],\n                      [ 0.0163, -0.0347, -0.0088,  ..., -0.0102,  0.0265, -0.0041]],\n                     device='cuda:0')),\n             ('model.model.layers.21.input_layernorm.weight',\n              tensor([0.2391, 0.2600, 0.2351,  ..., 0.2603, 0.2318, 0.2440], device='cuda:0')),\n             ('model.model.layers.21.post_attention_layernorm.weight',\n              tensor([0.3170, 0.3200, 0.3243,  ..., 0.3302, 0.3234, 0.3197], device='cuda:0')),\n             ('model.model.layers.22.self_attn.q_proj.weight',\n              tensor([[ 0.0576, -0.0652,  0.0854,  ..., -0.0004,  0.0107,  0.0043],\n                      [-0.0234, -0.0248,  0.0451,  ..., -0.0480,  0.0013,  0.0369],\n                      [-0.0070, -0.0179,  0.0635,  ..., -0.0041,  0.0200, -0.0510],\n                      ...,\n                      [-0.0126,  0.0403, -0.0286,  ...,  0.0501, -0.0311,  0.0343],\n                      [-0.0275,  0.0474,  0.0346,  ...,  0.0407, -0.0491, -0.0480],\n                      [ 0.0249,  0.0119, -0.0140,  ...,  0.0033, -0.0318, -0.0280]],\n                     device='cuda:0')),\n             ('model.model.layers.22.self_attn.k_proj.weight',\n              tensor([[ 0.0020, -0.0220, -0.0353,  ...,  0.0159,  0.0279, -0.0810],\n                      [-0.0371,  0.0722,  0.0046,  ..., -0.0291,  0.0376,  0.0681],\n                      [ 0.0173, -0.0869, -0.0260,  ..., -0.0368, -0.0173,  0.0360],\n                      ...,\n                      [-0.0324, -0.0487, -0.0209,  ...,  0.0074,  0.0554, -0.0462],\n                      [-0.0149, -0.0064, -0.0218,  ..., -0.0366,  0.0260, -0.0037],\n                      [ 0.0062, -0.0281,  0.0099,  ..., -0.0520, -0.0094, -0.0222]],\n                     device='cuda:0')),\n             ('model.model.layers.22.self_attn.v_proj.weight',\n              tensor([[-0.0134, -0.0035,  0.0070,  ...,  0.0311, -0.0118,  0.0141],\n                      [ 0.0320, -0.0202, -0.0064,  ..., -0.0081, -0.0108,  0.0349],\n                      [-0.0060, -0.0102, -0.0305,  ...,  0.0184, -0.0059, -0.0134],\n                      ...,\n                      [-0.0127,  0.0196,  0.0194,  ...,  0.0180, -0.0148,  0.0110],\n                      [-0.0007, -0.0086, -0.0021,  ...,  0.0190,  0.0050, -0.0021],\n                      [ 0.0042,  0.0293,  0.0095,  ..., -0.0068, -0.0127, -0.0176]],\n                     device='cuda:0')),\n             ('model.model.layers.22.self_attn.o_proj.weight',\n              tensor([[-0.0067,  0.0060,  0.0016,  ...,  0.0145,  0.0060,  0.0051],\n                      [ 0.0109,  0.0175,  0.0056,  ..., -0.0386,  0.0085, -0.0064],\n                      [-0.0028, -0.0133,  0.0195,  ..., -0.0147, -0.0091, -0.0066],\n                      ...,\n                      [-0.0003, -0.0004, -0.0145,  ..., -0.0090,  0.0053, -0.0180],\n                      [-0.0051,  0.0068, -0.0296,  ...,  0.0095, -0.0046, -0.0006],\n                      [ 0.0030, -0.0184, -0.0061,  ...,  0.0016, -0.0386, -0.0046]],\n                     device='cuda:0')),\n             ('model.model.layers.22.mlp.gate_proj.weight',\n              tensor([[ 0.0037, -0.0273,  0.0005,  ..., -0.0214, -0.0320,  0.0195],\n                      [-0.0212, -0.0033, -0.0099,  ..., -0.0059, -0.0080,  0.0129],\n                      [ 0.0246,  0.0373,  0.0263,  ..., -0.0256, -0.0128,  0.0012],\n                      ...,\n                      [ 0.0152,  0.0170,  0.0174,  ..., -0.0103,  0.0102, -0.0044],\n                      [-0.0326, -0.0231,  0.0190,  ...,  0.0272,  0.0138, -0.0117],\n                      [ 0.0065,  0.0350,  0.0061,  ...,  0.0384,  0.0183, -0.0174]],\n                     device='cuda:0')),\n             ('model.model.layers.22.mlp.up_proj.weight',\n              tensor([[-0.0321, -0.0254, -0.0509,  ...,  0.0125, -0.0440, -0.0295],\n                      [-0.0456,  0.0028,  0.0022,  ...,  0.0148, -0.0007,  0.0247],\n                      [-0.0038, -0.0197,  0.0121,  ..., -0.0331, -0.0475,  0.0119],\n                      ...,\n                      [-0.0143, -0.0426, -0.0504,  ...,  0.0324, -0.0318, -0.0005],\n                      [-0.0271, -0.0028,  0.0239,  ..., -0.0048,  0.0169,  0.0069],\n                      [-0.0140,  0.0066,  0.0127,  ..., -0.0340, -0.0207,  0.0216]],\n                     device='cuda:0')),\n             ('model.model.layers.22.mlp.down_proj.weight',\n              tensor([[ 0.0215,  0.0473, -0.0018,  ..., -0.0136, -0.0146,  0.0308],\n                      [-0.0151, -0.0051, -0.0133,  ..., -0.0220, -0.0306, -0.0086],\n                      [-0.0056, -0.0178,  0.0091,  ...,  0.0311,  0.0111,  0.0042],\n                      ...,\n                      [-0.0150, -0.0019,  0.0186,  ...,  0.0054,  0.0115,  0.0478],\n                      [-0.0233, -0.0190,  0.0020,  ..., -0.0106, -0.0404, -0.0153],\n                      [-0.0160,  0.0044, -0.0127,  ...,  0.0285, -0.0273,  0.0109]],\n                     device='cuda:0')),\n             ('model.model.layers.22.input_layernorm.weight',\n              tensor([0.2496, 0.2876, 0.2552,  ..., 0.2616, 0.2450, 0.2623], device='cuda:0')),\n             ('model.model.layers.22.post_attention_layernorm.weight',\n              tensor([0.3347, 0.3409, 0.3288,  ..., 0.3327, 0.3470, 0.3306], device='cuda:0')),\n             ('model.model.layers.23.self_attn.q_proj.weight',\n              tensor([[ 0.0207,  0.0098,  0.0117,  ..., -0.0413, -0.0288, -0.0040],\n                      [ 0.0065,  0.0310,  0.0417,  ...,  0.0011, -0.0006,  0.0467],\n                      [ 0.0120, -0.0044, -0.0250,  ..., -0.0113, -0.0135, -0.0446],\n                      ...,\n                      [-0.0447,  0.0178, -0.0402,  ...,  0.0143, -0.0318,  0.0278],\n                      [-0.0250, -0.0050, -0.0180,  ..., -0.0469, -0.0115,  0.0181],\n                      [-0.0580,  0.0375, -0.0202,  ...,  0.0421, -0.0754,  0.0700]],\n                     device='cuda:0')),\n             ('model.model.layers.23.self_attn.k_proj.weight',\n              tensor([[ 0.0537, -0.0229, -0.0080,  ..., -0.1149, -0.0539, -0.0302],\n                      [ 0.0204, -0.0089,  0.0662,  ..., -0.0631,  0.0120,  0.0666],\n                      [-0.0210, -0.0347,  0.0147,  ..., -0.0069, -0.0258, -0.0055],\n                      ...,\n                      [ 0.0506,  0.0129, -0.0071,  ...,  0.0121,  0.0059,  0.0580],\n                      [ 0.0159,  0.0037,  0.0379,  ..., -0.0508, -0.0198,  0.0492],\n                      [ 0.0918,  0.0484,  0.0554,  ..., -0.0062,  0.0403, -0.0062]],\n                     device='cuda:0')),\n             ('model.model.layers.23.self_attn.v_proj.weight',\n              tensor([[-0.0012,  0.0258, -0.0112,  ...,  0.0040, -0.0034, -0.0107],\n                      [-0.0153,  0.0086,  0.0322,  ..., -0.0034, -0.0104, -0.0260],\n                      [ 0.0172,  0.0178,  0.0097,  ...,  0.0179, -0.0175,  0.0037],\n                      ...,\n                      [ 0.0314,  0.0315, -0.0099,  ...,  0.0165,  0.0311,  0.0322],\n                      [-0.0010,  0.0116, -0.0228,  ..., -0.0095,  0.0083, -0.0026],\n                      [ 0.0156, -0.0022,  0.0405,  ...,  0.0329, -0.0087,  0.0512]],\n                     device='cuda:0')),\n             ('model.model.layers.23.self_attn.o_proj.weight',\n              tensor([[-0.0467,  0.0104, -0.0012,  ..., -0.0345,  0.0094, -0.0024],\n                      [-0.0297, -0.0049, -0.0155,  ..., -0.0188, -0.0011, -0.0057],\n                      [ 0.0100, -0.0373, -0.0165,  ..., -0.0055,  0.0309, -0.0088],\n                      ...,\n                      [ 0.0008,  0.0246,  0.0091,  ...,  0.0017,  0.0354,  0.0406],\n                      [ 0.0006,  0.0040,  0.0137,  ..., -0.0294,  0.0009,  0.0556],\n                      [ 0.0064, -0.0009,  0.0063,  ...,  0.0027,  0.0233,  0.0266]],\n                     device='cuda:0')),\n             ('model.model.layers.23.mlp.gate_proj.weight',\n              tensor([[-0.0278,  0.0312, -0.0049,  ...,  0.0124,  0.0117, -0.0066],\n                      [-0.0019,  0.0635,  0.0193,  ..., -0.0189, -0.0025, -0.0023],\n                      [ 0.0017, -0.0396,  0.0102,  ..., -0.0199,  0.0245,  0.0031],\n                      ...,\n                      [-0.0202, -0.0325,  0.0431,  ..., -0.0190, -0.0234,  0.0214],\n                      [ 0.0141, -0.0251, -0.0037,  ...,  0.0093, -0.0083, -0.0012],\n                      [ 0.0004, -0.0083, -0.0197,  ..., -0.0086,  0.0187, -0.0170]],\n                     device='cuda:0')),\n             ('model.model.layers.23.mlp.up_proj.weight',\n              tensor([[ 0.0047, -0.0244, -0.0124,  ...,  0.0017,  0.0119, -0.0183],\n                      [-0.0151,  0.0104, -0.0099,  ..., -0.0327,  0.0312,  0.0215],\n                      [-0.0386, -0.0213,  0.0316,  ..., -0.0020, -0.0437, -0.0028],\n                      ...,\n                      [-0.0007, -0.0044,  0.0137,  ...,  0.0131, -0.0005, -0.0053],\n                      [ 0.0092,  0.0096, -0.0017,  ..., -0.0082, -0.0005, -0.0002],\n                      [ 0.0104, -0.0184,  0.0200,  ...,  0.0094,  0.0114, -0.0073]],\n                     device='cuda:0')),\n             ('model.model.layers.23.mlp.down_proj.weight',\n              tensor([[-0.0075, -0.0179,  0.0335,  ...,  0.0363, -0.0159,  0.0360],\n                      [-0.0437, -0.0093,  0.0282,  ..., -0.0098,  0.0006,  0.0153],\n                      [-0.0496,  0.0196, -0.0315,  ..., -0.0023,  0.0227,  0.0063],\n                      ...,\n                      [-0.0047, -0.0032,  0.0021,  ..., -0.0146,  0.0108, -0.0133],\n                      [-0.0281,  0.0001, -0.0036,  ...,  0.0116, -0.0148,  0.0158],\n                      [ 0.0022,  0.0148,  0.0359,  ...,  0.0295,  0.0236, -0.0026]],\n                     device='cuda:0')),\n             ('model.model.layers.23.input_layernorm.weight',\n              tensor([0.2772, 0.2898, 0.2808,  ..., 0.2947, 0.2809, 0.2882], device='cuda:0')),\n             ('model.model.layers.23.post_attention_layernorm.weight',\n              tensor([0.3496, 0.3379, 0.3515,  ..., 0.3577, 0.3526, 0.3502], device='cuda:0')),\n             ('model.model.layers.24.self_attn.q_proj.weight',\n              tensor([[ 0.0217,  0.0409,  0.0053,  ...,  0.0653, -0.0467,  0.0459],\n                      [-0.0137, -0.0173, -0.0481,  ..., -0.0221, -0.0140, -0.0318],\n                      [ 0.0090,  0.0505, -0.0413,  ...,  0.0064, -0.0101, -0.0025],\n                      ...,\n                      [-0.0168,  0.0500,  0.0331,  ...,  0.0378, -0.0021, -0.0302],\n                      [ 0.0356,  0.0078,  0.0187,  ...,  0.0547, -0.0085, -0.0052],\n                      [ 0.0045,  0.0230, -0.0530,  ..., -0.0086,  0.0227, -0.0309]],\n                     device='cuda:0')),\n             ('model.model.layers.24.self_attn.k_proj.weight',\n              tensor([[ 0.0512,  0.0517,  0.0144,  ..., -0.0370,  0.0064, -0.0380],\n                      [ 0.0109, -0.0231,  0.0942,  ..., -0.0180, -0.0254,  0.0177],\n                      [ 0.0082,  0.0176,  0.0207,  ...,  0.0018, -0.0026, -0.0394],\n                      ...,\n                      [ 0.0093, -0.0219, -0.0114,  ..., -0.0049,  0.0317, -0.0242],\n                      [ 0.0243, -0.0297,  0.0364,  ..., -0.0478, -0.0188,  0.0255],\n                      [ 0.0072, -0.0100,  0.0983,  ...,  0.0354,  0.0001,  0.0212]],\n                     device='cuda:0')),\n             ('model.model.layers.24.self_attn.v_proj.weight',\n              tensor([[ 0.0055, -0.0190,  0.0011,  ...,  0.0258, -0.0046,  0.0201],\n                      [-0.0351, -0.0198,  0.0092,  ..., -0.0041,  0.0366,  0.0213],\n                      [-0.0027, -0.0177,  0.0099,  ..., -0.0294,  0.0153,  0.0138],\n                      ...,\n                      [ 0.0010, -0.0125,  0.0045,  ..., -0.0097,  0.0113, -0.0059],\n                      [ 0.0295,  0.0070, -0.0140,  ...,  0.0014,  0.0009, -0.0092],\n                      [-0.0154,  0.0081, -0.0218,  ...,  0.0183, -0.0335, -0.0132]],\n                     device='cuda:0')),\n             ('model.model.layers.24.self_attn.o_proj.weight',\n              tensor([[-0.0036, -0.0111,  0.0002,  ...,  0.0064, -0.0159,  0.0159],\n                      [-0.0015, -0.0048,  0.0110,  ..., -0.0037,  0.0168,  0.0283],\n                      [ 0.0119, -0.0028, -0.0210,  ...,  0.0117,  0.0045, -0.0086],\n                      ...,\n                      [-0.0066,  0.0139,  0.0026,  ...,  0.0226,  0.0081, -0.0062],\n                      [-0.0136, -0.0190, -0.0254,  ..., -0.0266, -0.0065,  0.0228],\n                      [ 0.0076, -0.0012,  0.0137,  ...,  0.0208,  0.0025, -0.0072]],\n                     device='cuda:0')),\n             ('model.model.layers.24.mlp.gate_proj.weight',\n              tensor([[ 0.0032, -0.0302,  0.0158,  ...,  0.0623,  0.0156, -0.0064],\n                      [-0.0332,  0.0024, -0.0136,  ..., -0.0288,  0.0036,  0.0125],\n                      [-0.0014,  0.0703, -0.0165,  ..., -0.0253, -0.0283, -0.0114],\n                      ...,\n                      [ 0.0172,  0.0061, -0.0365,  ..., -0.0140, -0.0140,  0.0077],\n                      [ 0.0197,  0.0102, -0.0316,  ..., -0.0115, -0.0090, -0.0002],\n                      [-0.0060,  0.0064,  0.0087,  ...,  0.0093,  0.0175,  0.0166]],\n                     device='cuda:0')),\n             ('model.model.layers.24.mlp.up_proj.weight',\n              tensor([[-0.0138,  0.0139,  0.0284,  ..., -0.0242, -0.0400, -0.0065],\n                      [-0.0160, -0.0129, -0.0042,  ...,  0.0016,  0.0352,  0.0024],\n                      [ 0.0282,  0.0121, -0.0179,  ..., -0.0002, -0.0090, -0.0104],\n                      ...,\n                      [-0.0104,  0.0060, -0.0534,  ..., -0.0215, -0.0276, -0.0096],\n                      [-0.0118,  0.0439, -0.0049,  ...,  0.0046,  0.0116, -0.0099],\n                      [ 0.0204,  0.0148, -0.0453,  ..., -0.0338,  0.0004, -0.0383]],\n                     device='cuda:0')),\n             ('model.model.layers.24.mlp.down_proj.weight',\n              tensor([[-0.0155, -0.0003,  0.0068,  ..., -0.0060, -0.0320, -0.0306],\n                      [-0.0102,  0.0031,  0.0199,  ..., -0.0078, -0.0073,  0.0167],\n                      [ 0.0049,  0.0068,  0.0336,  ...,  0.0357, -0.0130, -0.0088],\n                      ...,\n                      [-0.0194,  0.0180, -0.0152,  ...,  0.0261, -0.0353,  0.0206],\n                      [ 0.0285,  0.0047, -0.0312,  ...,  0.0130, -0.0326, -0.0046],\n                      [-0.0014,  0.0110, -0.0202,  ...,  0.0004,  0.0112,  0.0107]],\n                     device='cuda:0')),\n             ('model.model.layers.24.input_layernorm.weight',\n              tensor([0.2665, 0.2799, 0.2562,  ..., 0.2819, 0.2610, 0.2769], device='cuda:0')),\n             ('model.model.layers.24.post_attention_layernorm.weight',\n              tensor([0.3479, 0.3586, 0.3624,  ..., 0.3613, 0.3722, 0.3643], device='cuda:0')),\n             ('model.model.layers.25.self_attn.q_proj.weight',\n              tensor([[ 0.0145,  0.0229, -0.0146,  ..., -0.0119,  0.0299, -0.0010],\n                      [-0.0281, -0.0304,  0.0710,  ..., -0.0204, -0.0004,  0.0106],\n                      [ 0.0232,  0.0063,  0.0506,  ..., -0.0177,  0.0195, -0.0162],\n                      ...,\n                      [-0.0211,  0.0332,  0.0149,  ..., -0.0188, -0.0008, -0.0184],\n                      [ 0.0408, -0.0143, -0.0092,  ..., -0.0609,  0.0184, -0.0023],\n                      [-0.0678,  0.0247,  0.0494,  ...,  0.0202, -0.0180,  0.0492]],\n                     device='cuda:0')),\n             ('model.model.layers.25.self_attn.k_proj.weight',\n              tensor([[ 5.2354e-02, -4.0343e-02,  1.6187e-02,  ...,  8.5689e-02,\n                        4.8028e-03,  6.4127e-02],\n                      [ 3.0958e-02,  7.8993e-03,  4.0627e-02,  ..., -1.1874e-01,\n                       -5.8370e-02, -3.0930e-02],\n                      [-3.1905e-02, -4.8599e-02, -3.2385e-02,  ..., -3.3170e-03,\n                        4.6217e-03, -7.8223e-03],\n                      ...,\n                      [-7.0770e-04, -3.0825e-02, -2.2337e-02,  ...,  3.7058e-02,\n                        5.8845e-02, -1.6296e-02],\n                      [ 6.3263e-02, -3.0339e-02, -4.2075e-03,  ...,  3.0222e-03,\n                       -4.8286e-03, -3.4252e-05],\n                      [-7.3388e-02, -1.5020e-02, -1.0592e-02,  ..., -4.2291e-02,\n                        4.6351e-02, -1.7403e-02]], device='cuda:0')),\n             ('model.model.layers.25.self_attn.v_proj.weight',\n              tensor([[-0.0133,  0.0313,  0.0021,  ...,  0.0034, -0.0109, -0.0039],\n                      [-0.0198, -0.0038,  0.0087,  ...,  0.0059, -0.0058, -0.0124],\n                      [-0.0215, -0.0157,  0.0173,  ...,  0.0302,  0.0287,  0.0155],\n                      ...,\n                      [ 0.0221,  0.0015, -0.0190,  ...,  0.0001,  0.0172,  0.0034],\n                      [-0.0304, -0.0043,  0.0156,  ...,  0.0117,  0.0310, -0.0466],\n                      [ 0.0075,  0.0045,  0.0327,  ...,  0.0254, -0.0228,  0.0196]],\n                     device='cuda:0')),\n             ('model.model.layers.25.self_attn.o_proj.weight',\n              tensor([[ 0.0068,  0.0311,  0.0142,  ..., -0.0044,  0.0194, -0.0016],\n                      [ 0.0125,  0.0117,  0.0162,  ..., -0.0105, -0.0028, -0.0130],\n                      [ 0.0022, -0.0256, -0.0333,  ...,  0.0152,  0.0063, -0.0477],\n                      ...,\n                      [ 0.0086, -0.0281,  0.0023,  ...,  0.0113,  0.0110, -0.0129],\n                      [-0.0291, -0.0016, -0.0135,  ..., -0.0184, -0.0165,  0.0217],\n                      [-0.0143,  0.0293, -0.0081,  ...,  0.0116, -0.0055, -0.0110]],\n                     device='cuda:0')),\n             ('model.model.layers.25.mlp.gate_proj.weight',\n              tensor([[-0.0061,  0.0079, -0.0045,  ..., -0.0202,  0.0061,  0.0078],\n                      [-0.0195,  0.0240, -0.0086,  ..., -0.0243,  0.0043, -0.0253],\n                      [-0.0155, -0.0009, -0.0102,  ...,  0.0098,  0.0025,  0.0379],\n                      ...,\n                      [ 0.0112,  0.0010, -0.0169,  ...,  0.0084,  0.0226,  0.0048],\n                      [-0.0226, -0.0315, -0.0163,  ...,  0.0070, -0.0323, -0.0278],\n                      [ 0.0349, -0.0314, -0.0208,  ..., -0.0246, -0.0256,  0.0119]],\n                     device='cuda:0')),\n             ('model.model.layers.25.mlp.up_proj.weight',\n              tensor([[-0.0142, -0.0235,  0.0169,  ...,  0.0140, -0.0284,  0.0046],\n                      [-0.0211,  0.0223,  0.0129,  ...,  0.0136,  0.0109, -0.0288],\n                      [-0.0024,  0.0382, -0.0023,  ...,  0.0021,  0.0334, -0.0050],\n                      ...,\n                      [-0.0212, -0.0291, -0.0149,  ...,  0.0051,  0.0050, -0.0289],\n                      [ 0.0262,  0.0263, -0.0109,  ..., -0.0278, -0.0024, -0.0042],\n                      [-0.0031, -0.0616, -0.0083,  ...,  0.0125, -0.0162,  0.0145]],\n                     device='cuda:0')),\n             ('model.model.layers.25.mlp.down_proj.weight',\n              tensor([[-0.0061, -0.0130,  0.0135,  ...,  0.0134, -0.0533,  0.0379],\n                      [ 0.0411, -0.0090, -0.0138,  ...,  0.0043, -0.0076, -0.0204],\n                      [-0.0147,  0.0014, -0.0127,  ...,  0.0098,  0.0056,  0.0165],\n                      ...,\n                      [-0.0163,  0.0118, -0.0018,  ..., -0.0372, -0.0342,  0.0036],\n                      [ 0.0010, -0.0066,  0.0034,  ...,  0.0135,  0.0039, -0.0057],\n                      [ 0.0285,  0.0023,  0.0040,  ...,  0.0138,  0.0128, -0.0225]],\n                     device='cuda:0')),\n             ('model.model.layers.25.input_layernorm.weight',\n              tensor([0.2782, 0.3080, 0.2850,  ..., 0.2947, 0.2797, 0.2837], device='cuda:0')),\n             ('model.model.layers.25.post_attention_layernorm.weight',\n              tensor([0.3739, 0.3713, 0.3764,  ..., 0.3752, 0.3767, 0.3679], device='cuda:0')),\n             ('model.model.layers.26.self_attn.q_proj.weight',\n              tensor([[ 0.0681, -0.0015, -0.0488,  ..., -0.0303,  0.0027, -0.0301],\n                      [ 0.0587,  0.0760, -0.0121,  ..., -0.0640, -0.0400,  0.0268],\n                      [-0.0133, -0.0411, -0.0339,  ..., -0.0154,  0.0543,  0.0172],\n                      ...,\n                      [-0.0401,  0.0075, -0.0212,  ..., -0.0055,  0.0005, -0.0527],\n                      [ 0.0964,  0.0007,  0.0218,  ..., -0.0167, -0.0491,  0.0162],\n                      [ 0.0345,  0.0196,  0.0130,  ...,  0.0364, -0.0257, -0.0179]],\n                     device='cuda:0')),\n             ('model.model.layers.26.self_attn.k_proj.weight',\n              tensor([[-5.5315e-02, -3.9003e-02, -2.8909e-02,  ..., -3.7965e-02,\n                        5.5736e-03, -4.8907e-03],\n                      [ 2.5397e-02, -9.5106e-03,  1.7201e-02,  ...,  3.5949e-02,\n                       -4.0486e-03, -1.4002e-03],\n                      [ 2.8362e-02,  7.3675e-03,  9.9017e-03,  ..., -2.7599e-02,\n                       -2.5416e-02,  2.2182e-02],\n                      ...,\n                      [ 6.4982e-03, -1.2236e-02,  1.0962e-01,  ..., -4.1488e-02,\n                       -1.9768e-02, -1.0812e-02],\n                      [-4.5626e-03,  7.7561e-03,  6.4078e-03,  ..., -4.6019e-05,\n                        2.3133e-02,  2.8794e-02],\n                      [ 2.2155e-02, -6.1793e-02,  1.4870e-03,  ..., -3.3470e-03,\n                       -3.0467e-02, -5.0274e-02]], device='cuda:0')),\n             ('model.model.layers.26.self_attn.v_proj.weight',\n              tensor([[-0.0048,  0.0315,  0.0052,  ..., -0.0105, -0.0047, -0.0086],\n                      [ 0.0063, -0.0281, -0.0016,  ..., -0.0080,  0.0015,  0.0090],\n                      [-0.0109, -0.0116, -0.0150,  ..., -0.0233,  0.0371, -0.0074],\n                      ...,\n                      [ 0.0009, -0.0143, -0.0029,  ..., -0.0092, -0.0008,  0.0161],\n                      [ 0.0069, -0.0021,  0.0030,  ..., -0.0156,  0.0109, -0.0468],\n                      [ 0.0091, -0.0320,  0.0327,  ...,  0.0016,  0.0234,  0.0109]],\n                     device='cuda:0')),\n             ('model.model.layers.26.self_attn.o_proj.weight',\n              tensor([[-0.0155, -0.0056,  0.0220,  ...,  0.0133, -0.0009, -0.0077],\n                      [-0.0257,  0.0141,  0.0346,  ...,  0.0062, -0.0057,  0.0178],\n                      [-0.0113,  0.0120,  0.0130,  ...,  0.0026,  0.0240, -0.0159],\n                      ...,\n                      [ 0.0168,  0.0026,  0.0065,  ...,  0.0355, -0.0013,  0.0108],\n                      [-0.0001, -0.0002, -0.0032,  ..., -0.0192,  0.0032,  0.0057],\n                      [-0.0155, -0.0280, -0.0103,  ..., -0.0308,  0.0273, -0.0175]],\n                     device='cuda:0')),\n             ('model.model.layers.26.mlp.gate_proj.weight',\n              tensor([[ 2.1945e-02,  1.2692e-03, -3.5510e-03,  ...,  7.4141e-05,\n                        1.7859e-02,  2.6728e-02],\n                      [-2.9827e-02, -7.2305e-03,  1.0040e-02,  ...,  2.0281e-02,\n                       -3.2150e-02,  1.0426e-02],\n                      [-1.5386e-02,  6.2120e-03,  1.6012e-02,  ...,  1.4794e-02,\n                        1.9805e-02, -8.4753e-03],\n                      ...,\n                      [ 1.2508e-02,  5.8362e-02, -4.6039e-02,  ...,  5.3729e-03,\n                       -2.3656e-02,  9.7598e-03],\n                      [-9.6428e-03, -4.4720e-02,  4.5830e-02,  ..., -2.1256e-02,\n                       -1.5134e-02, -2.9552e-02],\n                      [ 3.1297e-02, -3.6645e-02,  1.8479e-02,  ...,  4.9125e-03,\n                        1.5957e-02,  2.5802e-02]], device='cuda:0')),\n             ('model.model.layers.26.mlp.up_proj.weight',\n              tensor([[ 0.0159, -0.0433,  0.0038,  ..., -0.0315, -0.0442, -0.0165],\n                      [ 0.0239, -0.0212, -0.0267,  ..., -0.0117, -0.0341,  0.0261],\n                      [ 0.0086, -0.0088,  0.0104,  ...,  0.0265, -0.0186, -0.0095],\n                      ...,\n                      [ 0.0118,  0.0291,  0.0344,  ..., -0.0294,  0.0074,  0.0082],\n                      [-0.0123,  0.0069, -0.0103,  ..., -0.0161,  0.0182,  0.0082],\n                      [-0.0006, -0.0045, -0.0024,  ...,  0.0178,  0.0284, -0.0107]],\n                     device='cuda:0')),\n             ('model.model.layers.26.mlp.down_proj.weight',\n              tensor([[ 0.0039, -0.0142,  0.0243,  ...,  0.0033,  0.0047, -0.0282],\n                      [-0.0064, -0.0234, -0.0216,  ...,  0.0034, -0.0036,  0.0405],\n                      [ 0.0012,  0.0204,  0.0255,  ..., -0.0032, -0.0562, -0.0163],\n                      ...,\n                      [-0.0043,  0.0033, -0.0064,  ...,  0.0165,  0.0086,  0.0347],\n                      [ 0.0161,  0.0301, -0.0431,  ...,  0.0137,  0.0071, -0.0094],\n                      [ 0.0143,  0.0084,  0.0142,  ..., -0.0025,  0.0081,  0.0285]],\n                     device='cuda:0')),\n             ('model.model.layers.26.input_layernorm.weight',\n              tensor([0.2600, 0.2905, 0.2665,  ..., 0.2807, 0.2554, 0.2741], device='cuda:0')),\n             ('model.model.layers.26.post_attention_layernorm.weight',\n              tensor([0.3817, 0.3924, 0.3979,  ..., 0.3916, 0.3953, 0.3906], device='cuda:0')),\n             ('model.model.layers.27.self_attn.q_proj.weight',\n              tensor([[ 0.0211, -0.0472, -0.0067,  ..., -0.0240,  0.0226, -0.0128],\n                      [-0.0492,  0.0458,  0.0495,  ..., -0.0158,  0.0361, -0.0037],\n                      [-0.0312,  0.0357,  0.0037,  ...,  0.0270,  0.0058,  0.0291],\n                      ...,\n                      [-0.0539, -0.0133, -0.0210,  ..., -0.0356,  0.0384, -0.0013],\n                      [-0.0094,  0.0186,  0.0278,  ...,  0.0078, -0.0298,  0.0560],\n                      [ 0.0164,  0.0281,  0.0352,  ..., -0.0164,  0.0066, -0.0190]],\n                     device='cuda:0')),\n             ('model.model.layers.27.self_attn.k_proj.weight',\n              tensor([[-0.0067, -0.0367, -0.0212,  ..., -0.0496, -0.0130, -0.0330],\n                      [ 0.0299,  0.0154, -0.0110,  ...,  0.0194, -0.0005,  0.0330],\n                      [ 0.0274,  0.0452,  0.0058,  ...,  0.0093, -0.0361,  0.0261],\n                      ...,\n                      [-0.0375,  0.0136, -0.0010,  ...,  0.0427,  0.1013,  0.0452],\n                      [-0.0930,  0.0008, -0.0090,  ..., -0.0471,  0.0172, -0.0055],\n                      [ 0.0328, -0.0387, -0.0038,  ..., -0.0710, -0.0259, -0.0446]],\n                     device='cuda:0')),\n             ('model.model.layers.27.self_attn.v_proj.weight',\n              tensor([[-0.0037, -0.0213, -0.0039,  ..., -0.0188,  0.0219, -0.0120],\n                      [-0.0185, -0.0346,  0.0307,  ..., -0.0090,  0.0074,  0.0194],\n                      [-0.0024, -0.0194, -0.0203,  ..., -0.0145, -0.0065,  0.0161],\n                      ...,\n                      [-0.0042, -0.0120, -0.0263,  ...,  0.0274, -0.0197,  0.0220],\n                      [-0.0311,  0.0083,  0.0055,  ..., -0.0094, -0.0126, -0.0046],\n                      [ 0.0114, -0.0321, -0.0196,  ..., -0.0202, -0.0134,  0.0211]],\n                     device='cuda:0')),\n             ('model.model.layers.27.self_attn.o_proj.weight',\n              tensor([[ 0.0001, -0.0015,  0.0208,  ...,  0.0060,  0.0064,  0.0172],\n                      [-0.0061,  0.0090, -0.0265,  ...,  0.0041, -0.0031, -0.0107],\n                      [ 0.0160, -0.0165, -0.0126,  ...,  0.0096,  0.0274,  0.0193],\n                      ...,\n                      [-0.0175, -0.0070, -0.0030,  ..., -0.0196, -0.0097, -0.0035],\n                      [-0.0239, -0.0100, -0.0063,  ..., -0.0205,  0.0347,  0.0142],\n                      [ 0.0126, -0.0184, -0.0042,  ..., -0.0079,  0.0012, -0.0037]],\n                     device='cuda:0')),\n             ('model.model.layers.27.mlp.gate_proj.weight',\n              tensor([[ 0.0030,  0.0417,  0.0137,  ...,  0.0035, -0.0371, -0.0089],\n                      [-0.0455,  0.0420,  0.0050,  ..., -0.0078, -0.0458,  0.0158],\n                      [ 0.0294, -0.0249,  0.0195,  ...,  0.0166, -0.0009, -0.0423],\n                      ...,\n                      [ 0.0173, -0.0207, -0.0276,  ..., -0.0016, -0.0130, -0.0142],\n                      [-0.0014, -0.0179,  0.0214,  ...,  0.0591, -0.0068, -0.0178],\n                      [ 0.0208, -0.0222,  0.0135,  ...,  0.0182, -0.0330,  0.0156]],\n                     device='cuda:0')),\n             ('model.model.layers.27.mlp.up_proj.weight',\n              tensor([[ 0.0553,  0.0087,  0.0028,  ..., -0.0024,  0.0169, -0.0125],\n                      [ 0.0390, -0.0037, -0.0317,  ..., -0.0183, -0.0358,  0.0033],\n                      [ 0.0347, -0.0034, -0.0085,  ..., -0.0244, -0.0019,  0.0088],\n                      ...,\n                      [ 0.0176, -0.0170, -0.0299,  ..., -0.0132,  0.0180,  0.0298],\n                      [-0.0053,  0.0077,  0.0446,  ...,  0.0275, -0.0278, -0.0362],\n                      [ 0.0153, -0.0335,  0.0078,  ...,  0.0097, -0.0106, -0.0194]],\n                     device='cuda:0')),\n             ('model.model.layers.27.mlp.down_proj.weight',\n              tensor([[ 0.0241,  0.0162, -0.0328,  ...,  0.0062, -0.0477,  0.0092],\n                      [ 0.0056, -0.0242,  0.0182,  ..., -0.0227,  0.0030, -0.0227],\n                      [-0.0187,  0.0034,  0.0098,  ..., -0.0029,  0.0161,  0.0016],\n                      ...,\n                      [-0.0089, -0.0033,  0.0091,  ..., -0.0285, -0.0100, -0.0218],\n                      [ 0.0075,  0.0293,  0.0140,  ...,  0.0031,  0.0409, -0.0269],\n                      [ 0.0301,  0.0017,  0.0109,  ...,  0.0267, -0.0026, -0.0242]],\n                     device='cuda:0')),\n             ('model.model.layers.27.input_layernorm.weight',\n              tensor([0.2756, 0.2922, 0.2807,  ..., 0.2733, 0.2644, 0.3077], device='cuda:0')),\n             ('model.model.layers.27.post_attention_layernorm.weight',\n              tensor([0.4083, 0.4006, 0.3991,  ..., 0.4002, 0.3930, 0.4064], device='cuda:0')),\n             ('model.model.layers.28.self_attn.q_proj.weight',\n              tensor([[ 0.0492, -0.0468, -0.0281,  ...,  0.0520,  0.0212, -0.0479],\n                      [ 0.0268, -0.0705, -0.0196,  ..., -0.0260,  0.0214,  0.0006],\n                      [-0.0154, -0.0252, -0.0178,  ..., -0.0451, -0.0061, -0.0084],\n                      ...,\n                      [ 0.0249,  0.0190,  0.0851,  ..., -0.0121, -0.0163,  0.0169],\n                      [-0.0055,  0.0472,  0.0574,  ..., -0.0053, -0.0231,  0.0529],\n                      [-0.0217,  0.0078, -0.0455,  ...,  0.0679,  0.0062, -0.0270]],\n                     device='cuda:0')),\n             ('model.model.layers.28.self_attn.k_proj.weight',\n              tensor([[-0.0142, -0.0545, -0.0487,  ..., -0.0280, -0.0039, -0.0002],\n                      [-0.0369, -0.0307,  0.0310,  ...,  0.0003, -0.0174, -0.0210],\n                      [ 0.0014,  0.0096,  0.0456,  ...,  0.0356,  0.0204,  0.0219],\n                      ...,\n                      [-0.0228,  0.0202,  0.0289,  ..., -0.0345,  0.0400,  0.0558],\n                      [-0.0099, -0.0548,  0.0477,  ..., -0.0062,  0.0097,  0.0055],\n                      [ 0.0315, -0.0077, -0.0404,  ..., -0.0235,  0.0228,  0.0442]],\n                     device='cuda:0')),\n             ('model.model.layers.28.self_attn.v_proj.weight',\n              tensor([[-0.0319, -0.0122, -0.0136,  ..., -0.0287, -0.0014,  0.0162],\n                      [ 0.0458, -0.0125,  0.0003,  ...,  0.0106, -0.0108, -0.0074],\n                      [ 0.0014, -0.0046,  0.0027,  ..., -0.0135,  0.0044, -0.0043],\n                      ...,\n                      [ 0.0216, -0.0381,  0.0054,  ..., -0.0028,  0.0166, -0.0007],\n                      [ 0.0232, -0.0127, -0.0108,  ..., -0.0010, -0.0092,  0.0119],\n                      [-0.0179, -0.0173, -0.0070,  ..., -0.0109,  0.0013,  0.0093]],\n                     device='cuda:0')),\n             ('model.model.layers.28.self_attn.o_proj.weight',\n              tensor([[-0.0160, -0.0119, -0.0014,  ..., -0.0238, -0.0002, -0.0008],\n                      [-0.0279,  0.0226, -0.0080,  ...,  0.0418,  0.0315,  0.0209],\n                      [ 0.0501,  0.0029,  0.0101,  ..., -0.0022,  0.0056,  0.0086],\n                      ...,\n                      [ 0.0342,  0.0101, -0.0076,  ..., -0.0054,  0.0069,  0.0042],\n                      [ 0.0269,  0.0021,  0.0057,  ..., -0.0234,  0.0218, -0.0006],\n                      [-0.0028, -0.0073,  0.0178,  ...,  0.0045, -0.0212,  0.0097]],\n                     device='cuda:0')),\n             ('model.model.layers.28.mlp.gate_proj.weight',\n              tensor([[-0.0030, -0.0153, -0.0281,  ...,  0.0030, -0.0238,  0.0248],\n                      [ 0.0069, -0.0413,  0.0272,  ..., -0.0011,  0.0094,  0.0130],\n                      [ 0.0090,  0.0209,  0.0007,  ..., -0.0162,  0.0059, -0.0200],\n                      ...,\n                      [ 0.0107, -0.0347, -0.0082,  ...,  0.0275,  0.0003, -0.0100],\n                      [-0.0102,  0.0121, -0.0364,  ...,  0.0362, -0.0096, -0.0074],\n                      [-0.0033, -0.0068, -0.0206,  ..., -0.0411,  0.0281,  0.0252]],\n                     device='cuda:0')),\n             ('model.model.layers.28.mlp.up_proj.weight',\n              tensor([[-0.0540, -0.0171,  0.0290,  ...,  0.0454,  0.0118,  0.0163],\n                      [ 0.0247, -0.0037, -0.0179,  ..., -0.0043, -0.0027, -0.0210],\n                      [-0.0162, -0.0065, -0.0133,  ...,  0.0221, -0.0163, -0.0364],\n                      ...,\n                      [ 0.0298, -0.0207,  0.0031,  ...,  0.0064, -0.0393, -0.0266],\n                      [ 0.0216, -0.0099, -0.0068,  ...,  0.0252, -0.0229, -0.0132],\n                      [ 0.0058, -0.0017,  0.0095,  ...,  0.0490,  0.0147,  0.0281]],\n                     device='cuda:0')),\n             ('model.model.layers.28.mlp.down_proj.weight',\n              tensor([[-0.0226,  0.0136, -0.0053,  ...,  0.0325,  0.0122, -0.0062],\n                      [ 0.0283, -0.0055, -0.0350,  ...,  0.0025, -0.0045, -0.0109],\n                      [ 0.0684, -0.0488,  0.0162,  ...,  0.0078, -0.0060, -0.0119],\n                      ...,\n                      [-0.0169, -0.0105, -0.0070,  ...,  0.0251,  0.0049, -0.0212],\n                      [ 0.0172,  0.0353,  0.0082,  ..., -0.0066, -0.0057,  0.0127],\n                      [ 0.0067, -0.0153,  0.0319,  ...,  0.0333,  0.0286,  0.0419]],\n                     device='cuda:0')),\n             ('model.model.layers.28.input_layernorm.weight',\n              tensor([0.2511, 0.2723, 0.2510,  ..., 0.2518, 0.2539, 0.2750], device='cuda:0')),\n             ('model.model.layers.28.post_attention_layernorm.weight',\n              tensor([0.4244, 0.4330, 0.4065,  ..., 0.4200, 0.4239, 0.4323], device='cuda:0')),\n             ('model.model.layers.29.self_attn.q_proj.weight',\n              tensor([[ 0.0189,  0.0074,  0.0203,  ..., -0.0290, -0.0285,  0.0223],\n                      [ 0.0007,  0.0682,  0.0024,  ..., -0.0051, -0.0590, -0.0482],\n                      [-0.0172, -0.0319, -0.0218,  ..., -0.0097, -0.0082,  0.0227],\n                      ...,\n                      [ 0.0317,  0.0354, -0.0004,  ..., -0.0042, -0.0414,  0.0370],\n                      [ 0.0173, -0.0170, -0.0402,  ...,  0.0558, -0.0493, -0.0303],\n                      [ 0.0275,  0.0052, -0.0441,  ..., -0.0143,  0.0062,  0.0617]],\n                     device='cuda:0')),\n             ('model.model.layers.29.self_attn.k_proj.weight',\n              tensor([[ 0.0178, -0.0002, -0.0004,  ..., -0.0105, -0.0235, -0.0112],\n                      [ 0.0212,  0.0169,  0.0208,  ..., -0.0124, -0.0223,  0.0130],\n                      [-0.0109,  0.0110, -0.0152,  ..., -0.0506,  0.0105,  0.0477],\n                      ...,\n                      [-0.0043,  0.0236, -0.0034,  ..., -0.0008,  0.0269,  0.0017],\n                      [ 0.0146, -0.0235,  0.0639,  ...,  0.0141, -0.0013,  0.0099],\n                      [ 0.0306,  0.0375,  0.0020,  ..., -0.0428,  0.0115, -0.0336]],\n                     device='cuda:0')),\n             ('model.model.layers.29.self_attn.v_proj.weight',\n              tensor([[ 0.0230, -0.0193, -0.0333,  ...,  0.0069, -0.0279,  0.0343],\n                      [-0.0120, -0.0003, -0.0150,  ...,  0.0004,  0.0166,  0.0096],\n                      [-0.0197, -0.0083, -0.0173,  ..., -0.0135, -0.0018,  0.0046],\n                      ...,\n                      [ 0.0351, -0.0029,  0.0435,  ...,  0.0191,  0.0041,  0.0247],\n                      [-0.0191, -0.0312, -0.0016,  ..., -0.0073,  0.0267, -0.0077],\n                      [ 0.0255, -0.0010, -0.0150,  ...,  0.0283,  0.0029, -0.0256]],\n                     device='cuda:0')),\n             ('model.model.layers.29.self_attn.o_proj.weight',\n              tensor([[-0.0221,  0.0058,  0.0196,  ..., -0.0250,  0.0151, -0.0293],\n                      [ 0.0078,  0.0038, -0.0031,  ..., -0.0149,  0.0234, -0.0137],\n                      [ 0.0050, -0.0064,  0.0135,  ..., -0.0134, -0.0007, -0.0046],\n                      ...,\n                      [ 0.0125,  0.0019,  0.0124,  ..., -0.0314,  0.0197, -0.0423],\n                      [ 0.0209, -0.0024,  0.0123,  ...,  0.0288,  0.0162,  0.0093],\n                      [ 0.0049, -0.0124,  0.0132,  ..., -0.0128,  0.0110,  0.0244]],\n                     device='cuda:0')),\n             ('model.model.layers.29.mlp.gate_proj.weight',\n              tensor([[ 0.0114, -0.0154,  0.0183,  ..., -0.0164, -0.0630,  0.0149],\n                      [-0.0167, -0.0205, -0.0207,  ..., -0.0253, -0.0187,  0.0628],\n                      [-0.0227, -0.0154, -0.0074,  ...,  0.0020,  0.0270,  0.0035],\n                      ...,\n                      [-0.0239, -0.0132,  0.0445,  ..., -0.0151,  0.0031,  0.0173],\n                      [-0.0110, -0.0062,  0.0079,  ...,  0.0361, -0.0097, -0.0509],\n                      [ 0.0310,  0.0138, -0.0319,  ...,  0.0212,  0.0029, -0.0106]],\n                     device='cuda:0')),\n             ('model.model.layers.29.mlp.up_proj.weight',\n              tensor([[-1.9907e-02, -1.8165e-02, -1.1538e-02,  ...,  3.2365e-03,\n                        1.7281e-02, -2.8849e-03],\n                      [-1.2944e-02, -3.1581e-02, -3.7033e-02,  ..., -3.9632e-02,\n                        2.9929e-02,  4.9149e-02],\n                      [ 1.1120e-02,  1.6511e-02,  1.2737e-02,  ...,  1.7633e-02,\n                       -2.8828e-02,  3.2214e-02],\n                      ...,\n                      [ 1.0329e-02, -1.5583e-02, -1.2764e-02,  ...,  1.6282e-05,\n                       -6.1364e-05,  1.7904e-02],\n                      [-3.5144e-02, -8.1598e-03, -1.1904e-02,  ..., -3.4875e-02,\n                       -1.8276e-02,  1.0505e-02],\n                      [ 1.8889e-02, -5.4876e-02,  9.6822e-03,  ...,  1.7812e-03,\n                       -2.5664e-02,  8.6466e-03]], device='cuda:0')),\n             ('model.model.layers.29.mlp.down_proj.weight',\n              tensor([[ 0.0240, -0.0022,  0.0273,  ..., -0.0048, -0.0027, -0.0051],\n                      [-0.0069,  0.0272,  0.0114,  ..., -0.0313, -0.0306,  0.0159],\n                      [ 0.0126, -0.0011,  0.0240,  ..., -0.0194,  0.0358,  0.0015],\n                      ...,\n                      [ 0.0210, -0.0141, -0.0452,  ..., -0.0093,  0.0247,  0.0146],\n                      [-0.0243,  0.0305, -0.0041,  ...,  0.0015,  0.0468,  0.0220],\n                      [-0.0005,  0.0260, -0.0002,  ...,  0.0290, -0.0120, -0.0418]],\n                     device='cuda:0')),\n             ('model.model.layers.29.input_layernorm.weight',\n              tensor([0.2661, 0.2649, 0.2541,  ..., 0.2623, 0.2673, 0.2620], device='cuda:0')),\n             ('model.model.layers.29.post_attention_layernorm.weight',\n              tensor([0.4481, 0.4613, 0.4404,  ..., 0.4565, 0.4487, 0.4683], device='cuda:0')),\n             ('model.model.norm.weight',\n              tensor([4.9726, 4.4890, 5.0897,  ..., 5.1320, 4.9673, 4.7817], device='cuda:0')),\n             ('model.lm_head.weight',\n              tensor([[-2.7064e-05, -1.6689e-02, -1.2669e-02,  ..., -2.4549e-02,\n                        4.7390e-03,  1.8498e-02],\n                      [-3.2170e-05, -1.6688e-02, -1.2669e-02,  ..., -2.4549e-02,\n                        4.7411e-03,  1.8499e-02],\n                      [-2.6027e-02, -3.9334e-02, -6.5326e-03,  ..., -6.6878e-03,\n                        4.5405e-02,  4.6173e-02],\n                      ...,\n                      [-3.1249e-02,  2.1540e-03, -1.2880e-04,  ..., -4.3672e-02,\n                       -1.6217e-02,  3.2448e-03],\n                      [-8.4461e-03, -1.7288e-02,  1.9416e-02,  ..., -7.5289e-03,\n                       -3.9414e-03,  1.1035e-03],\n                      [-2.5759e-02, -2.0721e-02, -9.3709e-03,  ..., -2.0021e-02,\n                       -1.3554e-02,  1.5376e-02]], device='cuda:0'))])"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = checkpoint['state_dict']\n",
    "state_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "pretrainLlama(\n  (model): LlamaForCausalLM(\n    (model): LlamaModel(\n      (embed_tokens): Embedding(10000, 1280)\n      (layers): ModuleList(\n        (0-29): 30 x LlamaDecoderLayer(\n          (self_attn): LlamaFlashAttention2(\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (o_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (rotary_emb): LlamaRotaryEmbedding()\n          )\n          (mlp): LlamaMLP(\n            (gate_proj): Linear(in_features=1280, out_features=3440, bias=False)\n            (up_proj): Linear(in_features=1280, out_features=3440, bias=False)\n            (down_proj): Linear(in_features=3440, out_features=1280, bias=False)\n            (act_fn): SiLUActivation()\n          )\n          (input_layernorm): LlamaRMSNorm()\n          (post_attention_layernorm): LlamaRMSNorm()\n        )\n      )\n      (norm): LlamaRMSNorm()\n    )\n    (lm_head): Linear(in_features=1280, out_features=10000, bias=False)\n  )\n)"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 664, 663, 2]"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer.encode('AAK')\n",
    "t = [tokenizer.bos_id()] + t + [tokenizer.eos_id()]\n",
    "t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "'AAK'"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "<sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x2aafd6133420> >"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test tokenizer\n",
    "tokenizer = model.tokenizer\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_id()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "[664, 663]"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('AAK')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "'AAK'"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([664,663])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[tokenizer.bos_id()] + tokenizer.encode(x) + [tokenizer.eos_id()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizerFast, LlamaForCausalLM\n",
    "tokenizer = LlamaTokenizerFast('/data/rozen/home/e0833634/lama/protllama/batch_script/protein_10k.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "prompt = \"YAPSALVLTVGKGVSATTAAPERAVTLTCAPGPSGTHPAAGSACADLAAVGGDLNALTRGEDVMCPMVYDPVLLTVDGVWQGKRVSYERVFSNECEMNAHGSSVFAF\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[   1,  664, 3656,  484,   28,   56, 8333, 1076,   39,  808,  743,  499,\n         6070, 6303, 3565,  418, 4297,  375,  369, 1301, 1134,  980, 7270,  353,\n          393, 2069, 7160, 8569,    5,  426, 6185,  227,  995, 2420, 6438, 4638,\n         3594, 1879,  157]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'pretrainLlama' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [157]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m generate_ids \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m(inputs\u001B[38;5;241m.\u001B[39minput_ids, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1614\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1612\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1613\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1614\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1615\u001B[0m     \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'pretrainLlama' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "generate_ids = model.generate(inputs.input_ids, max_length=30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "data": {
      "text/plain": "'AAK'"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from torch.cuda.amp import autocast\n",
    "import numpy as np\n",
    "from typing import List, Literal, Optional, Tuple, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "\n",
    "class Message(TypedDict):\n",
    "    role: Role\n",
    "    content: str\n",
    "\n",
    "\n",
    "class CompletionPrediction(TypedDict, total=False):\n",
    "    generation: str\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n",
    "\n",
    "\n",
    "class ChatPrediction(TypedDict, total=False):\n",
    "    generation: Message\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n",
    "\n",
    "\n",
    "Dialog = List[Message]\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "SPECIAL_TAGS = [B_INST, E_INST, \"<<SYS>>\", \"<</SYS>>\"]\n",
    "UNSAFE_ERROR = \"Error: special tags are not allowed as part of the prompt.\"\n",
    "\n",
    "\n",
    "class Llama:\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "        model_parallel_size: Optional[int] = None,\n",
    "    ) -> \"Llama\":\n",
    "        checkpoint = torch.load(\n",
    "            '/data/rozen/home/e0833634/lama/protllama/pl_model_cache/epoch=23-train_perplexity=1.161-val_perplexity=255.593-ppi_10_26_10k_2048.ckpt')\n",
    "        hyper_parameters = checkpoint[\"hyper_parameters\"]\n",
    "        original_hparam = hyper_parameters['hparam']\n",
    "\n",
    "        new_hparam = Namespace(\n",
    "            accumulate_grad_batches=original_hparam.accumulate_grad_batches,\n",
    "            attempts=original_hparam.attempts,\n",
    "            batch_size=original_hparam.batch_size,\n",
    "            date=original_hparam.date,\n",
    "            devices=original_hparam.devices,\n",
    "            epoch=original_hparam.epoch,\n",
    "            flash_attention=original_hparam.flash_attention,\n",
    "            hidden_size=original_hparam.hidden_size,\n",
    "            input_dataset_path=original_hparam.input_dataset_path,\n",
    "            intermediate_size=original_hparam.intermediate_size,\n",
    "            learning_rate=original_hparam.learning_rate,\n",
    "            max_position_embeddings=original_hparam.max_position_embeddings,\n",
    "            num_attention_heads=original_hparam.num_attention_heads,\n",
    "            num_hidden_layers=original_hparam.num_hidden_layers,\n",
    "            num_key_value_heads=original_hparam.num_key_value_heads,\n",
    "            num_workers=original_hparam.num_workers,\n",
    "            output_dataset_path=original_hparam.output_dataset_path,\n",
    "            save_top_k=original_hparam.save_top_k,\n",
    "            scheduler=original_hparam.scheduler,\n",
    "            strategy=original_hparam.strategy,\n",
    "            target=original_hparam.target,\n",
    "            tokenizer_path='/data/rozen/home/e0833634/lama/protllama/batch_script/',  # Update the tokenizer_path here\n",
    "            train_dataloader_length=original_hparam.train_dataloader_length,\n",
    "            vocab_size=original_hparam.vocab_size,\n",
    "\n",
    "            max_batch_size=max_batch_size,\n",
    "            max_seq_len=max_seq_len\n",
    "            )\n",
    "\n",
    "        # Update the hyper_parameters with the new Namespace\n",
    "        hyper_parameters['hparam'] = new_hparam\n",
    "        model = pretrainLlama(**hyper_parameters)\n",
    "        model.configure_model()\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        model.load_state_dict(state_dict)\n",
    "        model = model.cuda()\n",
    "        tokenizer = model.tokenizer\n",
    "\n",
    "        return Llama(model, tokenizer)\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tokens: List[List[int]],\n",
    "        max_gen_len: int,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n",
    "        params = self.model.hparam\n",
    "        bsz = len(prompt_tokens)\n",
    "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "        min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "        max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "        assert max_prompt_len <= params.max_seq_len\n",
    "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        pad_id = self.tokenizer.unk_id() #original pad is -1, make it equals to unk to make the id to 0\n",
    "        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "        if logprobs:\n",
    "            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "\n",
    "        prev_pos = 0\n",
    "        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "        input_text_mask = tokens != pad_id\n",
    "        if min_prompt_len == total_len:\n",
    "            #logits = self.model.forward(tokens, prev_pos)\n",
    "            logits = self.model.forward(tokens)\n",
    "            token_logprobs = -F.cross_entropy(\n",
    "                input=logits.transpose(1, 2),\n",
    "                target=tokens,\n",
    "                reduction=\"none\",\n",
    "                ignore_index=pad_id,\n",
    "            )\n",
    "\n",
    "        for cur_pos in range(min_prompt_len, total_len):\n",
    "            print(tokens[:, prev_pos:cur_pos])\n",
    "            with autocast():\n",
    "                logits = self.model.forward(input_ids=tokens[:, prev_pos:cur_pos])[0]\n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if prompt has already been generated\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            if logprobs:\n",
    "                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "                    input=logits.transpose(1, 2),\n",
    "                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "                    reduction=\"none\",\n",
    "                    ignore_index=pad_id,\n",
    "                )\n",
    "            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "                next_token == self.tokenizer.eos_id()\n",
    "            )\n",
    "            prev_pos = cur_pos\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        if logprobs:\n",
    "            token_logprobs = token_logprobs.tolist()\n",
    "        out_tokens, out_logprobs = [], []\n",
    "        for i, toks in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            start = 0 if echo else len(prompt_tokens[i])\n",
    "            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            probs = None\n",
    "            if logprobs:\n",
    "                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to eos tok if any\n",
    "            #if self.tokenizer.eos_id() in toks:\n",
    "                #eos_idx = toks.index(self.tokenizer.eos_id())\n",
    "                #toks = toks[:eos_idx]\n",
    "                #probs = probs[:eos_idx] if logprobs else None\n",
    "            out_tokens.append(toks)\n",
    "            out_logprobs.append(probs)\n",
    "        return (out_tokens, out_logprobs if logprobs else None)\n",
    "\n",
    "    def text_completion(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> List[CompletionPrediction]:\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.model.hparam.max_seq_len - 1\n",
    "        #prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=True) for x in prompts]\n",
    "        prompt_tokens = []\n",
    "        for x in prompts:\n",
    "            t = [self.tokenizer.bos_id()] + self.tokenizer.encode(x) + [self.tokenizer.eos_id()]\n",
    "            prompt_tokens.append(t)\n",
    "        generation_tokens, generation_logprobs = self.generate(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            logprobs=logprobs,\n",
    "            echo=echo,\n",
    "        )\n",
    "        if logprobs:\n",
    "            return [\n",
    "                {\n",
    "                    \"generation\": self.tokenizer.decode(t),\n",
    "                    \"tokens\": [self.tokenizer.decode(x) for x in t],\n",
    "                    \"logprobs\": logprobs_i,\n",
    "                }\n",
    "                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "            ]\n",
    "        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]\n",
    "\n",
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1280,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3440,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 20,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 10000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generator = Llama.build(\n",
    "    max_seq_len=2048,\n",
    "    max_batch_size=2,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "prompts: List[str] = [\n",
    "        # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "    \"YAPSALVLTVGKGVSATTAAPERAVTLTCAPGPSGTHPAAGSACADLAAVGGDLNALTRGEDVMCPMVYDPVLLTVDGVWQGKRVSYERVFSNECEMNAHGSSVFAF\",\n",
    "    \"DFVLDNEGNPLENGGTYYILSDITAFGGIRAAPTGNERCPLTVVQSRNELDKGIGTIISSPYRIRFIAEGHPLSLKFDSFAVIMLCVGIPTEWSVVEDLPEGPAVKIGENKDAMDGWFRLERVSDDEFNNYKLVFCPQKCGDIGISIDHDDGTRRLVVSKNKPLVVQFQKLD\"\n",
    "        # Few shot prompt (providing a few examples before asking model to complete more);\n",
    "        #\"\"\"Translate English to French:\n",
    "\n",
    "        #sea otter => loutre de mer\n",
    "        #peppermint => menthe poivr√©e\n",
    "        #plush girafe => girafe peluche\n",
    "        #cheese =>\"\"\",\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,  664, 3656,  484,   28,   56, 8333, 1076,   39,  808,  743,  499,\n",
      "         6070, 6303, 3565,  418, 4297,  375,  369, 1301, 1134,  980, 7270,  353,\n",
      "          393, 2069, 7160, 8569,    5,  426, 6185,  227,  995, 2420, 6438, 4638,\n",
      "         3594, 1879,  157,    2],\n",
      "        [   1,  664, 1160, 2385,  122, 1198, 1089, 2401,  263,  456,  975,  579,\n",
      "          146, 1151, 2361, 4953, 9992, 3219, 4362,  689,  681, 2696, 1046, 5014,\n",
      "         4033,  608, 2403, 1649, 2860, 2912,  256, 6626, 1108, 7041,   52, 1512,\n",
      "         6008, 1482, 9014, 2947]], device='cuda:0')\n",
      "tensor([[ 259],\n",
      "        [5047]], device='cuda:0')\n",
      "tensor([[4493],\n",
      "        [6512]], device='cuda:0')\n",
      "tensor([[4801],\n",
      "        [  67]], device='cuda:0')\n",
      "tensor([[2208],\n",
      "        [ 995]], device='cuda:0')\n",
      "tensor([[4806],\n",
      "        [6643]], device='cuda:0')\n",
      "tensor([[1742],\n",
      "        [4099]], device='cuda:0')\n",
      "tensor([[5389],\n",
      "        [4243]], device='cuda:0')\n",
      "tensor([[9288],\n",
      "        [ 172]], device='cuda:0')\n",
      "tensor([[9186],\n",
      "        [6795]], device='cuda:0')\n",
      "tensor([[4493],\n",
      "        [8627]], device='cuda:0')\n",
      "tensor([[4493],\n",
      "        [5130]], device='cuda:0')\n",
      "tensor([[4143],\n",
      "        [2172]], device='cuda:0')\n",
      "tensor([[5301],\n",
      "        [5749]], device='cuda:0')\n",
      "tensor([[1000],\n",
      "        [1747]], device='cuda:0')\n",
      "tensor([[7085],\n",
      "        [ 564]], device='cuda:0')\n",
      "tensor([[3542],\n",
      "        [ 716]], device='cuda:0')\n",
      "tensor([[2966],\n",
      "        [ 727]], device='cuda:0')\n",
      "tensor([[7133],\n",
      "        [9508]], device='cuda:0')\n",
      "tensor([[1742],\n",
      "        [5741]], device='cuda:0')\n",
      "tensor([[8899],\n",
      "        [1412]], device='cuda:0')\n",
      "tensor([[9288],\n",
      "        [   2]], device='cuda:0')\n",
      "tensor([[2746],\n",
      "        [6872]], device='cuda:0')\n",
      "tensor([[5949],\n",
      "        [1742]], device='cuda:0')\n",
      "tensor([[2966],\n",
      "        [4801]], device='cuda:0')\n",
      "tensor([[8657],\n",
      "        [ 329]], device='cuda:0')\n",
      "tensor([[2966],\n",
      "        [8769]], device='cuda:0')\n",
      "tensor([[1742],\n",
      "        [4138]], device='cuda:0')\n",
      "tensor([[4801],\n",
      "        [4493]], device='cuda:0')\n",
      "tensor([[4493],\n",
      "        [1742]], device='cuda:0')\n",
      "tensor([[4493],\n",
      "        [9288]], device='cuda:0')\n",
      "tensor([[7085],\n",
      "        [9288]], device='cuda:0')\n",
      "tensor([[4138],\n",
      "        [9119]], device='cuda:0')\n",
      "tensor([[4493],\n",
      "        [9288]], device='cuda:0')\n",
      "tensor([[6926],\n",
      "        [9119]], device='cuda:0')\n",
      "tensor([[4521],\n",
      "        [9119]], device='cuda:0')\n",
      "tensor([[7085],\n",
      "        [9288]], device='cuda:0')\n",
      "tensor([[3264],\n",
      "        [1742]], device='cuda:0')\n",
      "tensor([[8769],\n",
      "        [9288]], device='cuda:0')\n",
      "tensor([[7610],\n",
      "        [7085]], device='cuda:0')\n",
      "tensor([[8769],\n",
      "        [4957]], device='cuda:0')\n",
      "tensor([[1633],\n",
      "        [4044]], device='cuda:0')\n",
      "tensor([[7267],\n",
      "        [1742]], device='cuda:0')\n",
      "tensor([[4307],\n",
      "        [9288]], device='cuda:0')\n",
      "tensor([[3241],\n",
      "        [6041]], device='cuda:0')\n",
      "tensor([[3264],\n",
      "        [1869]], device='cuda:0')\n",
      "tensor([[6428],\n",
      "        [4493]], device='cuda:0')\n",
      "tensor([[3241],\n",
      "        [7085]], device='cuda:0')\n",
      "tensor([[7403],\n",
      "        [3264]], device='cuda:0')\n",
      "tensor([[1742],\n",
      "        [3794]], device='cuda:0')\n",
      "tensor([[4867],\n",
      "        [4493]], device='cuda:0')\n",
      "tensor([[1869],\n",
      "        [6816]], device='cuda:0')\n",
      "tensor([[1742],\n",
      "        [5301]], device='cuda:0')\n",
      "tensor([[4801],\n",
      "        [7403]], device='cuda:0')\n",
      "tensor([[4493],\n",
      "        [1742]], device='cuda:0')\n",
      "tensor([[3329],\n",
      "        [4867]], device='cuda:0')\n",
      "tensor([[7133],\n",
      "        [7085]], device='cuda:0')\n",
      "tensor([[4798],\n",
      "        [1869]], device='cuda:0')\n",
      "tensor([[9288],\n",
      "        [4307]], device='cuda:0')\n",
      "tensor([[9119],\n",
      "        [6926]], device='cuda:0')\n",
      "tensor([[9288],\n",
      "        [4307]], device='cuda:0')\n",
      "tensor([[5932],\n",
      "        [4180]], device='cuda:0')\n",
      "tensor([[8985],\n",
      "        [6444]], device='cuda:0')\n",
      "tensor([[9288],\n",
      "        [7085]], device='cuda:0')\n",
      "tensor([[4403],\n",
      "        [1869]], device='cuda:0')\n",
      "tensor([[4493],\n",
      "        [4493]], device='cuda:0')\n",
      "tensor([[9274],\n",
      "        [7085]], device='cuda:0')\n",
      "tensor([[1742],\n",
      "        [6424]], device='cuda:0')\n",
      "tensor([[7924],\n",
      "        [2189]], device='cuda:0')\n",
      "tensor([[1742],\n",
      "        [7085]], device='cuda:0')\n",
      "tensor([[4867],\n",
      "        [1742]], device='cuda:0')\n",
      "tensor([[9198],\n",
      "        [7085]], device='cuda:0')\n",
      "tensor([[7085],\n",
      "        [3264]], device='cuda:0')\n",
      "tensor([[1869],\n",
      "        [8769]], device='cuda:0')\n",
      "tensor([[1742],\n",
      "        [9288]], device='cuda:0')\n",
      "tensor([[9051],\n",
      "        [1742]], device='cuda:0')\n",
      "tensor([[5704],\n",
      "        [7267]], device='cuda:0')\n",
      "tensor([[9437],\n",
      "        [4307]], device='cuda:0')\n",
      "tensor([[7085],\n",
      "        [9559]], device='cuda:0')\n",
      "tensor([[5972],\n",
      "        [4493]], device='cuda:0')\n",
      "tensor([[8094],\n",
      "        [4867]], device='cuda:0')\n",
      "tensor([[5468],\n",
      "        [8094]], device='cuda:0')\n",
      "tensor([[1742],\n",
      "        [5468]], device='cuda:0')\n",
      "tensor([[3994],\n",
      "        [4307]], device='cuda:0')\n",
      "tensor([[5655],\n",
      "        [9288]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "results = generator.text_completion(\n",
    "        prompts,\n",
    "        max_gen_len=64,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        echo=True\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAPSALVLTVGKGVSATTAAPERAVTLTCAPGPSGTHPAAGSACADLAAVGGDLNALTRGEDVMCPMVYDPVLLTVDGVWQGKRVSYERVFSNECEMNAHGSSVFAF\n",
      "> YAPSALVLTVGKGVSATTAAPERAVTLTCAPGPSGTHPAAGSACADLAAVGGDLNALTRGEDVMCPMVYDPVLLTVDGVWQGKRVSYERVFSNECEMNAHGSSVFAF MAQTEPAHYTGRNQFSDPIDTSSTSLSGQTEQTEEECIRIIPALRKLPVKQVSLEGLFSDCQSTSSTELNGMNQVSEELDQVSFSDPAHQTEQTELRKLTVYQTEIHHAAWLRKLKGTTPTPSHKTPTPGYTDSHMVRPENKGTSLSLPENPKWFSDHSDVPFFSDPAHQTEAQELEGLQSTTSSTVAQATSSTNKGSGASTSSTFPI\n",
      "\n",
      "==================================\n",
      "\n",
      "DFVLDNEGNPLENGGTYYILSDITAFGGIRAAPTGNERCPLTVVQSRNELDKGIGTIISSPYRIRFIAEGHPLSLKFDSFAVIMLCVGIPTEWSVVEDLPEGPAVKIGENKDAMDGWFRLERVSDDEFNNYKLVFCPQKCGDIGISIDHDDGTRRLVVSKNKPLVVQFQKLD\n",
      "> DFVLDNEGNPLENGGTYYILSDITAFGGIRAAPTGNERCPLTVVQSRNELDKGIGTIISSPYRIRFIAEGHPLSLKFDSFAVIMLCVGIPTEWSVVEDLPEGPAVKIGENKDAMDGWFRLERVSDDEFNNYKLVFCPQKCGDIGISIDHDDGTRRLVVSKNKPLVVQFQKLD MSQFSDPAHHPTPTPTVYQTEFSDTSSTTSSTVAQATSSTVAQAVAQATSSTFSDTSSTLRKLTSYQGYFSDTSSTNLMVPFQTELRKLKGTKVPQTEPNRIRIPKWFSDHSDLRKLVPFMVRIHHMVRMTGVPGGLRKLVPFQTELRKLHKRAITLRKLFSDLRKLKGTTPTPTSSTFSDDSHMVRGDGVQTEHSDCKSWLTMVRTSSTVAQA\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt, result in zip(prompts, results):\n",
    "    print(prompt)\n",
    "    print(f\"> {result['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "data": {
      "text/plain": "[1,\n 664,\n 3656,\n 484,\n 28,\n 56,\n 8333,\n 1076,\n 39,\n 808,\n 743,\n 499,\n 6070,\n 6303,\n 3565,\n 418,\n 4297,\n 375,\n 369,\n 1301,\n 1134,\n 980,\n 7270,\n 353,\n 393,\n 2069,\n 7160,\n 8569,\n 5,\n 426,\n 6185,\n 227,\n 995,\n 2420,\n 6438,\n 4638,\n 3594,\n 1879,\n 157]"
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer.encode('YAPSALVLTVGKGVSATTAAPERAVTLTCAPGPSGTHPAAGSACADLAAVGGDLNALTRGEDVMCPMVYDPVLLTVDGVWQGKRVSYERVFSNECEMNAHGSSVFAF')\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "data": {
      "text/plain": "39"
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "data": {
      "text/plain": "104"
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_id()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "full() received an invalid combination of arguments - got (tuple, method, device=str, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, Number fill_value, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [123]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m tokens \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfull\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m104\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munk_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlong\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m tokens\n",
      "\u001B[0;31mTypeError\u001B[0m: full() received an invalid combination of arguments - got (tuple, method, device=str, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, Number fill_value, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, Number fill_value, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "tokens = torch.full((1, 104), tokenizer.unk_id, dtype=torch.long, device=\"cuda\")\n",
    "tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import sentencepiece as spm\n",
    "from typing import List, Literal, Optional, Tuple, TypedDict\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CompletionPrediction(TypedDict, total=False):\n",
    "    generation: str\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n",
    "\n",
    "class TextCompletionModel(pl.LightningModule):\n",
    "    \"\"\"this class is used for inference only\"\"\"\n",
    "    def __init__(self,\n",
    "                 ckpt_dir: str,\n",
    "                 #max_gen_len: int,\n",
    "                 hparam,\n",
    "                 tokenizer_path: str,\n",
    "                 #max_seq_len: int,\n",
    "                 #max_position_embeddings: int,\n",
    "                 #temperature: float = 0.6,\n",
    "                 #top_p: float = 0.9,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = self.load_from_checkpoint(ckpt_dir)\n",
    "        self.hparam = hparam\n",
    "        #self.max_gen_len = max_gen_len\n",
    "        #self.max_seq_len = max_seq_len\n",
    "        #self.max_position_embeddings = max_position_embeddings\n",
    "        self.tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path + \"protein_%s.model\" % ('10k'))\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        \"\"\" Pytorch forward function\n",
    "        input: huggingface datasets object containing [input_ids, labels, attention_masks]\n",
    "        Returns:\n",
    "        dict with model outputs (loss, logits, hidden layer, attention)\n",
    "        \"\"\"\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def inference(self, prompt_tokens: List[List[int]],\n",
    "                        max_gen_len: int,\n",
    "                        temperature: float,\n",
    "                        top_p: float,\n",
    "                        echo: bool,\n",
    "                        logprobs: bool = False):\n",
    "        # Inference method for generating completions\n",
    "        bsz = len(prompt_tokens)\n",
    "        assert bsz <= self.hparam.max_position_embeddings, (bsz, self.hparam.max_position_embeddings)\n",
    "\n",
    "        min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "        max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "        assert max_prompt_len <= self.hparam.max_position_embeddings\n",
    "        total_len = min(self.hparam.max_position_embeddings, max_gen_len + max_prompt_len)\n",
    "        \n",
    "        pad_id = self.tokenizer.pad_id\n",
    "        print(bsz, total_len)\n",
    "        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "        if logprobs:\n",
    "            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "        prev_pos = 0\n",
    "        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "        input_text_mask = tokens != pad_id\n",
    "        if min_prompt_len == total_len:\n",
    "            logits = self.forward(tokens, prev_pos)\n",
    "            token_logprobs = -F.cross_entropy(\n",
    "                input=logits.transpose(1, 2),\n",
    "                target=tokens,\n",
    "                reduction=\"none\",\n",
    "                ignore_index=pad_id,\n",
    "            )\n",
    "\n",
    "        for cur_pos in range(min_prompt_len, total_len):\n",
    "            logits = self.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if prompt has already been generated\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            if logprobs:\n",
    "                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "                    input=logits.transpose(1, 2),\n",
    "                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "                    reduction=\"none\",\n",
    "                    ignore_index=pad_id,\n",
    "                )\n",
    "            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "                next_token == self.tokenizer.eos_id\n",
    "            )\n",
    "            prev_pos = cur_pos\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "                \n",
    "        if logprobs:\n",
    "            token_logprobs = token_logprobs.tolist()\n",
    "            \n",
    "        out_tokens, out_logprobs = [], []\n",
    "        for i, toks in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            start = 0 if echo else len(prompt_tokens[i])\n",
    "            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            probs = None\n",
    "            if logprobs:\n",
    "                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to eos tok if any\n",
    "            if self.tokenizer.eos_id in toks:\n",
    "                eos_idx = toks.index(self.tokenizer.eos_id)\n",
    "                toks = toks[:eos_idx]\n",
    "                probs = probs[:eos_idx] if logprobs else None\n",
    "            out_tokens.append(toks)\n",
    "            out_logprobs.append(probs)\n",
    "        return (out_tokens, out_logprobs if logprobs else None)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        pass\n",
    "    \n",
    "    def predict_step(self, batch, batch_nb: int, logprobs: bool = True) -> List[CompletionPrediction]:\n",
    "        max_gen_len = self.hparam.max_gen_len\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.param.max_position_embeddings - 1\n",
    "        temperature = self.hparam.temperature\n",
    "        top_p = self.hparam.top_p\n",
    "        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=True) for x in prompts]\n",
    "        modified_prompt_tokens = [batch + [self.tokenizer.eos_id] + B_token for A_token, B_token in prompt_tokens]\n",
    "        generation_tokens, generation_logprobs = self.inference(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            logprobs=True   ,\n",
    "            echo=True,\n",
    "        )\n",
    "        if logprobs:\n",
    "            return [\n",
    "                {\n",
    "                    \"generation\": self.tokenizer.decode(t),\n",
    "                    \"tokens\": [self.tokenizer.decode(x) for x in t],\n",
    "                    \"logprobs\": logprobs_i,\n",
    "                }\n",
    "                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "            ]\n",
    "        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]\n",
    "def sample_top_p(probs, p):\n",
    "    \"\"\"\n",
    "    Perform top-p (nucleus) sampling on a probability distribution.\n",
    "\n",
    "    Args:\n",
    "        probs (torch.Tensor): Probability distribution tensor.\n",
    "        p (float): Probability threshold for top-p sampling.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Sampled token indices.\n",
    "\n",
    "    Note:\n",
    "        Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n",
    "        exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n",
    "\n",
    "    \"\"\"\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/rozen/home/e0833634/lama/protllama/notebooks/checkpoint.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [103]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Model loading\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mTextCompletionModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_from_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcheckpoint.ckpt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/core/module.py:1543\u001B[0m, in \u001B[0;36mLightningModule.load_from_checkpoint\u001B[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001B[0m\n\u001B[1;32m   1463\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m   1464\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_from_checkpoint\u001B[39m(\n\u001B[1;32m   1465\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1470\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m   1471\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Self:\n\u001B[1;32m   1472\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1473\u001B[0m \u001B[38;5;124;03m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001B[39;00m\n\u001B[1;32m   1474\u001B[0m \u001B[38;5;124;03m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1541\u001B[0m \u001B[38;5;124;03m        y_hat = pretrained_model(x)\u001B[39;00m\n\u001B[1;32m   1542\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1543\u001B[0m     loaded \u001B[38;5;241m=\u001B[39m \u001B[43m_load_from_checkpoint\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1544\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1545\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1546\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1547\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhparams_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1548\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstrict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1549\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1550\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Self, loaded)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:63\u001B[0m, in \u001B[0;36m_load_from_checkpoint\u001B[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_from_checkpoint\u001B[39m(\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28mcls\u001B[39m: Union[Type[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m], Type[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningDataModule\u001B[39m\u001B[38;5;124m\"\u001B[39m]],\n\u001B[1;32m     56\u001B[0m     checkpoint_path: Union[_PATH, IO],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m     61\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningDataModule\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m pl_legacy_patch():\n\u001B[0;32m---> 63\u001B[0m         checkpoint \u001B[38;5;241m=\u001B[39m \u001B[43mpl_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmap_location\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;66;03m# convert legacy checkpoints to the new format\u001B[39;00m\n\u001B[1;32m     66\u001B[0m     checkpoint \u001B[38;5;241m=\u001B[39m _pl_migrate_checkpoint(\n\u001B[1;32m     67\u001B[0m         checkpoint, checkpoint_path\u001B[38;5;241m=\u001B[39m(checkpoint_path \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(checkpoint_path, (\u001B[38;5;28mstr\u001B[39m, Path)) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     68\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/lightning_fabric/utilities/cloud_io.py:51\u001B[0m, in \u001B[0;36m_load\u001B[0;34m(path_or_url, map_location)\u001B[0m\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mhub\u001B[38;5;241m.\u001B[39mload_state_dict_from_url(\n\u001B[1;32m     47\u001B[0m         \u001B[38;5;28mstr\u001B[39m(path_or_url),\n\u001B[1;32m     48\u001B[0m         map_location\u001B[38;5;241m=\u001B[39mmap_location,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m     49\u001B[0m     )\n\u001B[1;32m     50\u001B[0m fs \u001B[38;5;241m=\u001B[39m get_filesystem(path_or_url)\n\u001B[0;32m---> 51\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_or_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mload(f, map_location\u001B[38;5;241m=\u001B[39mmap_location)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/fsspec/spec.py:1241\u001B[0m, in \u001B[0;36mAbstractFileSystem.open\u001B[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001B[0m\n\u001B[1;32m   1239\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1240\u001B[0m     ac \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mautocommit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_intrans)\n\u001B[0;32m-> 1241\u001B[0m     f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1243\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1244\u001B[0m \u001B[43m        \u001B[49m\u001B[43mblock_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mblock_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1245\u001B[0m \u001B[43m        \u001B[49m\u001B[43mautocommit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mac\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1246\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1247\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1248\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1249\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1250\u001B[0m         \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfsspec\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompression\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compr\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/fsspec/implementations/local.py:184\u001B[0m, in \u001B[0;36mLocalFileSystem._open\u001B[0;34m(self, path, mode, block_size, **kwargs)\u001B[0m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_mkdir \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmakedirs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parent(path), exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 184\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mLocalFileOpener\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/fsspec/implementations/local.py:315\u001B[0m, in \u001B[0;36mLocalFileOpener.__init__\u001B[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001B[0m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression \u001B[38;5;241m=\u001B[39m get_compression(path, compression)\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocksize \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mDEFAULT_BUFFER_SIZE\n\u001B[0;32m--> 315\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/fsspec/implementations/local.py:320\u001B[0m, in \u001B[0;36mLocalFileOpener._open\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf\u001B[38;5;241m.\u001B[39mclosed:\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautocommit \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m--> 320\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression:\n\u001B[1;32m    322\u001B[0m             compress \u001B[38;5;241m=\u001B[39m compr[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression]\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/data/rozen/home/e0833634/lama/protllama/notebooks/checkpoint.ckpt'"
     ]
    }
   ],
   "source": [
    "# Model loading\n",
    "model = TextCompletionModel.load_from_checkpoint('checkpoint.ckpt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    def freeze(self) -> None:\n",
    "        \"\"\"freeze entire model\"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._frozen = True\n",
    "\n",
    "    def freeze_layers(self) -> None:\n",
    "        \"\"\"freeze parameters\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            # https://stackoverflow.com/questions/6531482/how-to-check-if-a-string-contains-an-element-from-a-list-in-python\n",
    "            # frozen_layers by default is 30\n",
    "            try:\n",
    "                if int(name.split('.')[1]) <= self.hparam.frozen_layers:\n",
    "                    param.requires_grad = False\n",
    "                    print(name, param)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        self._frozen = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}