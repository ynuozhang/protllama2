Global seed set to 42
Seed set to 42
/home/a03-yzhang/.local/lib/python3.8/site-packages/lightning/fabric/connector.py:565: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
[rank: 1] Global seed set to 42
[rank: 1] Seed set to 42
[rank: 2] Global seed set to 42
[rank: 2] Seed set to 42
[rank: 3] Global seed set to 42
[rank: 3] Seed set to 42
[rank: 1] Seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
[rank: 4] Global seed set to 42
[rank: 4] Seed set to 42
[rank: 2] Seed set to 42
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
[rank: 3] Seed set to 42
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
[rank: 4] Seed set to 42
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
[rank: 6] Global seed set to 42
[rank: 5] Global seed set to 42
[rank: 6] Seed set to 42
[rank: 5] Seed set to 42
[rank: 7] Global seed set to 42
[rank: 7] Seed set to 42
[rank: 6] Seed set to 42
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
[rank: 5] Seed set to 42
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
[rank: 7] Seed set to 42
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
wandb: Currently logged in as: ynuozhang (ynuoteam). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in pretrain_protllama_ppi/pl_logs/wandb/run-20231023_030827-version_7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppi_10_23_10k_pre-training_log
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ynuoteam/pretrain_protllama_ppi
wandb: üöÄ View run at https://wandb.ai/ynuoteam/pretrain_protllama_ppi/runs/version_7
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
/home/a03-yzhang/.local/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory /home/a03-yzhang/projects/protllama2/batch_script/pretrain_protllama_ppi/pl_model_cache exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]

  | Name  | Type             | Params
-------------------------------------------
0 | model | LlamaForCausalLM | 618 M 
-------------------------------------------
618 M     Trainable params
0         Non-trainable params
618 M     Total params
2,474.296 Total estimated model params size (MB)
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 10000
}

Initializing dataset...
Initializing dataset...
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 10000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 10000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 10000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 10000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 10000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 10000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 10000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:51<00:00,  0.04it/s]                                                                           Building training dataloader
Building training dataloader
Building training dataloader
Building training dataloader
Building training dataloader
Building training dataloader
Building training dataloader
Building training dataloader
/home/a03-yzhang/.local/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/22 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/22 [00:00<?, ?it/s] Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 10/22 [30:51<37:02,  0.01it/s]Epoch 0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 10/22 [30:51<37:02,  0.01it/s, v_num=on_7, train_loss_step=9.470, train_perplexity_step=1.3e+4, train_accuracy_step=0.000147]Epoch 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 20/22 [46:54<04:41,  0.01it/s, v_num=on_7, train_loss_step=9.470, train_perplexity_step=1.3e+4, train_accuracy_step=0.000147]Epoch 0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 20/22 [46:55<04:41,  0.01it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.29e+4, train_accuracy_step=0.000165]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [53:53<00:00,  0.01it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.29e+4, train_accuracy_step=0.000165]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [53:53<00:00,  0.01it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.29e+4, train_accuracy_step=0.000]   
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:51<00:00,  0.04it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [56:49<00:00,  0.01it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.29e+4, train_accuracy_step=0.000, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.3e+4, val_accuracy_epoch=9.79e-5]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [56:49<00:00,  0.01it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.29e+4, train_accuracy_step=0.000, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.3e+4, val_accuracy_epoch=9.79e-5, train_loss_epoch=9.500, train_perplexity_epoch=1.29e+4, train_accuracy_epoch=6.9e-5]Epoch 0, global step 3: 'val_perplexity' reached 12969.94141 (best 12969.94141), saving model to '/home/a03-yzhang/projects/protllama2/batch_script/pretrain_protllama_ppi/pl_model_cache/epoch=0-train_perplexity=12943.333-val_perplexity=12969.941-ppi_10_23_10k_2048.ckpt' as top 1
Epoch 0:   0%|          | 0/22 [00:00<?, ?it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.29e+4, train_accuracy_step=0.000, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.3e+4, val_accuracy_epoch=9.79e-5, train_loss_epoch=9.500, train_perplexity_epoch=1.29e+4, train_accuracy_epoch=6.9e-5]         Epoch 1:   0%|          | 0/22 [00:00<?, ?it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.29e+4, train_accuracy_step=0.000, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.3e+4, val_accuracy_epoch=9.79e-5, train_loss_epoch=9.500, train_perplexity_epoch=1.29e+4, train_accuracy_epoch=6.9e-5]Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 10/22 [48:32<58:14,  0.00it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.29e+4, train_accuracy_step=0.000, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.3e+4, val_accuracy_epoch=9.79e-5, train_loss_epoch=9.500, train_perplexity_epoch=1.29e+4, train_accuracy_epoch=6.9e-5]Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 10/22 [48:32<58:14,  0.00it/s, v_num=on_7, train_loss_step=9.480, train_perplexity_step=1.3e+4, train_accuracy_step=0.000, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.3e+4, val_accuracy_epoch=9.79e-5, train_loss_epoch=9.500, train_perplexity_epoch=1.29e+4, train_accuracy_epoch=6.9e-5] Epoch 1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 20/22 [56:35<05:39,  0.01it/s, v_num=on_7, train_loss_step=9.480, train_perplexity_step=1.3e+4, train_accuracy_step=0.000, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.3e+4, val_accuracy_epoch=9.79e-5, train_loss_epoch=9.500, train_perplexity_epoch=1.29e+4, train_accuracy_epoch=6.9e-5]Epoch 1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 20/22 [56:35<05:39,  0.01it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.28e+4, train_accuracy_step=0.000, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.3e+4, val_accuracy_epoch=9.79e-5, train_loss_epoch=9.500, train_perplexity_epoch=1.29e+4, train_accuracy_epoch=6.9e-5]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [56:52<00:00,  0.01it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.28e+4, train_accuracy_step=0.000, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.3e+4, val_accuracy_epoch=9.79e-5, train_loss_epoch=9.500, train_perplexity_epoch=1.29e+4, train_accuracy_epoch=6.9e-5]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [56:52<00:00,  0.01it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.29e+4, train_accuracy_step=0.000239, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.3e+4, val_accuracy_epoch=9.79e-5, train_loss_epoch=9.500, train_perplexity_epoch=1.29e+4, train_accuracy_epoch=6.9e-5]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:53<00:00,  0.04it/s][A
                                                                      [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [1:00:02<00:00,  0.01it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.29e+4, train_accuracy_step=0.000239, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.29e+4, val_accuracy_epoch=9.79e-5, train_loss_epoch=9.500, train_perplexity_epoch=1.29e+4, train_accuracy_epoch=6.9e-5]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [1:00:02<00:00,  0.01it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.29e+4, train_accuracy_step=0.000239, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.29e+4, val_accuracy_epoch=9.79e-5, train_loss_epoch=9.530, train_perplexity_epoch=1.3e+4, train_accuracy_epoch=0.000121]Epoch 1, global step 6: 'val_perplexity' reached 12944.50098 (best 12944.50098), saving model to '/home/a03-yzhang/projects/protllama2/batch_script/pretrain_protllama_ppi/pl_model_cache/epoch=1-train_perplexity=13004.136-val_perplexity=12944.501-ppi_10_23_10k_2048.ckpt' as top 2
Epoch 1:   0%|          | 0/22 [00:00<?, ?it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.29e+4, train_accuracy_step=0.000239, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.29e+4, val_accuracy_epoch=9.79e-5, train_loss_epoch=9.530, train_perplexity_epoch=1.3e+4, train_accuracy_epoch=0.000121]           Epoch 2:   0%|          | 0/22 [00:00<?, ?it/s, v_num=on_7, train_loss_step=9.460, train_perplexity_step=1.29e+4, train_accuracy_step=0.000239, val_loss_step=9.470, val_perplexity_step=1.29e+4, val_accuracy_step=0.000227, val_loss_epoch=9.470, val_perplexity_epoch=1.29e+4, val_accuracy_epoch=9.79e-5, train_loss_epoch=9.530, train_perplexity_epoch=1.3e+4, train_accuracy_epoch=0.000121]