Global seed set to 42
Seed set to 42
/home/a03-yzhang/.local/lib/python3.8/site-packages/lightning/fabric/connector.py:565: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
[rank: 1] Global seed set to 42
[rank: 1] Seed set to 42
[rank: 1] Seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
[rank: 6] Global seed set to 42
[rank: 6] Seed set to 42
[rank: 2] Global seed set to 42
[rank: 5] Global seed set to 42
[rank: 3] Global seed set to 42
[rank: 7] Global seed set to 42
[rank: 4] Global seed set to 42
[rank: 2] Seed set to 42
[rank: 5] Seed set to 42
[rank: 3] Seed set to 42
[rank: 7] Seed set to 42
[rank: 4] Seed set to 42
[rank: 6] Seed set to 42
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
[rank: 2] Seed set to 42
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
[rank: 5] Seed set to 42
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
[rank: 7] Seed set to 42
[rank: 3] Seed set to 42
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
[rank: 4] Seed set to 42
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
wandb: Currently logged in as: ynuozhang (ynuoteam). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in pretrain_protllama_ppi/pl_logs/wandb/run-20231020_011036-version_5
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run ppi_10_20_32k_pre-training_log
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ynuoteam/pretrain_protllama_ppi
wandb: üöÄ View run at https://wandb.ai/ynuoteam/pretrain_protllama_ppi/runs/version_5
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]

  | Name  | Type             | Params
-------------------------------------------
0 | model | LlamaForCausalLM | 674 M 
-------------------------------------------
674 M     Trainable params
0         Non-trainable params
674 M     Total params
2,699.576 Total estimated model params size (MB)
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 1024,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 32000
}

Initializing dataset...
Initializing dataset...
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 1024,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 32000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 1024,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 32000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 1024,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 32000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 1024,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 32000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 1024,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 32000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 1024,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 32000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1280,
  "initializer_range": 0.02,
  "intermediate_size": 3440,
  "max_position_embeddings": 1024,
  "model_type": "llama",
  "num_attention_heads": 20,
  "num_hidden_layers": 30,
  "num_key_value_heads": 20,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 32000
}

Initializing dataset...
Initializing dataset...
Building validation dataloader
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]STAGE:2023-10-20 01:13:19 805093:805093 ActivityProfilerController.cpp:312] Completed Stage: Warm UpSTAGE:2023-10-20 01:13:19 805087:805087 ActivityProfilerController.cpp:312] Completed Stage: Warm Up

STAGE:2023-10-20 01:13:19 805095:805095 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2023-10-20 01:13:19 805089:805089 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2023-10-20 01:13:19 805090:805090 ActivityProfilerController.cpp:312] Completed Stage: Warm UpSTAGE:2023-10-20 01:13:19 805088:805088 ActivityProfilerController.cpp:312] Completed Stage: Warm Up

STAGE:2023-10-20 01:13:19 804734:804734 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2023-10-20 01:13:19 805086:805086 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:44<00:00,  0.04it/s]                                                                           Building training dataloader
Building training dataloader
Building training dataloader
Building training dataloader
Building training dataloader
Building training dataloader
Building training dataloader
Building training dataloader
/home/a03-yzhang/.local/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (41) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/41 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/41 [00:00<?, ?it/s] STAGE:2023-10-20 01:43:11 805090:805090 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 01:43:11 805093:805093 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 01:43:11 805087:805087 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 01:43:11 805089:805089 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 01:43:11 805095:805095 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 01:43:11 805088:805088 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 01:43:11 805086:805086 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 01:43:11 804734:804734 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 01:43:11 805090:805090 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 01:43:11 805093:805093 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 01:43:11 805087:805087 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 01:43:11 805089:805089 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 01:43:11 805095:805095 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 01:43:11 805086:805086 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 01:43:11 805088:805088 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 01:43:11 804734:804734 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
Epoch 0:  24%|‚ñà‚ñà‚ñç       | 10/41 [25:50<1:20:05,  0.01it/s]Epoch 0:  24%|‚ñà‚ñà‚ñç       | 10/41 [25:50<1:20:05,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.08e+4, train_accuracy_step=0.000124]Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 20/41 [39:25<41:24,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.08e+4, train_accuracy_step=0.000124]  Epoch 0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 20/41 [39:25<41:24,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.07e+4, train_accuracy_step=0.000]   Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 30/41 [45:59<16:51,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.07e+4, train_accuracy_step=0.000]Epoch 0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 30/41 [45:59<16:51,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.11e+4, train_accuracy_step=0.000]Epoch 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 40/41 [52:54<01:19,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.11e+4, train_accuracy_step=0.000]Epoch 0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 40/41 [52:54<01:19,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.13e+4, train_accuracy_step=0.000]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [59:41<00:00,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.13e+4, train_accuracy_step=0.000]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [59:41<00:00,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.08e+4, train_accuracy_step=0.000]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/4 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s][ASTAGE:2023-10-20 02:18:58 805095:805095 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2023-10-20 02:18:58 805090:805090 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2023-10-20 02:18:58 805089:805089 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2023-10-20 02:18:58 805087:805087 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2023-10-20 02:18:58 805086:805086 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2023-10-20 02:18:58 804734:804734 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2023-10-20 02:18:58 805093:805093 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2023-10-20 02:18:58 805088:805088 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2023-10-20 02:20:25 805095:805095 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 02:20:25 805090:805090 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 02:20:25 805088:805088 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 02:20:25 804734:804734 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 02:20:25 805086:805086 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 02:20:25 805087:805087 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 02:20:25 805089:805089 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 02:20:25 805095:805095 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 02:20:25 805090:805090 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 02:20:25 805088:805088 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 02:20:25 804734:804734 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 02:20:25 805086:805086 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 02:20:25 805093:805093 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2023-10-20 02:20:25 805087:805087 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 02:20:25 805089:805089 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2023-10-20 02:20:25 805093:805093 ActivityProfilerController.cpp:322] Completed Stage: Post Processing

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:29<00:00,  0.04it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [1:02:55<00:00,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.08e+4, train_accuracy_step=0.000, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [1:02:55<00:00,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.08e+4, train_accuracy_step=0.000, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]Epoch 0, global step 5: 'val_perplexity' reached 43131.32422 (best 43131.32422), saving model to '/home/a03-yzhang/projects/protllama2/batch_script/pretrain_protllama_ppi/pl_model_cache/epoch=0-train_perplexity=41058.023-val_perplexity=43131.324-ppi_10_20_32k_1024.ckpt' as top 1
Epoch 0:   0%|          | 0/41 [00:00<?, ?it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.08e+4, train_accuracy_step=0.000, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]           Epoch 1:   0%|          | 0/41 [00:00<?, ?it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.08e+4, train_accuracy_step=0.000, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]Epoch 1:  24%|‚ñà‚ñà‚ñç       | 10/41 [41:08<2:07:33,  0.00it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.08e+4, train_accuracy_step=0.000, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]Epoch 1:  24%|‚ñà‚ñà‚ñç       | 10/41 [41:08<2:07:33,  0.00it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.06e+4, train_accuracy_step=0.000274, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 20/41 [48:00<50:24,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.06e+4, train_accuracy_step=0.000274, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]  Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 20/41 [48:00<50:24,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.01e+4, train_accuracy_step=0.000282, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 30/41 [55:00<20:10,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.01e+4, train_accuracy_step=0.000282, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 30/41 [55:00<20:10,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.07e+4, train_accuracy_step=0.000129, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]Epoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 40/41 [1:08:26<01:42,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.07e+4, train_accuracy_step=0.000129, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]Epoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 40/41 [1:08:26<01:42,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.01e+4, train_accuracy_step=0.000219, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [1:08:29<00:00,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.01e+4, train_accuracy_step=0.000219, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [1:08:29<00:00,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.1e+4, train_accuracy_step=0.000, val_loss_step=10.60, val_perplexity_step=4.12e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.31e+4, val_accuracy_epoch=0.000, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]    
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/4 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [02:07<00:00,  0.03it/s][A
                                                                      [AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [1:12:27<00:00,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.1e+4, train_accuracy_step=0.000, val_loss_step=10.60, val_perplexity_step=4.1e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.29e+4, val_accuracy_epoch=5.62e-5, train_loss_epoch=10.60, train_perplexity_epoch=4.11e+4, train_accuracy_epoch=4.21e-5]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [1:12:27<00:00,  0.01it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.1e+4, train_accuracy_step=0.000, val_loss_step=10.60, val_perplexity_step=4.1e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.29e+4, val_accuracy_epoch=5.62e-5, train_loss_epoch=10.70, train_perplexity_epoch=4.12e+4, train_accuracy_epoch=6.24e-5]Epoch 1, global step 10: 'val_perplexity' reached 42922.96875 (best 42922.96875), saving model to '/home/a03-yzhang/projects/protllama2/batch_script/pretrain_protllama_ppi/pl_model_cache/epoch=1-train_perplexity=41220.164-val_perplexity=42922.969-ppi_10_20_32k_1024.ckpt' as top 2
Epoch 1:   0%|          | 0/41 [00:00<?, ?it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.1e+4, train_accuracy_step=0.000, val_loss_step=10.60, val_perplexity_step=4.1e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.29e+4, val_accuracy_epoch=5.62e-5, train_loss_epoch=10.70, train_perplexity_epoch=4.12e+4, train_accuracy_epoch=6.24e-5]           Epoch 2:   0%|          | 0/41 [00:00<?, ?it/s, v_num=on_5, train_loss_step=10.60, train_perplexity_step=4.1e+4, train_accuracy_step=0.000, val_loss_step=10.60, val_perplexity_step=4.1e+4, val_accuracy_step=0.000, val_loss_epoch=11.10, val_perplexity_epoch=4.29e+4, val_accuracy_epoch=5.62e-5, train_loss_epoch=10.70, train_perplexity_epoch=4.12e+4, train_accuracy_epoch=6.24e-5]/home/a03-yzhang/projects/protllama2/batch_script/pretrain_protllama_ppi/pl_model_cache/epoch=1-train_perplexity=41220.164-val_perplexity=42922.969-ppi_10_20_32k_1024.ckpt
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/a03-yzhang/.local/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/home/a03-yzhang/.local/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/queues.py", line 116, in get
    return _ForkingPickler.loads(res)
  File "/home/a03-yzhang/.local/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 355, in rebuild_storage_fd
    fd = df.detach()
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/connection.py", line 512, in Client
    answer_challenge(c, authkey)
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/connection.py", line 756, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/connection.py", line 220, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/connection.py", line 418, in _recv_bytes
    buf = self._recv(4)
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/connection.py", line 383, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/a03-yzhang/.local/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/home/a03-yzhang/.local/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/queues.py", line 116, in get
    return _ForkingPickler.loads(res)
  File "/home/a03-yzhang/.local/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 355, in rebuild_storage_fd
    fd = df.detach()
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/connection.py", line 506, in Client
    c = SocketClient(address)
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/connection.py", line 634, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/a03-yzhang/.local/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/home/a03-yzhang/.local/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/queues.py", line 116, in get
    return _ForkingPickler.loads(res)
  File "/home/a03-yzhang/.local/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 355, in rebuild_storage_fd
    fd = df.detach()
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/connection.py", line 506, in Client
    c = SocketClient(address)
  File "/opt/rh/rh-python38/root/usr/lib64/python3.8/multiprocessing/connection.py", line 634, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory
