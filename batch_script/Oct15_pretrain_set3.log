Global seed set to 42
wandb: Currently logged in as: ynuozhang (ynuoteam). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in pretrain_protllama/pl_logs/wandb/run-20231016_022938-version_3
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run protein_Oct15_8k_pre-training_log
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ynuoteam/pretrain_protllama
wandb: üöÄ View run at https://wandb.ai/ynuoteam/pretrain_protllama/runs/version_3
Global seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
[rank: 1] Global seed set to 42
[rank: 2] Global seed set to 42
[rank: 3] Global seed set to 42
[rank: 5] Global seed set to 42
[rank: 4] Global seed set to 42
[rank: 6] Global seed set to 42
[rank: 7] Global seed set to 42
[rank: 1] Global seed set to 42
[rank: 2] Global seed set to 42
[rank: 3] Global seed set to 42
[rank: 5] Global seed set to 42
[rank: 4] Global seed set to 42
[rank: 6] Global seed set to 42
[rank: 7] Global seed set to 42
[rank: 2] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
[rank: 5] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
[rank: 3] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
[rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
[rank: 6] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
[rank: 7] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
[rank: 4] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]

  | Name  | Type             | Params
-------------------------------------------
0 | model | LlamaForCausalLM | 421 M 
-------------------------------------------
421 M     Trainable params
0         Non-trainable params
421 M     Total params
1,684.804 Total estimated model params size (MB)
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2752,
  "max_position_embeddings": 512,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 8000
}

Initializing dataset...
Initializing dataset...
Building training dataloader
Building validation dataloader
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2752,
  "max_position_embeddings": 512,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 8000
}

Initializing dataset...
Initializing dataset...
Building training dataloader
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2752,
  "max_position_embeddings": 512,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 8000
}

Initializing dataset...
Initializing dataset...
Building training dataloader
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2752,
  "max_position_embeddings": 512,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 8000
}

Initializing dataset...
Initializing dataset...
Building training dataloader
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2752,
  "max_position_embeddings": 512,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 8000
}

Initializing dataset...
Initializing dataset...
Building training dataloader
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2752,
  "max_position_embeddings": 512,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 8000
}

Initializing dataset...
Initializing dataset...
Building training dataloader
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2752,
  "max_position_embeddings": 512,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 8000
}

Initializing dataset...
Initializing dataset...
Building training dataloader
Building validation dataloader
Load processed datasets
LlamaConfig {
  "attention_bias": false,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2752,
  "max_position_embeddings": 512,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.34.0",
  "use_cache": true,
  "vocab_size": 8000
}

Initializing dataset...
Initializing dataset...
Building training dataloader
Building validation dataloader
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.22s/it]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.22s/it]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.24it/s][A
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:12<00:00, 12.01s/it, val_loss=11.90, val_perplexity=1.27e+4, val_accuracy=0.000]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:12<00:00, 12.02s/it, val_loss=11.90, val_perplexity=1.27e+4, val_accuracy=0.000, train_loss=10.70, train_perplexity=1.15e+4, train_accuracy=0.000]`Trainer.fit` stopped: `max_steps=1` reached.
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:12<00:00, 12.02s/it, val_loss=11.90, val_perplexity=1.27e+4, val_accuracy=0.000, train_loss=10.70, train_perplexity=1.15e+4, train_accuracy=0.000]
Training took 74.38 seconds.

wandb: Waiting for W&B process to finish... (success).
Training took 65.38 seconds.

Training took 67.09 seconds.

Training took 66.75 seconds.

Training took 67.40 seconds.

Training took 67.71 seconds.

Training took 67.06 seconds.

Training took 67.98 seconds.

wandb: 
wandb: Run history:
wandb: training_time ‚ñÅ
wandb: 
wandb: Run summary:
wandb: training_time 74.38237
wandb: 
wandb: üöÄ View run protein_Oct15_8k_pre-training_log at: https://wandb.ai/ynuoteam/pretrain_protllama/runs/version_3
wandb: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: pretrain_protllama/pl_logs/wandb/run-20231016_022938-version_3/logs
